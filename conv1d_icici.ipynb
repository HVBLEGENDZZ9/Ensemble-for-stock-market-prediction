{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7c8e985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "df_stock = pd.read_csv(\"icici/ICICIBANK.NS.csv\",\n",
    "                 parse_dates = [\"Date\"],\n",
    "                 index_col = [\"Date\"])\n",
    "df_posts = pd.read_excel(\"icici/icici_bank_final_posts.xlsx\")\n",
    "# removing the unnecessary columns\n",
    "df_posts.drop([\"Unnamed: 0\"], axis=1,inplace=True)\n",
    "# removing spam posts\n",
    "df_posts = df_posts[df_posts.Spam==0.0]\n",
    "df_posts.drop([\"Spam\"],axis=1,inplace=True)\n",
    "# sliding a window of 7 days and adding all the TIs\n",
    "from stock_helper import prepare_data\n",
    "x,y = prepare_data(df_stock)\n",
    "# slicing the data\n",
    "final_x = x[np.datetime64(\"2021-11-13\"):]\n",
    "final_y = y[np.datetime64(\"2021-11-13\"):]\n",
    "# reversing the posts data\n",
    "df_posts = df_posts[::-1]\n",
    "final_posts = df_posts[7:]\n",
    "# loading the sentiment analysis model\n",
    "sent_model = tf.keras.models.load_model(\"final_bert\")\n",
    "# removing duplicates from the data\n",
    "final_posts.drop_duplicates(subset=['Messages'])\n",
    "# calculating the sentiments score\n",
    "sentiments = []\n",
    "prev = np.datetime64(\"2015-11-12 21:31:26\")\n",
    "for i in final_y.index:\n",
    "    total=0\n",
    "    cnt=0\n",
    "    for j in final_posts.itertuples():\n",
    "        _,msg,time = j\n",
    "        if np.datetime64(time)<np.datetime64(i) and np.datetime64(time)>prev:\n",
    "            total += tf.squeeze(sent_model.predict([msg])).numpy()\n",
    "            cnt+=1\n",
    "    prev = np.datetime64(i)\n",
    "    if(cnt==0):\n",
    "        sentiments.append(0)\n",
    "    else:\n",
    "        sentiments.append(total/cnt)\n",
    "# getting indices where sentiments score is 0\n",
    "zero_index = []\n",
    "for i,j in enumerate(sentiments):\n",
    "    if(j==0):\n",
    "        zero_index.append(i)\n",
    "# removing all the zero values indices\n",
    "sentiments = np.delete(sentiments,zero_index)\n",
    "final_x_zeros = final_x.copy()\n",
    "final_y_zeros = final_y.copy()\n",
    "final_y_zeros = final_y_zeros.to_frame()\n",
    "final_x_zeros['removal_assist'] = np.arange(0,len(final_x),1)\n",
    "final_y_zeros['removal_assist'] = np.arange(0,len(final_x),1)\n",
    "final_y_zeros = final_y_zeros[final_y_zeros.removal_assist.isin(zero_index)==False]\n",
    "final_x_zeros = final_x_zeros[final_x_zeros.removal_assist.isin(zero_index)==False]\n",
    "# removing the added helper column\n",
    "final_x_zeros.drop([\"removal_assist\"], axis=1,inplace=True)\n",
    "final_y_zeros.drop([\"removal_assist\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16d18436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " inputs (InputLayer)         [(None, 71, 1)]           0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 71, 64)            256       \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 35, 64)           0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 35, 64)            12352     \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 17, 64)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 64)               0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,673\n",
      "Trainable params: 12,673\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "inputs = tf.keras.Input(shape=(71,1), name=\"inputs\")\n",
    "x = tf.keras.layers.Conv1D(64, 3, activation=\"relu\", padding=\"causal\")(inputs)\n",
    "x = tf.keras.layers.MaxPooling1D()(x)\n",
    "x = tf.keras.layers.Conv1D(64, 3, activation=\"relu\", padding=\"causal\")(x)\n",
    "x = tf.keras.layers.MaxPooling1D()(x)\n",
    "x = tf.keras.layers.GlobalMaxPooling1D()(x)\n",
    "outputs = tf.keras.layers.Dense(1)(x)\n",
    "model = tf.keras.models.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "974b5b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2/2 [==============================] - 3s 247ms/step - loss: 828.4128 - mae: 828.4128 - mse: 688463.0625 - val_loss: 722.7546 - val_mae: 722.7546 - val_mse: 522476.8750\n",
      "Epoch 2/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 662.6653 - mae: 662.6653 - mse: 441512.5938 - val_loss: 557.8418 - val_mae: 557.8418 - val_mse: 311285.6250\n",
      "Epoch 3/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 507.0433 - mae: 507.0433 - mse: 259409.0156 - val_loss: 400.2975 - val_mae: 400.2975 - val_mse: 160335.0469\n",
      "Epoch 4/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 358.1701 - mae: 358.1701 - mse: 129892.5000 - val_loss: 247.5738 - val_mae: 247.5738 - val_mse: 61390.0664\n",
      "Epoch 5/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 212.8154 - mae: 212.8154 - mse: 46813.9258 - val_loss: 96.3811 - val_mae: 96.3811 - val_mse: 9390.6807\n",
      "Epoch 6/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 72.0916 - mae: 72.0916 - mse: 6452.8359 - val_loss: 50.5814 - val_mae: 50.5814 - val_mse: 2667.1194\n",
      "Epoch 7/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 65.0839 - mae: 65.0839 - mse: 5432.2148 - val_loss: 130.2995 - val_mae: 130.2995 - val_mse: 17089.0605\n",
      "Epoch 8/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 131.6432 - mae: 131.6432 - mse: 18749.0547 - val_loss: 145.7442 - val_mae: 145.7442 - val_mse: 21358.6348\n",
      "Epoch 9/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 140.8673 - mae: 140.8673 - mse: 21226.3535 - val_loss: 119.0964 - val_mae: 119.0964 - val_mse: 14303.2725\n",
      "Epoch 10/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 111.8343 - mae: 111.8343 - mse: 14062.4639 - val_loss: 64.5843 - val_mae: 64.5843 - val_mse: 4291.2808\n",
      "Epoch 11/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 58.4168 - mae: 58.4168 - mse: 5027.9897 - val_loss: 10.2918 - val_mae: 10.2918 - val_mse: 138.5425\n",
      "Epoch 12/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 31.7784 - mae: 31.7784 - mse: 1666.1335 - val_loss: 62.9713 - val_mae: 62.9713 - val_mse: 4079.6897\n",
      "Epoch 13/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 59.0614 - mae: 59.0614 - mse: 4734.8169 - val_loss: 85.4923 - val_mae: 85.4923 - val_mse: 7418.4624\n",
      "Epoch 14/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 75.9797 - mae: 75.9797 - mse: 7050.0269 - val_loss: 75.3822 - val_mae: 75.3822 - val_mse: 5794.5195\n",
      "Epoch 15/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 63.6652 - mae: 63.6652 - mse: 5353.8311 - val_loss: 42.6829 - val_mae: 42.6829 - val_mse: 1936.7352\n",
      "Epoch 16/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 40.3965 - mae: 40.3965 - mse: 2478.7380 - val_loss: 9.9841 - val_mae: 9.9841 - val_mse: 123.6299\n",
      "Epoch 17/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 32.2282 - mae: 32.2282 - mse: 1543.3274 - val_loss: 24.4691 - val_mae: 24.4691 - val_mse: 715.3062\n",
      "Epoch 18/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 39.3600 - mae: 39.3600 - mse: 2212.7029 - val_loss: 29.1989 - val_mae: 29.1989 - val_mse: 964.6420\n",
      "Epoch 19/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 40.7080 - mae: 40.7080 - mse: 2338.9868 - val_loss: 17.5098 - val_mae: 17.5098 - val_mse: 412.4569\n",
      "Epoch 20/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 33.0763 - mae: 33.0763 - mse: 1662.4530 - val_loss: 10.3085 - val_mae: 10.3085 - val_mse: 139.0681\n",
      "Epoch 21/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 29.5167 - mae: 29.5167 - mse: 1254.0392 - val_loss: 25.7193 - val_mae: 25.7193 - val_mse: 755.9112\n",
      "Epoch 22/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 32.4623 - mae: 32.4623 - mse: 1597.0658 - val_loss: 28.3218 - val_mae: 28.3218 - val_mse: 893.5627\n",
      "Epoch 23/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 32.5089 - mae: 32.5089 - mse: 1613.9484 - val_loss: 17.7094 - val_mae: 17.7094 - val_mse: 403.5060\n",
      "Epoch 24/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 29.4493 - mae: 29.4493 - mse: 1258.7267 - val_loss: 8.3779 - val_mae: 8.3779 - val_mse: 89.2859\n",
      "Epoch 25/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 27.0851 - mae: 27.0851 - mse: 1087.8025 - val_loss: 16.5405 - val_mae: 16.5405 - val_mse: 361.3579\n",
      "Epoch 26/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 31.2940 - mae: 31.2940 - mse: 1455.5062 - val_loss: 20.7039 - val_mae: 20.7039 - val_mse: 515.1067\n",
      "Epoch 27/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 32.7587 - mae: 32.7587 - mse: 1570.8762 - val_loss: 11.0051 - val_mae: 11.0051 - val_mse: 193.2816\n",
      "Epoch 28/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 27.5487 - mae: 27.5487 - mse: 1113.4507 - val_loss: 9.7592 - val_mae: 9.7592 - val_mse: 131.3592\n",
      "Epoch 29/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 26.9731 - mae: 26.9731 - mse: 1039.0669 - val_loss: 16.3744 - val_mae: 16.3744 - val_mse: 341.8900\n",
      "Epoch 30/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 28.0779 - mae: 28.0779 - mse: 1139.9131 - val_loss: 14.8926 - val_mae: 14.8926 - val_mse: 284.2581\n",
      "Epoch 31/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 27.1579 - mae: 27.1579 - mse: 1060.8256 - val_loss: 8.6234 - val_mae: 8.6234 - val_mse: 96.4578\n",
      "Epoch 32/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 25.8633 - mae: 25.8633 - mse: 930.2357 - val_loss: 8.2350 - val_mae: 8.2350 - val_mse: 120.4738\n",
      "Epoch 33/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 25.6168 - mae: 25.6168 - mse: 986.3524 - val_loss: 8.5336 - val_mae: 8.5336 - val_mse: 128.6202\n",
      "Epoch 34/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 25.0862 - mae: 25.0862 - mse: 930.9559 - val_loss: 7.9207 - val_mae: 7.9207 - val_mse: 85.9189\n",
      "Epoch 35/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 24.8408 - mae: 24.8408 - mse: 868.3895 - val_loss: 10.6820 - val_mae: 10.6820 - val_mse: 152.8387\n",
      "Epoch 36/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 24.6693 - mae: 24.6693 - mse: 861.3452 - val_loss: 14.2993 - val_mae: 14.2993 - val_mse: 255.2088\n",
      "Epoch 37/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 25.3181 - mae: 25.3181 - mse: 921.1203 - val_loss: 12.5263 - val_mae: 12.5263 - val_mse: 204.5941\n",
      "Epoch 38/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 24.1254 - mae: 24.1254 - mse: 831.1168 - val_loss: 7.8868 - val_mae: 7.8868 - val_mse: 95.5607\n",
      "Epoch 39/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 23.7508 - mae: 23.7508 - mse: 761.5854 - val_loss: 13.5040 - val_mae: 13.5040 - val_mse: 280.1320\n",
      "Epoch 40/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 24.9371 - mae: 24.9371 - mse: 899.2518 - val_loss: 10.9471 - val_mae: 10.9471 - val_mse: 214.0668\n",
      "Epoch 41/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 23.3898 - mae: 23.3898 - mse: 779.9940 - val_loss: 8.7478 - val_mae: 8.7478 - val_mse: 111.2634\n",
      "Epoch 42/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 23.1094 - mae: 23.1094 - mse: 737.3557 - val_loss: 16.1652 - val_mae: 16.1652 - val_mse: 321.8286\n",
      "Epoch 43/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 24.7707 - mae: 24.7707 - mse: 898.2265 - val_loss: 11.8530 - val_mae: 11.8530 - val_mse: 183.4422\n",
      "Epoch 44/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 22.2180 - mae: 22.2180 - mse: 657.0012 - val_loss: 11.3939 - val_mae: 11.3939 - val_mse: 235.8503\n",
      "Epoch 45/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 22.8215 - mae: 22.8215 - mse: 725.9960 - val_loss: 24.4560 - val_mae: 24.4560 - val_mse: 717.1084\n",
      "Epoch 46/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 26.9969 - mae: 26.9969 - mse: 1091.6199 - val_loss: 18.9566 - val_mae: 18.9566 - val_mse: 484.0689\n",
      "Epoch 47/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 23.6656 - mae: 23.6656 - mse: 800.2416 - val_loss: 9.5546 - val_mae: 9.5546 - val_mse: 132.4420\n",
      "Epoch 48/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 22ms/step - loss: 21.4310 - mae: 21.4310 - mse: 626.8738 - val_loss: 16.8529 - val_mae: 16.8529 - val_mse: 362.8664\n",
      "Epoch 49/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 23.8498 - mae: 23.8498 - mse: 859.7641 - val_loss: 13.5902 - val_mae: 13.5902 - val_mse: 237.1821\n",
      "Epoch 50/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 21.5419 - mae: 21.5419 - mse: 660.7257 - val_loss: 10.4136 - val_mae: 10.4136 - val_mse: 207.7352\n",
      "Epoch 51/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 22.1110 - mae: 22.1110 - mse: 637.7178 - val_loss: 15.8939 - val_mae: 15.8939 - val_mse: 403.0934\n",
      "Epoch 52/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 22.2949 - mae: 22.2949 - mse: 664.8275 - val_loss: 11.5007 - val_mae: 11.5007 - val_mse: 249.2913\n",
      "Epoch 53/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 21.1900 - mae: 21.1900 - mse: 548.4055 - val_loss: 11.3664 - val_mae: 11.3664 - val_mse: 173.5413\n",
      "Epoch 54/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 20.6126 - mae: 20.6126 - mse: 546.7025 - val_loss: 13.2198 - val_mae: 13.2198 - val_mse: 211.2794\n",
      "Epoch 55/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 20.7650 - mae: 20.7650 - mse: 590.5432 - val_loss: 13.2460 - val_mae: 13.2460 - val_mse: 209.1819\n",
      "Epoch 56/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 20.5164 - mae: 20.5164 - mse: 573.2050 - val_loss: 13.0211 - val_mae: 13.0211 - val_mse: 202.1320\n",
      "Epoch 57/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 20.2922 - mae: 20.2922 - mse: 541.0798 - val_loss: 11.9761 - val_mae: 11.9761 - val_mse: 204.3164\n",
      "Epoch 58/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 20.1363 - mae: 20.1363 - mse: 496.1999 - val_loss: 11.2124 - val_mae: 11.2124 - val_mse: 252.0694\n",
      "Epoch 59/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 20.2952 - mae: 20.2952 - mse: 491.9180 - val_loss: 11.3885 - val_mae: 11.3885 - val_mse: 258.6192\n",
      "Epoch 60/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 20.6467 - mae: 20.6467 - mse: 501.4212 - val_loss: 11.3476 - val_mae: 11.3476 - val_mse: 254.2812\n",
      "Epoch 61/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 19.8776 - mae: 19.8776 - mse: 471.7051 - val_loss: 14.3485 - val_mae: 14.3485 - val_mse: 376.2585\n",
      "Epoch 62/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 20.2476 - mae: 20.2476 - mse: 530.5723 - val_loss: 14.3734 - val_mae: 14.3734 - val_mse: 378.5623\n",
      "Epoch 63/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 20.0261 - mae: 20.0261 - mse: 506.0511 - val_loss: 11.4692 - val_mae: 11.4692 - val_mse: 256.7346\n",
      "Epoch 64/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 19.5393 - mae: 19.5393 - mse: 453.9827 - val_loss: 13.1877 - val_mae: 13.1877 - val_mse: 221.1602\n",
      "Epoch 65/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 19.8652 - mae: 19.8652 - mse: 515.9529 - val_loss: 14.5379 - val_mae: 14.5379 - val_mse: 244.3853\n",
      "Epoch 66/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 19.9880 - mae: 19.9880 - mse: 548.7016 - val_loss: 13.9264 - val_mae: 13.9264 - val_mse: 232.4801\n",
      "Epoch 67/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 19.6040 - mae: 19.6040 - mse: 516.1938 - val_loss: 13.9448 - val_mae: 13.9448 - val_mse: 233.0084\n",
      "Epoch 68/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 19.4671 - mae: 19.4671 - mse: 496.7205 - val_loss: 12.9585 - val_mae: 12.9585 - val_mse: 228.5427\n",
      "Epoch 69/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 19.6563 - mae: 19.6563 - mse: 473.8663 - val_loss: 12.9772 - val_mae: 12.9772 - val_mse: 221.7832\n",
      "Epoch 70/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 19.1542 - mae: 19.1542 - mse: 469.1685 - val_loss: 14.8730 - val_mae: 14.8730 - val_mse: 257.8235\n",
      "Epoch 71/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 19.6444 - mae: 19.6444 - mse: 574.7158 - val_loss: 15.0934 - val_mae: 15.0934 - val_mse: 266.9898\n",
      "Epoch 72/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 19.4910 - mae: 19.4910 - mse: 566.5922 - val_loss: 12.2644 - val_mae: 12.2644 - val_mse: 241.9189\n",
      "Epoch 73/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 18.8506 - mae: 18.8506 - mse: 427.3440 - val_loss: 19.8460 - val_mae: 19.8460 - val_mse: 601.0781\n",
      "Epoch 74/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 20.8198 - mae: 20.8198 - mse: 616.9008 - val_loss: 16.2336 - val_mae: 16.2336 - val_mse: 453.0216\n",
      "Epoch 75/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 19.3026 - mae: 19.3026 - mse: 462.7910 - val_loss: 14.4969 - val_mae: 14.4969 - val_mse: 246.5291\n",
      "Epoch 76/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 19.1125 - mae: 19.1125 - mse: 552.0635 - val_loss: 16.7503 - val_mae: 16.7503 - val_mse: 353.9404\n",
      "Epoch 77/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 20.5601 - mae: 20.5601 - mse: 672.7477 - val_loss: 13.8035 - val_mae: 13.8035 - val_mse: 240.3623\n",
      "Epoch 78/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 18.9024 - mae: 18.9024 - mse: 473.2110 - val_loss: 13.6382 - val_mae: 13.6382 - val_mse: 368.5049\n",
      "Epoch 79/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 18.4654 - mae: 18.4654 - mse: 432.7296 - val_loss: 19.9966 - val_mae: 19.9966 - val_mse: 612.5269\n",
      "Epoch 80/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 19.7210 - mae: 19.7210 - mse: 548.9084 - val_loss: 19.9833 - val_mae: 19.9833 - val_mse: 613.0320\n",
      "Epoch 81/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 19.3985 - mae: 19.3985 - mse: 529.8401 - val_loss: 13.6329 - val_mae: 13.6329 - val_mse: 372.7402\n",
      "Epoch 82/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 18.4876 - mae: 18.4876 - mse: 432.7349 - val_loss: 13.5374 - val_mae: 13.5374 - val_mse: 259.8720\n",
      "Epoch 83/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 18.3507 - mae: 18.3507 - mse: 450.0925 - val_loss: 14.6391 - val_mae: 14.6391 - val_mse: 261.3197\n",
      "Epoch 84/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 18.6068 - mae: 18.6068 - mse: 483.1755 - val_loss: 13.5586 - val_mae: 13.5586 - val_mse: 273.2845\n",
      "Epoch 85/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 18.4060 - mae: 18.4060 - mse: 440.3387 - val_loss: 13.4089 - val_mae: 13.4089 - val_mse: 369.3973\n",
      "Epoch 86/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 18.9222 - mae: 18.9222 - mse: 454.3445 - val_loss: 13.1922 - val_mae: 13.1922 - val_mse: 360.8701\n",
      "Epoch 87/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 17.8026 - mae: 17.8026 - mse: 398.7329 - val_loss: 15.1193 - val_mae: 15.1193 - val_mse: 271.7402\n",
      "Epoch 88/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 19.3763 - mae: 19.3763 - mse: 553.9335 - val_loss: 15.8159 - val_mae: 15.8159 - val_mse: 287.1926\n",
      "Epoch 89/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 18.6655 - mae: 18.6655 - mse: 500.8429 - val_loss: 13.7333 - val_mae: 13.7333 - val_mse: 386.4579\n",
      "Epoch 90/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 18.7665 - mae: 18.7665 - mse: 444.5728 - val_loss: 20.0997 - val_mae: 20.0997 - val_mse: 629.1153\n",
      "Epoch 91/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 18.5539 - mae: 18.5539 - mse: 494.8438 - val_loss: 12.7969 - val_mae: 12.7969 - val_mse: 335.1032\n",
      "Epoch 92/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 17.9719 - mae: 17.9719 - mse: 414.1334 - val_loss: 15.3135 - val_mae: 15.3135 - val_mse: 271.7516\n",
      "Epoch 93/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 18.5541 - mae: 18.5541 - mse: 512.1691 - val_loss: 15.6890 - val_mae: 15.6890 - val_mse: 281.9663\n",
      "Epoch 94/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 18.6295 - mae: 18.6295 - mse: 541.4254 - val_loss: 12.9769 - val_mae: 12.9769 - val_mse: 268.6250\n",
      "Epoch 95/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 18.0112 - mae: 18.0112 - mse: 422.0764 - val_loss: 18.9018 - val_mae: 18.9018 - val_mse: 567.5491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 18.8403 - mae: 18.8403 - mse: 501.0805 - val_loss: 15.7378 - val_mae: 15.7378 - val_mse: 441.7759\n",
      "Epoch 97/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 17.8697 - mae: 17.8697 - mse: 447.6784 - val_loss: 14.0385 - val_mae: 14.0385 - val_mse: 241.8929\n",
      "Epoch 98/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 17.9432 - mae: 17.9432 - mse: 470.8245 - val_loss: 14.8803 - val_mae: 14.8803 - val_mse: 254.8666\n",
      "Epoch 99/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 18.4171 - mae: 18.4171 - mse: 509.0380 - val_loss: 14.5461 - val_mae: 14.5461 - val_mse: 247.3487\n",
      "Epoch 100/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 17.8815 - mae: 17.8815 - mse: 478.2939 - val_loss: 15.0160 - val_mae: 15.0160 - val_mse: 258.6189\n",
      "Epoch 101/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 17.7392 - mae: 17.7392 - mse: 481.2440 - val_loss: 12.1564 - val_mae: 12.1564 - val_mse: 313.3188\n",
      "Epoch 102/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 18.4174 - mae: 18.4174 - mse: 428.6810 - val_loss: 19.7173 - val_mae: 19.7173 - val_mse: 598.5005\n",
      "Epoch 103/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 18.2610 - mae: 18.2610 - mse: 463.9109 - val_loss: 12.0513 - val_mae: 12.0513 - val_mse: 308.0917\n",
      "Epoch 104/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 16.8874 - mae: 16.8874 - mse: 363.2345 - val_loss: 15.2878 - val_mae: 15.2878 - val_mse: 269.4119\n",
      "Epoch 105/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 18.3574 - mae: 18.3574 - mse: 541.3670 - val_loss: 15.4839 - val_mae: 15.4839 - val_mse: 279.1494\n",
      "Epoch 106/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 17.9900 - mae: 17.9900 - mse: 522.3237 - val_loss: 11.8983 - val_mae: 11.8983 - val_mse: 301.5777\n",
      "Epoch 107/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 17.4934 - mae: 17.4934 - mse: 388.7106 - val_loss: 15.4567 - val_mae: 15.4567 - val_mse: 423.5529\n",
      "Epoch 108/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 17.5512 - mae: 17.5512 - mse: 402.7521 - val_loss: 11.7146 - val_mae: 11.7146 - val_mse: 272.3628\n",
      "Epoch 109/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 17.8227 - mae: 17.8227 - mse: 417.2093 - val_loss: 11.7444 - val_mae: 11.7444 - val_mse: 263.9243\n",
      "Epoch 110/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 16.7671 - mae: 16.7671 - mse: 363.4154 - val_loss: 17.2140 - val_mae: 17.2140 - val_mse: 486.2771\n",
      "Epoch 111/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 17.9721 - mae: 17.9721 - mse: 453.6899 - val_loss: 18.8453 - val_mae: 18.8453 - val_mse: 554.8829\n",
      "Epoch 112/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 18.1483 - mae: 18.1483 - mse: 454.1844 - val_loss: 11.6931 - val_mae: 11.6931 - val_mse: 271.4826\n",
      "Epoch 113/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 16.6046 - mae: 16.6046 - mse: 356.6140 - val_loss: 15.2160 - val_mae: 15.2160 - val_mse: 269.8843\n",
      "Epoch 114/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 19.3578 - mae: 19.3578 - mse: 588.6375 - val_loss: 13.9031 - val_mae: 13.9031 - val_mse: 232.2960\n",
      "Epoch 115/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 16.4028 - mae: 16.4028 - mse: 387.2952 - val_loss: 20.0896 - val_mae: 20.0896 - val_mse: 616.7792\n",
      "Epoch 116/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 19.0113 - mae: 19.0113 - mse: 516.0455 - val_loss: 20.6294 - val_mae: 20.6294 - val_mse: 643.5648\n",
      "Epoch 117/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 18.0143 - mae: 18.0143 - mse: 470.3513 - val_loss: 13.6185 - val_mae: 13.6185 - val_mse: 230.9735\n",
      "Epoch 118/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 17.0564 - mae: 17.0564 - mse: 452.6544 - val_loss: 16.4111 - val_mae: 16.4111 - val_mse: 335.7241\n",
      "Epoch 119/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 19.8835 - mae: 19.8835 - mse: 613.0258 - val_loss: 12.8875 - val_mae: 12.8875 - val_mse: 231.6332\n",
      "Epoch 120/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 16.3406 - mae: 16.3406 - mse: 374.6346 - val_loss: 14.7480 - val_mae: 14.7480 - val_mse: 397.5695\n",
      "Epoch 121/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 17.1701 - mae: 17.1701 - mse: 418.1647 - val_loss: 20.3231 - val_mae: 20.3231 - val_mse: 627.1104\n",
      "Epoch 122/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 17.9122 - mae: 17.9122 - mse: 462.0564 - val_loss: 12.1808 - val_mae: 12.1808 - val_mse: 256.0837\n",
      "Epoch 123/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 16.2395 - mae: 16.2395 - mse: 378.3546 - val_loss: 14.4686 - val_mae: 14.4686 - val_mse: 244.9790\n",
      "Epoch 124/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 16.9679 - mae: 16.9679 - mse: 437.0694 - val_loss: 11.9973 - val_mae: 11.9973 - val_mse: 297.0754\n",
      "Epoch 125/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 16.2239 - mae: 16.2239 - mse: 350.9850 - val_loss: 16.3080 - val_mae: 16.3080 - val_mse: 461.5474\n",
      "Epoch 126/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 16.8957 - mae: 16.8957 - mse: 393.6651 - val_loss: 18.7803 - val_mae: 18.7803 - val_mse: 564.2513\n",
      "Epoch 127/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 17.2247 - mae: 17.2247 - mse: 423.4972 - val_loss: 12.4208 - val_mae: 12.4208 - val_mse: 325.9830\n",
      "Epoch 128/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 15.9579 - mae: 15.9579 - mse: 361.8260 - val_loss: 15.8051 - val_mae: 15.8051 - val_mse: 288.0109\n",
      "Epoch 129/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 18.5115 - mae: 18.5115 - mse: 530.5633 - val_loss: 13.5898 - val_mae: 13.5898 - val_mse: 257.6740\n",
      "Epoch 130/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 16.6692 - mae: 16.6692 - mse: 400.0293 - val_loss: 15.1824 - val_mae: 15.1824 - val_mse: 429.3366\n",
      "Epoch 131/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 16.4924 - mae: 16.4924 - mse: 371.5350 - val_loss: 13.7511 - val_mae: 13.7511 - val_mse: 375.2241\n",
      "Epoch 132/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 15.8187 - mae: 15.8187 - mse: 343.0453 - val_loss: 14.6613 - val_mae: 14.6613 - val_mse: 256.4620\n",
      "Epoch 133/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 17.3571 - mae: 17.3571 - mse: 461.4331 - val_loss: 12.8395 - val_mae: 12.8395 - val_mse: 269.0061\n",
      "Epoch 134/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 16.9911 - mae: 16.9911 - mse: 398.2101 - val_loss: 18.1174 - val_mae: 18.1174 - val_mse: 538.8562\n",
      "Epoch 135/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 17.0064 - mae: 17.0064 - mse: 411.9188 - val_loss: 12.4530 - val_mae: 12.4530 - val_mse: 293.2976\n",
      "Epoch 136/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 15.7403 - mae: 15.7403 - mse: 356.3194 - val_loss: 15.5401 - val_mae: 15.5401 - val_mse: 278.7596\n",
      "Epoch 137/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 18.6487 - mae: 18.6487 - mse: 527.2847 - val_loss: 13.5943 - val_mae: 13.5943 - val_mse: 245.0009\n",
      "Epoch 138/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 16.1492 - mae: 16.1492 - mse: 393.8941 - val_loss: 12.1868 - val_mae: 12.1868 - val_mse: 293.9243\n",
      "Epoch 139/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 15.6413 - mae: 15.6413 - mse: 339.5737 - val_loss: 12.8747 - val_mae: 12.8747 - val_mse: 336.5687\n",
      "Epoch 140/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 15.8599 - mae: 15.8599 - mse: 341.6983 - val_loss: 12.2115 - val_mae: 12.2115 - val_mse: 309.6343\n",
      "Epoch 141/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 15.8500 - mae: 15.8500 - mse: 344.5075 - val_loss: 12.0734 - val_mae: 12.0734 - val_mse: 286.0773\n",
      "Epoch 142/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 15.5973 - mae: 15.5973 - mse: 337.3018 - val_loss: 12.2638 - val_mae: 12.2638 - val_mse: 258.4117\n",
      "Epoch 143/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 15.4847 - mae: 15.4847 - mse: 353.6044 - val_loss: 13.4838 - val_mae: 13.4838 - val_mse: 232.3316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 144/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 15.8762 - mae: 15.8762 - mse: 388.2766 - val_loss: 12.0805 - val_mae: 12.0805 - val_mse: 278.8721\n",
      "Epoch 145/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 15.2805 - mae: 15.2805 - mse: 329.5874 - val_loss: 14.6405 - val_mae: 14.6405 - val_mse: 396.3718\n",
      "Epoch 146/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 15.9406 - mae: 15.9406 - mse: 347.7302 - val_loss: 13.2603 - val_mae: 13.2603 - val_mse: 346.3864\n",
      "Epoch 147/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 15.4848 - mae: 15.4848 - mse: 328.3020 - val_loss: 12.5564 - val_mae: 12.5564 - val_mse: 254.8651\n",
      "Epoch 148/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 15.1355 - mae: 15.1355 - mse: 345.7443 - val_loss: 12.4458 - val_mae: 12.4458 - val_mse: 275.9726\n",
      "Epoch 149/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 15.1566 - mae: 15.1566 - mse: 335.0591 - val_loss: 12.5678 - val_mae: 12.5678 - val_mse: 288.7591\n",
      "Epoch 150/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 15.1213 - mae: 15.1213 - mse: 338.9471 - val_loss: 13.7596 - val_mae: 13.7596 - val_mse: 367.5785\n",
      "Epoch 151/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 15.4600 - mae: 15.4600 - mse: 328.7266 - val_loss: 15.5154 - val_mae: 15.5154 - val_mse: 436.5283\n",
      "Epoch 152/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 15.7080 - mae: 15.7080 - mse: 345.6105 - val_loss: 12.5726 - val_mae: 12.5726 - val_mse: 282.5911\n",
      "Epoch 153/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 15.0137 - mae: 15.0137 - mse: 350.1386 - val_loss: 12.9117 - val_mae: 12.9117 - val_mse: 253.1153\n",
      "Epoch 154/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 14.9526 - mae: 14.9526 - mse: 341.6748 - val_loss: 15.0386 - val_mae: 15.0386 - val_mse: 420.0913\n",
      "Epoch 155/200\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 15.4920 - mae: 15.4920 - mse: 343.7664 - val_loss: 15.1675 - val_mae: 15.1675 - val_mse: 424.6522\n",
      "Epoch 156/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 15.3106 - mae: 15.3106 - mse: 332.0480 - val_loss: 13.0060 - val_mae: 13.0060 - val_mse: 253.4624\n",
      "Epoch 157/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 15.1630 - mae: 15.1630 - mse: 359.2320 - val_loss: 12.9163 - val_mae: 12.9163 - val_mse: 264.1273\n",
      "Epoch 158/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 14.7674 - mae: 14.7674 - mse: 347.0072 - val_loss: 12.8515 - val_mae: 12.8515 - val_mse: 271.1255\n",
      "Epoch 159/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 14.7008 - mae: 14.7008 - mse: 337.0358 - val_loss: 12.8132 - val_mae: 12.8132 - val_mse: 278.7924\n",
      "Epoch 160/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 14.6301 - mae: 14.6301 - mse: 330.1978 - val_loss: 12.7601 - val_mae: 12.7601 - val_mse: 294.0938\n",
      "Epoch 161/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 14.8245 - mae: 14.8245 - mse: 339.2177 - val_loss: 13.1319 - val_mae: 13.1319 - val_mse: 343.5859\n",
      "Epoch 162/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 14.8631 - mae: 14.8631 - mse: 320.8558 - val_loss: 15.3157 - val_mae: 15.3157 - val_mse: 428.2024\n",
      "Epoch 163/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 16.1009 - mae: 16.1009 - mse: 353.4707 - val_loss: 13.0719 - val_mae: 13.0719 - val_mse: 338.1144\n",
      "Epoch 164/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 14.7559 - mae: 14.7559 - mse: 318.2318 - val_loss: 12.8061 - val_mae: 12.8061 - val_mse: 302.0072\n",
      "Epoch 165/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 14.6938 - mae: 14.6938 - mse: 321.4497 - val_loss: 12.7981 - val_mae: 12.7981 - val_mse: 266.6495\n",
      "Epoch 166/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 14.3554 - mae: 14.3554 - mse: 327.6847 - val_loss: 13.8348 - val_mae: 13.8348 - val_mse: 368.2038\n",
      "Epoch 167/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 15.1203 - mae: 15.1203 - mse: 327.5312 - val_loss: 12.7480 - val_mae: 12.7480 - val_mse: 285.0015\n",
      "Epoch 168/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 15.8314 - mae: 15.8314 - mse: 381.2167 - val_loss: 12.7878 - val_mae: 12.7878 - val_mse: 273.0074\n",
      "Epoch 169/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 14.3681 - mae: 14.3681 - mse: 337.5779 - val_loss: 21.2221 - val_mae: 21.2221 - val_mse: 690.1638\n",
      "Epoch 170/200\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 17.2388 - mae: 17.2388 - mse: 435.0651 - val_loss: 13.9337 - val_mae: 13.9337 - val_mse: 372.2681\n",
      "Epoch 171/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 14.9959 - mae: 14.9959 - mse: 355.0959 - val_loss: 12.9205 - val_mae: 12.9205 - val_mse: 253.7841\n",
      "Epoch 172/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 14.1419 - mae: 14.1419 - mse: 322.6952 - val_loss: 25.9103 - val_mae: 25.9103 - val_mse: 907.0733\n",
      "Epoch 173/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 20.3928 - mae: 20.3928 - mse: 596.2900 - val_loss: 24.7266 - val_mae: 24.7266 - val_mse: 844.6689\n",
      "Epoch 174/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 19.7539 - mae: 19.7539 - mse: 534.3409 - val_loss: 15.6242 - val_mae: 15.6242 - val_mse: 300.5519\n",
      "Epoch 175/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 19.8319 - mae: 19.8319 - mse: 589.7460 - val_loss: 12.9095 - val_mae: 12.9095 - val_mse: 226.1472\n",
      "Epoch 176/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 17.2788 - mae: 17.2788 - mse: 459.0840 - val_loss: 26.9822 - val_mae: 26.9822 - val_mse: 932.8601\n",
      "Epoch 177/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 20.4911 - mae: 20.4911 - mse: 596.9321 - val_loss: 12.0660 - val_mae: 12.0660 - val_mse: 241.7884\n",
      "Epoch 178/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 16.3517 - mae: 16.3517 - mse: 387.6926 - val_loss: 15.8298 - val_mae: 15.8298 - val_mse: 326.2866\n",
      "Epoch 179/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 19.3458 - mae: 19.3458 - mse: 558.1979 - val_loss: 12.0176 - val_mae: 12.0176 - val_mse: 241.6643\n",
      "Epoch 180/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 14.5277 - mae: 14.5277 - mse: 296.2876 - val_loss: 15.1002 - val_mae: 15.1002 - val_mse: 398.3908\n",
      "Epoch 181/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 15.6438 - mae: 15.6438 - mse: 330.0850 - val_loss: 12.7236 - val_mae: 12.7236 - val_mse: 291.1262\n",
      "Epoch 182/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 14.4960 - mae: 14.4960 - mse: 294.8160 - val_loss: 14.0459 - val_mae: 14.0459 - val_mse: 241.9719\n",
      "Epoch 183/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 18.2221 - mae: 18.2221 - mse: 502.8542 - val_loss: 12.7143 - val_mae: 12.7143 - val_mse: 212.8901\n",
      "Epoch 184/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 15.2592 - mae: 15.2592 - mse: 332.3338 - val_loss: 23.7071 - val_mae: 23.7071 - val_mse: 763.7777\n",
      "Epoch 185/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 18.7064 - mae: 18.7064 - mse: 516.0897 - val_loss: 13.1537 - val_mae: 13.1537 - val_mse: 314.5266\n",
      "Epoch 186/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 14.5327 - mae: 14.5327 - mse: 283.4438 - val_loss: 14.8235 - val_mae: 14.8235 - val_mse: 273.1524\n",
      "Epoch 187/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 17.7856 - mae: 17.7856 - mse: 486.6704 - val_loss: 13.6740 - val_mae: 13.6740 - val_mse: 355.2063\n",
      "Epoch 188/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 15.7837 - mae: 15.7837 - mse: 358.1770 - val_loss: 23.5793 - val_mae: 23.5793 - val_mse: 777.8599\n",
      "Epoch 189/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 17.8473 - mae: 17.8473 - mse: 469.0435 - val_loss: 12.6921 - val_mae: 12.6921 - val_mse: 218.7342\n",
      "Epoch 190/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 15.0201 - mae: 15.0201 - mse: 346.7670 - val_loss: 12.6931 - val_mae: 12.6931 - val_mse: 219.3927\n",
      "Epoch 191/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 14.5476 - mae: 14.5476 - mse: 316.5929 - val_loss: 13.9555 - val_mae: 13.9555 - val_mse: 357.6788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 192/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 15.5867 - mae: 15.5867 - mse: 331.8222 - val_loss: 14.1518 - val_mae: 14.1518 - val_mse: 367.1736\n",
      "Epoch 193/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 14.8468 - mae: 14.8468 - mse: 314.7085 - val_loss: 14.8872 - val_mae: 14.8872 - val_mse: 399.7312\n",
      "Epoch 194/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 14.3469 - mae: 14.3469 - mse: 303.1079 - val_loss: 13.6003 - val_mae: 13.6003 - val_mse: 235.6586\n",
      "Epoch 195/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 15.9618 - mae: 15.9618 - mse: 383.3762 - val_loss: 14.3596 - val_mae: 14.3596 - val_mse: 248.0116\n",
      "Epoch 196/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 16.7117 - mae: 16.7117 - mse: 415.6179 - val_loss: 13.5125 - val_mae: 13.5125 - val_mse: 350.2792\n",
      "Epoch 197/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 14.1273 - mae: 14.1273 - mse: 304.4883 - val_loss: 17.0840 - val_mae: 17.0840 - val_mse: 501.5056\n",
      "Epoch 198/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 15.2540 - mae: 15.2540 - mse: 345.9101 - val_loss: 12.7246 - val_mae: 12.7246 - val_mse: 287.6309\n",
      "Epoch 199/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 14.2575 - mae: 14.2575 - mse: 294.5163 - val_loss: 13.0943 - val_mae: 13.0943 - val_mse: 232.3616\n",
      "Epoch 200/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 14.5168 - mae: 14.5168 - mse: 325.5142 - val_loss: 16.6612 - val_mae: 16.6612 - val_mse: 479.5851\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss = \"mae\", optimizer=tf.keras.optimizers.Adam(), \n",
    "              metrics=[\"mae\",\"mse\"])\n",
    "history = model.fit(final_x_zeros[:43],final_y_zeros[:43],\n",
    "                    epochs = 200,\n",
    "                    validation_data=(final_x_zeros[43:],final_y_zeros[43:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "172855c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step - loss: 16.6612 - mae: 16.6612 - mse: 479.5851\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[16.661226272583008, 16.661226272583008, 479.5850830078125]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(final_x_zeros[43:],final_y_zeros[43:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2927c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: conv1d_icici\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"conv1d_icici\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a845f346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([831.8659 , 829.67224, 826.1362 , 813.6705 , 814.49286, 813.731  ,\n",
       "       815.881  , 815.4066 , 808.57196, 799.2481 , 797.8554 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = tf.squeeze(model.predict(final_x_zeros[43:])).numpy()\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d5cee5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d094d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce03625",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c78527",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c863ed5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a2d2d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec45c636",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d098307",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ws_env",
   "language": "python",
   "name": "ws_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
