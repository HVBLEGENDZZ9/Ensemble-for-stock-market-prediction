{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07ab688f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7732669c",
   "metadata": {},
   "source": [
    "# PREPROCESSING THE DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ada804c",
   "metadata": {},
   "source": [
    "#### DATASETS:\n",
    "* itc_final_posts (excel) - money control posts divided into spam and legit\n",
    "* ITC.NS (excel) - ITC stock market data collected from yahoo finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4133adde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-01-31</th>\n",
       "      <td>203.766663</td>\n",
       "      <td>206.266663</td>\n",
       "      <td>202.066666</td>\n",
       "      <td>205.100006</td>\n",
       "      <td>162.141357</td>\n",
       "      <td>13790790.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-02-01</th>\n",
       "      <td>205.166672</td>\n",
       "      <td>206.633331</td>\n",
       "      <td>203.033340</td>\n",
       "      <td>206.266663</td>\n",
       "      <td>163.063675</td>\n",
       "      <td>9906601.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-02-04</th>\n",
       "      <td>206.266663</td>\n",
       "      <td>207.266663</td>\n",
       "      <td>204.166672</td>\n",
       "      <td>204.833328</td>\n",
       "      <td>161.930557</td>\n",
       "      <td>11610360.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-02-05</th>\n",
       "      <td>203.333328</td>\n",
       "      <td>204.466660</td>\n",
       "      <td>201.333328</td>\n",
       "      <td>201.633331</td>\n",
       "      <td>159.400803</td>\n",
       "      <td>7980306.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-02-06</th>\n",
       "      <td>202.233337</td>\n",
       "      <td>203.933334</td>\n",
       "      <td>200.733337</td>\n",
       "      <td>203.233337</td>\n",
       "      <td>160.665695</td>\n",
       "      <td>12331005.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Open        High         Low       Close   Adj Close  \\\n",
       "Date                                                                     \n",
       "2013-01-31  203.766663  206.266663  202.066666  205.100006  162.141357   \n",
       "2013-02-01  205.166672  206.633331  203.033340  206.266663  163.063675   \n",
       "2013-02-04  206.266663  207.266663  204.166672  204.833328  161.930557   \n",
       "2013-02-05  203.333328  204.466660  201.333328  201.633331  159.400803   \n",
       "2013-02-06  202.233337  203.933334  200.733337  203.233337  160.665695   \n",
       "\n",
       "                Volume  \n",
       "Date                    \n",
       "2013-01-31  13790790.0  \n",
       "2013-02-01   9906601.0  \n",
       "2013-02-04  11610360.0  \n",
       "2013-02-05   7980306.0  \n",
       "2013-02-06  12331005.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_itc = pd.read_csv(r\"itc/ITC.NS.csv\",\n",
    "                     parse_dates = [\"Date\"],\n",
    "                     index_col = [\"Date\"])\n",
    "df_itc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f35a305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Messages</th>\n",
       "      <th>Time Stamp</th>\n",
       "      <th>Spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NO ISSUE IN MANAGEMENT THEY ARE FOCUSED ON BUS...</td>\n",
       "      <td>2022-02-10 19:23:04</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Adani Wilmar up 72% in just 3 sessions from li...</td>\n",
       "      <td>2022-02-10 16:12:41</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Sure, one day this company will be taken over ...</td>\n",
       "      <td>2022-02-10 17:53:08</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Timeless financial quotes give investors a bet...</td>\n",
       "      <td>2022-02-10 19:43:23</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Positive Rally in Nifty, No Rally in ITC and N...</td>\n",
       "      <td>2022-02-10 19:28:39</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           Messages  \\\n",
       "0           0  NO ISSUE IN MANAGEMENT THEY ARE FOCUSED ON BUS...   \n",
       "1           1  Adani Wilmar up 72% in just 3 sessions from li...   \n",
       "2           2  Sure, one day this company will be taken over ...   \n",
       "3           3  Timeless financial quotes give investors a bet...   \n",
       "4           4  Positive Rally in Nifty, No Rally in ITC and N...   \n",
       "\n",
       "           Time Stamp  Spam  \n",
       "0 2022-02-10 19:23:04   1.0  \n",
       "1 2022-02-10 16:12:41   1.0  \n",
       "2 2022-02-10 17:53:08   1.0  \n",
       "3 2022-02-10 19:43:23   1.0  \n",
       "4 2022-02-10 19:28:39   0.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_itc_posts = pd.read_excel(\"itc/itc_final_posts.xlsx\")\n",
    "df_itc_posts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5294890",
   "metadata": {},
   "source": [
    "#### STEPS TO PERFORM:\n",
    "* Removing unnecessary columns\n",
    "* Removing spam posts\n",
    "* Getting the correct dates\n",
    "* Sliding a window of 7 days and preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fae0d01",
   "metadata": {},
   "source": [
    "#### Removing unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "741eb60e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Messages</th>\n",
       "      <th>Time Stamp</th>\n",
       "      <th>Spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NO ISSUE IN MANAGEMENT THEY ARE FOCUSED ON BUS...</td>\n",
       "      <td>2022-02-10 19:23:04</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adani Wilmar up 72% in just 3 sessions from li...</td>\n",
       "      <td>2022-02-10 16:12:41</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sure, one day this company will be taken over ...</td>\n",
       "      <td>2022-02-10 17:53:08</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Timeless financial quotes give investors a bet...</td>\n",
       "      <td>2022-02-10 19:43:23</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Positive Rally in Nifty, No Rally in ITC and N...</td>\n",
       "      <td>2022-02-10 19:28:39</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Messages          Time Stamp  Spam\n",
       "0  NO ISSUE IN MANAGEMENT THEY ARE FOCUSED ON BUS... 2022-02-10 19:23:04   1.0\n",
       "1  Adani Wilmar up 72% in just 3 sessions from li... 2022-02-10 16:12:41   1.0\n",
       "2  Sure, one day this company will be taken over ... 2022-02-10 17:53:08   1.0\n",
       "3  Timeless financial quotes give investors a bet... 2022-02-10 19:43:23   1.0\n",
       "4  Positive Rally in Nifty, No Rally in ITC and N... 2022-02-10 19:28:39   0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_itc_posts.drop([\"Unnamed: 0\"], axis=1,inplace=True)\n",
    "df_itc_posts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6730803a",
   "metadata": {},
   "source": [
    "#### Removing spam posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2b3f550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Messages</th>\n",
       "      <th>Time Stamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Positive Rally in Nifty, No Rally in ITC and N...</td>\n",
       "      <td>2022-02-10 19:28:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Strong Long Term Fundamental Strength with an ...</td>\n",
       "      <td>2022-02-10 15:59:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>after results today some kind up sides movemen...</td>\n",
       "      <td>2022-02-10 15:43:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>do not misguide people in this common forum ev...</td>\n",
       "      <td>2022-02-10 15:42:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Positive Rally in Nifty, No Rally in ITC and N...</td>\n",
       "      <td>2022-02-10 19:28:39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Messages          Time Stamp\n",
       "4   Positive Rally in Nifty, No Rally in ITC and N... 2022-02-10 19:28:39\n",
       "7   Strong Long Term Fundamental Strength with an ... 2022-02-10 15:59:02\n",
       "8   after results today some kind up sides movemen... 2022-02-10 15:43:25\n",
       "9   do not misguide people in this common forum ev... 2022-02-10 15:42:33\n",
       "14  Positive Rally in Nifty, No Rally in ITC and N... 2022-02-10 19:28:39"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_itc_posts = df_itc_posts[df_itc_posts.Spam==0.0]\n",
    "df_itc_posts.drop([\"Spam\"],axis=1,inplace=True)\n",
    "df_itc_posts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041c9fb4",
   "metadata": {},
   "source": [
    "#### Getting the correct dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b6adbe",
   "metadata": {},
   "source": [
    "* ITC.NS has stock market data from 31-01-2013 to 11-02-2022 and itc_final_posts has data from 12-11-2021 to 10-02-2022.\n",
    "* 12-11-2021 has only one post. Hence, the price on 14-11-2021 can be predicted using posts from 13-11-2021. But 14-11-2021 is   a sunday. Hence, the price from 15-11-2021 can be predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239a5e36",
   "metadata": {},
   "source": [
    "#### Sliding a window of 7 days and preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb127f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BBL_20_2.0</th>\n",
       "      <th>BBM_20_2.0</th>\n",
       "      <th>BBU_20_2.0</th>\n",
       "      <th>BBB_20_2.0</th>\n",
       "      <th>BBP_20_2.0</th>\n",
       "      <th>MACDh_12_26_9</th>\n",
       "      <th>RSI_14</th>\n",
       "      <th>AROONOSC_14</th>\n",
       "      <th>Close + 1</th>\n",
       "      <th>BBL_20_2.0 + 1</th>\n",
       "      <th>...</th>\n",
       "      <th>AROONOSC_14 + 6</th>\n",
       "      <th>Close + 7</th>\n",
       "      <th>BBL_20_2.0 + 7</th>\n",
       "      <th>BBM_20_2.0 + 7</th>\n",
       "      <th>BBU_20_2.0 + 7</th>\n",
       "      <th>BBB_20_2.0 + 7</th>\n",
       "      <th>BBP_20_2.0 + 7</th>\n",
       "      <th>MACDh_12_26_9 + 7</th>\n",
       "      <th>RSI_14 + 7</th>\n",
       "      <th>AROONOSC_14 + 7</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-04-01</th>\n",
       "      <td>191.946030</td>\n",
       "      <td>200.301666</td>\n",
       "      <td>208.657303</td>\n",
       "      <td>8.343053</td>\n",
       "      <td>0.769180</td>\n",
       "      <td>0.826599</td>\n",
       "      <td>59.372993</td>\n",
       "      <td>92.857140</td>\n",
       "      <td>206.300003</td>\n",
       "      <td>191.646591</td>\n",
       "      <td>...</td>\n",
       "      <td>85.714287</td>\n",
       "      <td>204.199997</td>\n",
       "      <td>190.924728</td>\n",
       "      <td>197.585007</td>\n",
       "      <td>204.245285</td>\n",
       "      <td>6.741683</td>\n",
       "      <td>0.996601</td>\n",
       "      <td>1.614461</td>\n",
       "      <td>64.485970</td>\n",
       "      <td>78.571426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-04-02</th>\n",
       "      <td>192.759949</td>\n",
       "      <td>200.831665</td>\n",
       "      <td>208.903381</td>\n",
       "      <td>8.038295</td>\n",
       "      <td>0.743752</td>\n",
       "      <td>0.702402</td>\n",
       "      <td>59.263271</td>\n",
       "      <td>78.571426</td>\n",
       "      <td>204.800003</td>\n",
       "      <td>191.946030</td>\n",
       "      <td>...</td>\n",
       "      <td>85.714287</td>\n",
       "      <td>204.800003</td>\n",
       "      <td>190.519272</td>\n",
       "      <td>197.824997</td>\n",
       "      <td>205.130737</td>\n",
       "      <td>7.386054</td>\n",
       "      <td>0.977365</td>\n",
       "      <td>1.593910</td>\n",
       "      <td>65.764305</td>\n",
       "      <td>85.714287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-04-03</th>\n",
       "      <td>193.529572</td>\n",
       "      <td>201.335007</td>\n",
       "      <td>209.140427</td>\n",
       "      <td>7.753670</td>\n",
       "      <td>0.739043</td>\n",
       "      <td>0.601454</td>\n",
       "      <td>59.979939</td>\n",
       "      <td>78.571426</td>\n",
       "      <td>204.766663</td>\n",
       "      <td>192.759949</td>\n",
       "      <td>...</td>\n",
       "      <td>85.714287</td>\n",
       "      <td>203.333328</td>\n",
       "      <td>190.415604</td>\n",
       "      <td>198.104996</td>\n",
       "      <td>205.794388</td>\n",
       "      <td>7.762946</td>\n",
       "      <td>0.839970</td>\n",
       "      <td>1.414442</td>\n",
       "      <td>60.072098</td>\n",
       "      <td>85.714287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-04-04</th>\n",
       "      <td>194.260071</td>\n",
       "      <td>201.566666</td>\n",
       "      <td>208.873260</td>\n",
       "      <td>7.249799</td>\n",
       "      <td>0.372261</td>\n",
       "      <td>0.157366</td>\n",
       "      <td>44.797306</td>\n",
       "      <td>71.428574</td>\n",
       "      <td>205.066666</td>\n",
       "      <td>193.529572</td>\n",
       "      <td>...</td>\n",
       "      <td>85.714287</td>\n",
       "      <td>203.733337</td>\n",
       "      <td>190.666824</td>\n",
       "      <td>198.558334</td>\n",
       "      <td>206.449844</td>\n",
       "      <td>7.948807</td>\n",
       "      <td>0.827884</td>\n",
       "      <td>1.260681</td>\n",
       "      <td>61.061985</td>\n",
       "      <td>85.714287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-04-05</th>\n",
       "      <td>195.124146</td>\n",
       "      <td>201.708328</td>\n",
       "      <td>208.292526</td>\n",
       "      <td>6.528425</td>\n",
       "      <td>-0.070179</td>\n",
       "      <td>-0.487384</td>\n",
       "      <td>35.015064</td>\n",
       "      <td>-35.714287</td>\n",
       "      <td>199.699997</td>\n",
       "      <td>194.260071</td>\n",
       "      <td>...</td>\n",
       "      <td>92.857140</td>\n",
       "      <td>201.600006</td>\n",
       "      <td>191.053146</td>\n",
       "      <td>198.883331</td>\n",
       "      <td>206.713516</td>\n",
       "      <td>7.874147</td>\n",
       "      <td>0.673474</td>\n",
       "      <td>0.966548</td>\n",
       "      <td>53.451057</td>\n",
       "      <td>85.714287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-07</th>\n",
       "      <td>210.069550</td>\n",
       "      <td>222.669998</td>\n",
       "      <td>235.270447</td>\n",
       "      <td>11.317599</td>\n",
       "      <td>0.798799</td>\n",
       "      <td>1.845161</td>\n",
       "      <td>60.010979</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>234.300003</td>\n",
       "      <td>209.845398</td>\n",
       "      <td>...</td>\n",
       "      <td>-35.714287</td>\n",
       "      <td>214.600006</td>\n",
       "      <td>212.705643</td>\n",
       "      <td>219.389999</td>\n",
       "      <td>226.074356</td>\n",
       "      <td>6.093582</td>\n",
       "      <td>0.141701</td>\n",
       "      <td>-0.658273</td>\n",
       "      <td>40.944489</td>\n",
       "      <td>-35.714287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-08</th>\n",
       "      <td>209.922806</td>\n",
       "      <td>223.067505</td>\n",
       "      <td>236.212204</td>\n",
       "      <td>11.785402</td>\n",
       "      <td>0.811247</td>\n",
       "      <td>1.664053</td>\n",
       "      <td>61.201462</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>230.199997</td>\n",
       "      <td>210.069550</td>\n",
       "      <td>...</td>\n",
       "      <td>-35.714287</td>\n",
       "      <td>217.600006</td>\n",
       "      <td>212.912766</td>\n",
       "      <td>219.470001</td>\n",
       "      <td>226.027237</td>\n",
       "      <td>5.975522</td>\n",
       "      <td>0.357410</td>\n",
       "      <td>-0.431872</td>\n",
       "      <td>46.709850</td>\n",
       "      <td>-35.714287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-09</th>\n",
       "      <td>209.990723</td>\n",
       "      <td>223.477493</td>\n",
       "      <td>236.964279</td>\n",
       "      <td>12.069926</td>\n",
       "      <td>0.747372</td>\n",
       "      <td>1.382966</td>\n",
       "      <td>59.212685</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>231.250000</td>\n",
       "      <td>209.922806</td>\n",
       "      <td>...</td>\n",
       "      <td>28.571428</td>\n",
       "      <td>220.199997</td>\n",
       "      <td>213.046463</td>\n",
       "      <td>219.577499</td>\n",
       "      <td>226.108536</td>\n",
       "      <td>5.948732</td>\n",
       "      <td>0.547657</td>\n",
       "      <td>-0.090360</td>\n",
       "      <td>51.160023</td>\n",
       "      <td>-35.714287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-10</th>\n",
       "      <td>209.942627</td>\n",
       "      <td>223.952499</td>\n",
       "      <td>237.962372</td>\n",
       "      <td>12.511462</td>\n",
       "      <td>0.796131</td>\n",
       "      <td>1.255015</td>\n",
       "      <td>61.766991</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>230.149994</td>\n",
       "      <td>209.990723</td>\n",
       "      <td>...</td>\n",
       "      <td>35.714287</td>\n",
       "      <td>227.750000</td>\n",
       "      <td>212.579086</td>\n",
       "      <td>220.009995</td>\n",
       "      <td>227.440918</td>\n",
       "      <td>6.755075</td>\n",
       "      <td>1.020797</td>\n",
       "      <td>0.621503</td>\n",
       "      <td>61.273457</td>\n",
       "      <td>28.571428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-11</th>\n",
       "      <td>209.886292</td>\n",
       "      <td>224.377502</td>\n",
       "      <td>238.868713</td>\n",
       "      <td>12.916818</td>\n",
       "      <td>0.778531</td>\n",
       "      <td>1.105639</td>\n",
       "      <td>62.010990</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>232.250000</td>\n",
       "      <td>209.942627</td>\n",
       "      <td>...</td>\n",
       "      <td>42.857143</td>\n",
       "      <td>232.149994</td>\n",
       "      <td>211.479935</td>\n",
       "      <td>220.604996</td>\n",
       "      <td>229.730072</td>\n",
       "      <td>8.272765</td>\n",
       "      <td>1.132598</td>\n",
       "      <td>1.325734</td>\n",
       "      <td>65.727539</td>\n",
       "      <td>35.714287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2149 rows × 71 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            BBL_20_2.0  BBM_20_2.0  BBU_20_2.0  BBB_20_2.0  BBP_20_2.0  \\\n",
       "Date                                                                     \n",
       "2013-04-01  191.946030  200.301666  208.657303    8.343053    0.769180   \n",
       "2013-04-02  192.759949  200.831665  208.903381    8.038295    0.743752   \n",
       "2013-04-03  193.529572  201.335007  209.140427    7.753670    0.739043   \n",
       "2013-04-04  194.260071  201.566666  208.873260    7.249799    0.372261   \n",
       "2013-04-05  195.124146  201.708328  208.292526    6.528425   -0.070179   \n",
       "...                ...         ...         ...         ...         ...   \n",
       "2022-02-07  210.069550  222.669998  235.270447   11.317599    0.798799   \n",
       "2022-02-08  209.922806  223.067505  236.212204   11.785402    0.811247   \n",
       "2022-02-09  209.990723  223.477493  236.964279   12.069926    0.747372   \n",
       "2022-02-10  209.942627  223.952499  237.962372   12.511462    0.796131   \n",
       "2022-02-11  209.886292  224.377502  238.868713   12.916818    0.778531   \n",
       "\n",
       "            MACDh_12_26_9     RSI_14  AROONOSC_14   Close + 1  BBL_20_2.0 + 1  \\\n",
       "Date                                                                            \n",
       "2013-04-01       0.826599  59.372993    92.857140  206.300003      191.646591   \n",
       "2013-04-02       0.702402  59.263271    78.571426  204.800003      191.946030   \n",
       "2013-04-03       0.601454  59.979939    78.571426  204.766663      192.759949   \n",
       "2013-04-04       0.157366  44.797306    71.428574  205.066666      193.529572   \n",
       "2013-04-05      -0.487384  35.015064   -35.714287  199.699997      194.260071   \n",
       "...                   ...        ...          ...         ...             ...   \n",
       "2022-02-07       1.845161  60.010979    50.000000  234.300003      209.845398   \n",
       "2022-02-08       1.664053  61.201462    50.000000  230.199997      210.069550   \n",
       "2022-02-09       1.382966  59.212685    50.000000  231.250000      209.922806   \n",
       "2022-02-10       1.255015  61.766991    50.000000  230.149994      209.990723   \n",
       "2022-02-11       1.105639  62.010990    50.000000  232.250000      209.942627   \n",
       "\n",
       "            ...  AROONOSC_14 + 6   Close + 7  BBL_20_2.0 + 7  BBM_20_2.0 + 7  \\\n",
       "Date        ...                                                                \n",
       "2013-04-01  ...        85.714287  204.199997      190.924728      197.585007   \n",
       "2013-04-02  ...        85.714287  204.800003      190.519272      197.824997   \n",
       "2013-04-03  ...        85.714287  203.333328      190.415604      198.104996   \n",
       "2013-04-04  ...        85.714287  203.733337      190.666824      198.558334   \n",
       "2013-04-05  ...        92.857140  201.600006      191.053146      198.883331   \n",
       "...         ...              ...         ...             ...             ...   \n",
       "2022-02-07  ...       -35.714287  214.600006      212.705643      219.389999   \n",
       "2022-02-08  ...       -35.714287  217.600006      212.912766      219.470001   \n",
       "2022-02-09  ...        28.571428  220.199997      213.046463      219.577499   \n",
       "2022-02-10  ...        35.714287  227.750000      212.579086      220.009995   \n",
       "2022-02-11  ...        42.857143  232.149994      211.479935      220.604996   \n",
       "\n",
       "            BBU_20_2.0 + 7  BBB_20_2.0 + 7  BBP_20_2.0 + 7  MACDh_12_26_9 + 7  \\\n",
       "Date                                                                            \n",
       "2013-04-01      204.245285        6.741683        0.996601           1.614461   \n",
       "2013-04-02      205.130737        7.386054        0.977365           1.593910   \n",
       "2013-04-03      205.794388        7.762946        0.839970           1.414442   \n",
       "2013-04-04      206.449844        7.948807        0.827884           1.260681   \n",
       "2013-04-05      206.713516        7.874147        0.673474           0.966548   \n",
       "...                    ...             ...             ...                ...   \n",
       "2022-02-07      226.074356        6.093582        0.141701          -0.658273   \n",
       "2022-02-08      226.027237        5.975522        0.357410          -0.431872   \n",
       "2022-02-09      226.108536        5.948732        0.547657          -0.090360   \n",
       "2022-02-10      227.440918        6.755075        1.020797           0.621503   \n",
       "2022-02-11      229.730072        8.272765        1.132598           1.325734   \n",
       "\n",
       "            RSI_14 + 7  AROONOSC_14 + 7  \n",
       "Date                                     \n",
       "2013-04-01   64.485970        78.571426  \n",
       "2013-04-02   65.764305        85.714287  \n",
       "2013-04-03   60.072098        85.714287  \n",
       "2013-04-04   61.061985        85.714287  \n",
       "2013-04-05   53.451057        85.714287  \n",
       "...                ...              ...  \n",
       "2022-02-07   40.944489       -35.714287  \n",
       "2022-02-08   46.709850       -35.714287  \n",
       "2022-02-09   51.160023       -35.714287  \n",
       "2022-02-10   61.273457        28.571428  \n",
       "2022-02-11   65.727539        35.714287  \n",
       "\n",
       "[2149 rows x 71 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stock_helper import prepare_data\n",
    "x,y = prepare_data(df_itc)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8e3e742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "2013-04-01    204.800003\n",
       "2013-04-02    204.766663\n",
       "2013-04-03    205.066666\n",
       "2013-04-04    199.699997\n",
       "2013-04-05    194.199997\n",
       "                 ...    \n",
       "2022-02-07    230.199997\n",
       "2022-02-08    231.250000\n",
       "2022-02-09    230.149994\n",
       "2022-02-10    232.250000\n",
       "2022-02-11    232.449997\n",
       "Name: Close, Length: 2149, dtype: float32"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43a2df3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_x = x[np.datetime64(\"2021-11-13\"):]\n",
    "final_y = y[np.datetime64(\"2021-11-13\"):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c104867a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Messages</th>\n",
       "      <th>Time Stamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2045</th>\n",
       "      <td>ITC safe investment bet lowest PE in FMCG sect...</td>\n",
       "      <td>2021-11-12 21:31:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2042</th>\n",
       "      <td>Aravachan, The expected price Target for ITC t...</td>\n",
       "      <td>2021-11-13 18:02:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040</th>\n",
       "      <td>This is only buy for investment purpose for 25...</td>\n",
       "      <td>2021-11-13 18:03:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2037</th>\n",
       "      <td>Tips for Monday....buy ITC at 232 sl 230 and t...</td>\n",
       "      <td>2021-11-13 18:48:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2035</th>\n",
       "      <td>ITC Infotech eyes Rs 3,000 crore revenue mark ...</td>\n",
       "      <td>2021-11-13 19:00:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Positive Rally in Nifty, No Rally in ITC and N...</td>\n",
       "      <td>2022-02-10 19:28:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>do not misguide people in this common forum ev...</td>\n",
       "      <td>2022-02-10 15:42:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>after results today some kind up sides movemen...</td>\n",
       "      <td>2022-02-10 15:43:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Strong Long Term Fundamental Strength with an ...</td>\n",
       "      <td>2022-02-10 15:59:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Positive Rally in Nifty, No Rally in ITC and N...</td>\n",
       "      <td>2022-02-10 19:28:39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>621 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Messages          Time Stamp\n",
       "2045  ITC safe investment bet lowest PE in FMCG sect... 2021-11-12 21:31:26\n",
       "2042  Aravachan, The expected price Target for ITC t... 2021-11-13 18:02:15\n",
       "2040  This is only buy for investment purpose for 25... 2021-11-13 18:03:55\n",
       "2037  Tips for Monday....buy ITC at 232 sl 230 and t... 2021-11-13 18:48:36\n",
       "2035  ITC Infotech eyes Rs 3,000 crore revenue mark ... 2021-11-13 19:00:12\n",
       "...                                                 ...                 ...\n",
       "14    Positive Rally in Nifty, No Rally in ITC and N... 2022-02-10 19:28:39\n",
       "9     do not misguide people in this common forum ev... 2022-02-10 15:42:33\n",
       "8     after results today some kind up sides movemen... 2022-02-10 15:43:25\n",
       "7     Strong Long Term Fundamental Strength with an ... 2022-02-10 15:59:02\n",
       "4     Positive Rally in Nifty, No Rally in ITC and N... 2022-02-10 19:28:39\n",
       "\n",
       "[621 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_itc_posts = df_itc_posts[::-1]\n",
    "df_itc_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17990aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Messages</th>\n",
       "      <th>Time Stamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2030</th>\n",
       "      <td>ITC safe investment bet lowest PE in FMCG sect...</td>\n",
       "      <td>2021-11-14 07:31:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2028</th>\n",
       "      <td>ITC safe investment bet lowest PE in FMCG sect...</td>\n",
       "      <td>2021-11-14 09:20:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025</th>\n",
       "      <td>ITC Infotech is open for inorganic growth. Don...</td>\n",
       "      <td>2021-11-13 23:20:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023</th>\n",
       "      <td>The bonus may be announced by ITC in 2022. So ...</td>\n",
       "      <td>2021-11-14 10:35:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>Technical and financial analysis shows a stron...</td>\n",
       "      <td>2021-11-14 20:39:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Positive Rally in Nifty, No Rally in ITC and N...</td>\n",
       "      <td>2022-02-10 19:28:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>do not misguide people in this common forum ev...</td>\n",
       "      <td>2022-02-10 15:42:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>after results today some kind up sides movemen...</td>\n",
       "      <td>2022-02-10 15:43:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Strong Long Term Fundamental Strength with an ...</td>\n",
       "      <td>2022-02-10 15:59:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Positive Rally in Nifty, No Rally in ITC and N...</td>\n",
       "      <td>2022-02-10 19:28:39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>614 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Messages          Time Stamp\n",
       "2030  ITC safe investment bet lowest PE in FMCG sect... 2021-11-14 07:31:06\n",
       "2028  ITC safe investment bet lowest PE in FMCG sect... 2021-11-14 09:20:37\n",
       "2025  ITC Infotech is open for inorganic growth. Don... 2021-11-13 23:20:56\n",
       "2023  The bonus may be announced by ITC in 2022. So ... 2021-11-14 10:35:23\n",
       "2019  Technical and financial analysis shows a stron... 2021-11-14 20:39:57\n",
       "...                                                 ...                 ...\n",
       "14    Positive Rally in Nifty, No Rally in ITC and N... 2022-02-10 19:28:39\n",
       "9     do not misguide people in this common forum ev... 2022-02-10 15:42:33\n",
       "8     after results today some kind up sides movemen... 2022-02-10 15:43:25\n",
       "7     Strong Long Term Fundamental Strength with an ... 2022-02-10 15:59:02\n",
       "4     Positive Rally in Nifty, No Rally in ITC and N... 2022-02-10 19:28:39\n",
       "\n",
       "[614 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_posts = df_itc_posts[7:]\n",
    "final_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8abf1113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63, 614)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_x),len(final_posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae4ab46",
   "metadata": {},
   "source": [
    "# CALCULATING SENTIMENTS SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3b214f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " inputs (InputLayer)            [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " preprocessor (KerasLayer)      {'input_word_ids':   0           ['inputs[0][0]']                 \n",
      "                                (None, 128),                                                      \n",
      "                                 'input_mask': (Non                                               \n",
      "                                e, 128),                                                          \n",
      "                                 'input_type_ids':                                                \n",
      "                                (None, 128)}                                                      \n",
      "                                                                                                  \n",
      " encoder (KerasLayer)           {'default': (None,   109482241   ['preprocessor[0][0]',           \n",
      "                                768),                             'preprocessor[0][1]',           \n",
      "                                 'sequence_output':               'preprocessor[0][2]']           \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 'pooled_output': (                                               \n",
      "                                None, 768),                                                       \n",
      "                                 'encoder_outputs':                                               \n",
      "                                 [(None, 128, 768),                                               \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768)]}                                               \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 768)          0           ['encoder[0][13]']               \n",
      "                                                                                                  \n",
      " classifier_output (Dense)      (None, 1)            769         ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,483,010\n",
      "Trainable params: 109,483,009\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sent_model = tf.keras.models.load_model(\"final_bert\")\n",
    "sent_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3565a9",
   "metadata": {},
   "source": [
    "#### Removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46e50f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Messages</th>\n",
       "      <th>Time Stamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2030</th>\n",
       "      <td>ITC safe investment bet lowest PE in FMCG sect...</td>\n",
       "      <td>2021-11-14 07:31:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025</th>\n",
       "      <td>ITC Infotech is open for inorganic growth. Don...</td>\n",
       "      <td>2021-11-13 23:20:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023</th>\n",
       "      <td>The bonus may be announced by ITC in 2022. So ...</td>\n",
       "      <td>2021-11-14 10:35:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>Technical and financial analysis shows a stron...</td>\n",
       "      <td>2021-11-14 20:39:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>Itc is biggest beneficiary for boom in sales t...</td>\n",
       "      <td>2021-11-14 20:41:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>after results today some kind up sides movemen...</td>\n",
       "      <td>2022-02-10 15:43:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Strong Long Term Fundamental Strength with an ...</td>\n",
       "      <td>2022-02-10 15:59:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>M Ambani Or G Adani should buy controlling sta...</td>\n",
       "      <td>2022-02-10 17:16:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Positive Rally in Nifty, No Rally in ITC and N...</td>\n",
       "      <td>2022-02-10 19:28:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>do not misguide people in this common forum ev...</td>\n",
       "      <td>2022-02-10 15:42:33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>593 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Messages          Time Stamp\n",
       "2030  ITC safe investment bet lowest PE in FMCG sect... 2021-11-14 07:31:06\n",
       "2025  ITC Infotech is open for inorganic growth. Don... 2021-11-13 23:20:56\n",
       "2023  The bonus may be announced by ITC in 2022. So ... 2021-11-14 10:35:23\n",
       "2019  Technical and financial analysis shows a stron... 2021-11-14 20:39:57\n",
       "2018  Itc is biggest beneficiary for boom in sales t... 2021-11-14 20:41:18\n",
       "...                                                 ...                 ...\n",
       "38    after results today some kind up sides movemen... 2022-02-10 15:43:25\n",
       "37    Strong Long Term Fundamental Strength with an ... 2022-02-10 15:59:02\n",
       "36    M Ambani Or G Adani should buy controlling sta... 2022-02-10 17:16:29\n",
       "34    Positive Rally in Nifty, No Rally in ITC and N... 2022-02-10 19:28:39\n",
       "19    do not misguide people in this common forum ev... 2022-02-10 15:42:33\n",
       "\n",
       "[593 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_posts.drop_duplicates(subset=['Messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c304088",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.9444242715835571\n",
      "Score: 0.9444242715835571\n",
      "Score: 0.9895212650299072\n",
      "Score: 0.9904130101203918\n",
      "Score: 0.9859876036643982\n",
      "Score: 0.9878939986228943\n",
      "Score: 0.9913119077682495\n",
      "Score: 0.56297367811203\n",
      "Score: 0.9855405688285828\n",
      "Score: 0.9444242715835571\n",
      "Score: 0.9900887608528137\n",
      "Score: 0.9864593744277954\n",
      "Score: 0.8831420540809631\n",
      "Score: 0.9906072616577148\n",
      "Score: 0.9881131052970886\n",
      "Score: 0.9378788471221924\n",
      "Score: 0.9728671312332153\n",
      "Score: 0.9444242715835571\n",
      "Score: 0.9893563389778137\n",
      "Score: 0.7976952791213989\n",
      "Score: 0.9805727005004883\n",
      "Score: 0.8743585348129272\n",
      "Score: 0.9097331762313843\n",
      "Score: 0.12076428532600403\n",
      "Score: 0.9446114897727966\n",
      "Score: 0.9894975423812866\n",
      "Score: 0.938491940498352\n",
      "Score: 0.9903286099433899\n",
      "Score: 0.9900509715080261\n",
      "Score: 0.9887735843658447\n",
      "Score: 0.9857649207115173\n",
      "Score: 0.8578930497169495\n",
      "Score: 0.9444242715835571\n",
      "Score: 0.9726449847221375\n",
      "Score: 0.9897757768630981\n",
      "Score: 0.9899868369102478\n",
      "Score: 0.9444242715835571\n",
      "Score: 0.9891798496246338\n",
      "Score: 0.09316136687994003\n",
      "Score: 0.9854835867881775\n",
      "Score: 0.9909731149673462\n",
      "Score: 0.9574634432792664\n",
      "Score: 0.9740139842033386\n",
      "Score: 0.9874873161315918\n",
      "Score: 0.9797950983047485\n",
      "Score: 0.8797633051872253\n",
      "Score: 0.034039709717035294\n",
      "Score: 0.11885513365268707\n",
      "Score: 0.9810054302215576\n",
      "Score: 0.033667173236608505\n",
      "Score: 0.9908764958381653\n",
      "Score: 0.976141095161438\n",
      "Score: 0.7937531471252441\n",
      "Score: 0.9381272196769714\n",
      "Score: 0.8782219290733337\n",
      "Score: 0.8539999127388\n",
      "Score: 0.8465670943260193\n",
      "Score: 0.9883180260658264\n",
      "Score: 0.990153968334198\n",
      "Score: 0.7261538505554199\n",
      "Score: 0.8392055630683899\n",
      "Score: 0.9800372123718262\n",
      "Score: 0.9041122198104858\n",
      "Score: 0.984126627445221\n",
      "Score: 0.990711510181427\n",
      "Score: 0.9843929409980774\n",
      "Score: 0.9855497479438782\n",
      "Score: 0.9870378971099854\n",
      "Score: 0.7725666761398315\n",
      "Score: 0.7990037798881531\n",
      "Score: 0.9915281534194946\n",
      "Score: 0.977400004863739\n",
      "Score: 0.9900953769683838\n",
      "Score: 0.01944134756922722\n",
      "Score: 0.9858651161193848\n",
      "Score: 0.989467203617096\n",
      "Score: 0.40818697214126587\n",
      "Score: 0.8322356939315796\n",
      "Score: 0.9898882508277893\n",
      "Score: 0.8465145826339722\n",
      "Score: 0.7453963160514832\n",
      "Score: 0.9902216196060181\n",
      "Score: 0.9834930896759033\n",
      "Score: 0.9765648245811462\n",
      "Score: 0.9875362515449524\n",
      "Score: 0.20056751370429993\n",
      "Score: 0.987548828125\n",
      "Score: 0.9904991984367371\n",
      "Score: 0.9865873456001282\n",
      "Score: 0.9913409352302551\n",
      "Score: 0.9813779592514038\n",
      "Score: 0.9867119789123535\n",
      "Score: 0.9914606809616089\n",
      "Score: 0.06467422097921371\n",
      "Score: 0.9887973666191101\n",
      "Score: 0.9894938468933105\n",
      "Score: 0.9902881383895874\n",
      "Score: 0.9017013311386108\n",
      "Score: 0.9770709276199341\n",
      "Score: 0.805365800857544\n",
      "Score: 0.9195140600204468\n",
      "Score: 0.9871276617050171\n",
      "Score: 0.9481526613235474\n",
      "Score: 0.9730026125907898\n",
      "Score: 0.052236802875995636\n",
      "Score: 0.9899569153785706\n",
      "Score: 0.9853633642196655\n",
      "Score: 0.9807502627372742\n",
      "Score: 0.9644381999969482\n",
      "Score: 0.8388793468475342\n",
      "Score: 0.9863768219947815\n",
      "Score: 0.9906779527664185\n",
      "Score: 0.9783377051353455\n",
      "Score: 0.7972179651260376\n",
      "Score: 0.8948396444320679\n",
      "Score: 0.02628789283335209\n",
      "Score: 0.9818441271781921\n",
      "Score: 0.9732499718666077\n",
      "Score: 0.1498844176530838\n",
      "Score: 0.95797199010849\n",
      "Score: 0.8356274366378784\n",
      "Score: 0.7310393452644348\n",
      "Score: 0.027803031727671623\n",
      "Score: 0.9840335845947266\n",
      "Score: 0.8943436145782471\n",
      "Score: 0.9842901825904846\n",
      "Score: 0.9851846694946289\n",
      "Score: 0.9826692342758179\n",
      "Score: 0.9909013509750366\n",
      "Score: 0.1346222460269928\n",
      "Score: 0.27932730317115784\n",
      "Score: 0.9851771593093872\n",
      "Score: 0.9496397972106934\n",
      "Score: 0.9568877220153809\n",
      "Score: 0.034072812646627426\n",
      "Score: 0.2501829266548157\n",
      "Score: 0.9744501709938049\n",
      "Score: 0.981681764125824\n",
      "Score: 0.9282696843147278\n",
      "Score: 0.4431488811969757\n",
      "Score: 0.9304240942001343\n",
      "Score: 0.9884769916534424\n",
      "Score: 0.9811460971832275\n",
      "Score: 0.9911435842514038\n",
      "Score: 0.9907591342926025\n",
      "Score: 0.9861840605735779\n",
      "Score: 0.9831873774528503\n",
      "Score: 0.8460059762001038\n",
      "Score: 0.07267196476459503\n",
      "Score: 0.9822774529457092\n",
      "Score: 0.1856566220521927\n",
      "Score: 0.9905440807342529\n",
      "Score: 0.9299825429916382\n",
      "Score: 0.6592581868171692\n",
      "Score: 0.9461658596992493\n",
      "Score: 0.9905397891998291\n",
      "Score: 0.9307355880737305\n",
      "Score: 0.9762519598007202\n",
      "Score: 0.9882189035415649\n",
      "Score: 0.9909526109695435\n",
      "Score: 0.989667534828186\n",
      "Score: 0.9846648573875427\n",
      "Score: 0.9855256676673889\n",
      "Score: 0.9580562114715576\n",
      "Score: 0.45677992701530457\n",
      "Score: 0.878189742565155\n",
      "Score: 0.9720034003257751\n",
      "Score: 0.9814964532852173\n",
      "Score: 0.9700414538383484\n",
      "Score: 0.9819309711456299\n",
      "Score: 0.9899001717567444\n",
      "Score: 0.9899711012840271\n",
      "Score: 0.06533671915531158\n",
      "Score: 0.9811155796051025\n",
      "Score: 0.7605912685394287\n",
      "Score: 0.988805890083313\n",
      "Score: 0.9458755254745483\n",
      "Score: 0.7892608642578125\n",
      "Score: 0.769371747970581\n",
      "Score: 0.8497326374053955\n",
      "Score: 0.9778253436088562\n",
      "Score: 0.9539130926132202\n",
      "Score: 0.9565756320953369\n",
      "Score: 0.07603129744529724\n",
      "Score: 0.9853479266166687\n",
      "Score: 0.9866245985031128\n",
      "Score: 0.9548957347869873\n",
      "Score: 0.9881675839424133\n",
      "Score: 0.9678481221199036\n",
      "Score: 0.7096464037895203\n",
      "Score: 0.9885974526405334\n",
      "Score: 0.9904712438583374\n",
      "Score: 0.030244410037994385\n",
      "Score: 0.9891049265861511\n",
      "Score: 0.9911633729934692\n",
      "Score: 0.9671428203582764\n",
      "Score: 0.9727123379707336\n",
      "Score: 0.9902650117874146\n",
      "Score: 0.9901712536811829\n",
      "Score: 0.9171134829521179\n",
      "Score: 0.9452317953109741\n",
      "Score: 0.03347985818982124\n",
      "Score: 0.1513173133134842\n",
      "Score: 0.03220939263701439\n",
      "Score: 0.5943299531936646\n",
      "Score: 0.9909988641738892\n",
      "Score: 0.9456078410148621\n",
      "Score: 0.9840588569641113\n",
      "Score: 0.9101335406303406\n",
      "Score: 0.9882484674453735\n",
      "Score: 0.9875467419624329\n",
      "Score: 0.6885226368904114\n",
      "Score: 0.9881675839424133\n",
      "Score: 0.9649925827980042\n",
      "Score: 0.9894881248474121\n",
      "Score: 0.99007248878479\n",
      "Score: 0.9856882095336914\n",
      "Score: 0.9721142053604126\n",
      "Score: 0.9820418357849121\n",
      "Score: 0.9364804625511169\n",
      "Score: 0.974683403968811\n",
      "Score: 0.9914984107017517\n",
      "Score: 0.9903806447982788\n",
      "Score: 0.9912524819374084\n",
      "Score: 0.9892256855964661\n",
      "Score: 0.027227092534303665\n",
      "Score: 0.9286389946937561\n",
      "Score: 0.9878453612327576\n",
      "Score: 0.991633415222168\n",
      "Score: 0.984095573425293\n",
      "Score: 0.9887564778327942\n",
      "Score: 0.9272538423538208\n",
      "Score: 0.1633109450340271\n",
      "Score: 0.9090752005577087\n",
      "Score: 0.5864178538322449\n",
      "Score: 0.9795937538146973\n",
      "Score: 0.9853641390800476\n",
      "Score: 0.9535320997238159\n",
      "Score: 0.310926228761673\n",
      "Score: 0.99070143699646\n",
      "Score: 0.9581051468849182\n",
      "Score: 0.9876587986946106\n",
      "Score: 0.8541537523269653\n",
      "Score: 0.9689189791679382\n",
      "Score: 0.5431731939315796\n",
      "Score: 0.5597115159034729\n",
      "Score: 0.8339618444442749\n",
      "Score: 0.9875620603561401\n",
      "Score: 0.9486878514289856\n",
      "Score: 0.49623191356658936\n",
      "Score: 0.07891330122947693\n",
      "Score: 0.03007296659052372\n",
      "Score: 0.971549391746521\n",
      "Score: 0.9905643463134766\n",
      "Score: 0.9740216732025146\n",
      "Score: 0.9304773807525635\n",
      "Score: 0.9892228841781616\n",
      "Score: 0.32671210169792175\n",
      "Score: 0.9905739426612854\n",
      "Score: 0.582470715045929\n",
      "Score: 0.9000672698020935\n",
      "Score: 0.9884192943572998\n",
      "Score: 0.9798011779785156\n",
      "Score: 0.2851739525794983\n",
      "Score: 0.9902878999710083\n",
      "Score: 0.741254985332489\n",
      "Score: 0.9655142426490784\n",
      "Score: 0.14800652861595154\n",
      "Score: 0.8322790265083313\n",
      "Score: 0.9691513776779175\n",
      "Score: 0.6437852382659912\n",
      "Score: 0.8094988465309143\n",
      "Score: 0.9099494218826294\n",
      "Score: 0.990426778793335\n",
      "Score: 0.5985044836997986\n",
      "Score: 0.9306519031524658\n",
      "Score: 0.7686665058135986\n",
      "Score: 0.18180647492408752\n",
      "Score: 0.9910007119178772\n",
      "Score: 0.9549418091773987\n",
      "Score: 0.9904640913009644\n",
      "Score: 0.11659757792949677\n",
      "Score: 0.026557650417089462\n",
      "Score: 0.09926211833953857\n",
      "Score: 0.9835364818572998\n",
      "Score: 0.9831704497337341\n",
      "Score: 0.984563410282135\n",
      "Score: 0.6644704341888428\n",
      "Score: 0.9908031225204468\n",
      "Score: 0.5168108940124512\n",
      "Score: 0.9804198145866394\n",
      "Score: 0.9867886304855347\n",
      "Score: 0.9861509203910828\n",
      "Score: 0.9887696504592896\n",
      "Score: 0.9533282518386841\n",
      "Score: 0.9483108520507812\n",
      "Score: 0.15079018473625183\n",
      "Score: 0.9877884984016418\n",
      "Score: 0.8149656653404236\n",
      "Score: 0.875293493270874\n",
      "Score: 0.9903197884559631\n",
      "Score: 0.9637259244918823\n",
      "Score: 0.9675150513648987\n",
      "Score: 0.9898878931999207\n",
      "Score: 0.01896277815103531\n",
      "Score: 0.9906836748123169\n",
      "Score: 0.9895477294921875\n",
      "Score: 0.4926995038986206\n",
      "Score: 0.8287743926048279\n",
      "Score: 0.04290924221277237\n",
      "Score: 0.978931725025177\n",
      "Score: 0.9331953525543213\n",
      "Score: 0.9881857633590698\n",
      "Score: 0.989816427230835\n",
      "Score: 0.9374108910560608\n",
      "Score: 0.9767638444900513\n",
      "Score: 0.07988718152046204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.9847087860107422\n",
      "Score: 0.06511615216732025\n",
      "Score: 0.9884053468704224\n",
      "Score: 0.8030979633331299\n",
      "Score: 0.15169541537761688\n",
      "Score: 0.9802668690681458\n",
      "Score: 0.9874778985977173\n",
      "Score: 0.9471572637557983\n",
      "Score: 0.9907183051109314\n",
      "Score: 0.08679325133562088\n",
      "Score: 0.04434254765510559\n",
      "Score: 0.9205825924873352\n",
      "Score: 0.07107432931661606\n",
      "Score: 0.9907665252685547\n",
      "Score: 0.7942423224449158\n",
      "Score: 0.18081220984458923\n",
      "Score: 0.9892515540122986\n",
      "Score: 0.9900057911872864\n",
      "Score: 0.9842122197151184\n",
      "Score: 0.9724715948104858\n",
      "Score: 0.9386971592903137\n",
      "Score: 0.035825181752443314\n",
      "Score: 0.6403425335884094\n",
      "Score: 0.08066266775131226\n",
      "Score: 0.9788405895233154\n",
      "Score: 0.021515879780054092\n",
      "Score: 0.9305002093315125\n",
      "Score: 0.9889113903045654\n",
      "Score: 0.704558789730072\n",
      "Score: 0.9879283308982849\n",
      "Score: 0.9676231741905212\n",
      "Score: 0.9781097769737244\n",
      "Score: 0.9715830087661743\n",
      "Score: 0.9212403297424316\n",
      "Score: 0.9858099818229675\n",
      "Score: 0.7468068599700928\n",
      "Score: 0.7792054414749146\n",
      "Score: 0.9387190341949463\n",
      "Score: 0.713131308555603\n",
      "Score: 0.9914210438728333\n",
      "Score: 0.7997207641601562\n",
      "Score: 0.08139618486166\n",
      "Score: 0.676831066608429\n",
      "Score: 0.9916291236877441\n",
      "Score: 0.8765708804130554\n",
      "Score: 0.035627882927656174\n",
      "Score: 0.8425219655036926\n",
      "Score: 0.986839771270752\n",
      "Score: 0.989702582359314\n",
      "Score: 0.8921496272087097\n",
      "Score: 0.9234636425971985\n",
      "Score: 0.9657105207443237\n",
      "Score: 0.885074257850647\n",
      "Score: 0.895102322101593\n",
      "Score: 0.9897922277450562\n",
      "Score: 0.9894562363624573\n",
      "Score: 0.8720839619636536\n",
      "Score: 0.5251845717430115\n",
      "Score: 0.9672918319702148\n",
      "Score: 0.8533541560173035\n",
      "Score: 0.9546211957931519\n",
      "Score: 0.9914210438728333\n",
      "Score: 0.9838482737541199\n",
      "Score: 0.9675622582435608\n",
      "Score: 0.9914566874504089\n",
      "Score: 0.9870303273200989\n",
      "Score: 0.8792696595191956\n",
      "Score: 0.99114590883255\n",
      "Score: 0.9914210438728333\n",
      "Score: 0.13397280871868134\n",
      "Score: 0.9864728450775146\n",
      "Score: 0.990827739238739\n",
      "Score: 0.9861552119255066\n",
      "Score: 0.8466500043869019\n",
      "Score: 0.9913052916526794\n",
      "Score: 0.8052987456321716\n",
      "Score: 0.771902859210968\n",
      "Score: 0.9533282518386841\n",
      "Score: 0.2851739525794983\n",
      "Score: 0.8191609382629395\n",
      "Score: 0.20820945501327515\n",
      "Score: 0.9917504191398621\n",
      "Score: 0.09728395938873291\n",
      "Score: 0.9611075520515442\n",
      "Score: 0.9445165991783142\n",
      "Score: 0.9909511804580688\n",
      "Score: 0.9797083735466003\n",
      "Score: 0.9830957055091858\n",
      "Score: 0.9885867238044739\n",
      "Score: 0.990652322769165\n",
      "Score: 0.6293938755989075\n",
      "Score: 0.9446629285812378\n",
      "Score: 0.892457902431488\n",
      "Score: 0.9900109171867371\n",
      "Score: 0.9836921691894531\n",
      "Score: 0.9836921691894531\n",
      "Score: 0.9312062859535217\n",
      "Score: 0.9883868098258972\n",
      "Score: 0.9887868762016296\n",
      "Score: 0.9684377908706665\n",
      "Score: 0.42475804686546326\n",
      "Score: 0.8372776508331299\n",
      "Score: 0.9883548021316528\n",
      "Score: 0.9882991313934326\n",
      "Score: 0.9816623330116272\n",
      "Score: 0.851936399936676\n",
      "Score: 0.9616186022758484\n",
      "Score: 0.9908039569854736\n",
      "Score: 0.2131422758102417\n",
      "Score: 0.9907318949699402\n",
      "Score: 0.9750239849090576\n",
      "Score: 0.990250825881958\n",
      "Score: 0.03082587756216526\n",
      "Score: 0.9900746941566467\n",
      "Score: 0.9823159575462341\n",
      "Score: 0.969277560710907\n",
      "Score: 0.9817504286766052\n",
      "Score: 0.8300484418869019\n",
      "Score: 0.9900228381156921\n",
      "Score: 0.6785240173339844\n",
      "Score: 0.9863892197608948\n",
      "Score: 0.04202520474791527\n",
      "Score: 0.05728648230433464\n",
      "Score: 0.969872772693634\n",
      "Score: 0.9492877125740051\n",
      "Score: 0.9899840354919434\n",
      "Score: 0.9293652772903442\n",
      "Score: 0.9878472089767456\n",
      "Score: 0.21096202731132507\n",
      "Score: 0.9914319515228271\n",
      "Score: 0.9008961319923401\n",
      "Score: 0.9539780020713806\n",
      "Score: 0.9867929220199585\n",
      "Score: 0.9908879995346069\n",
      "Score: 0.03466370329260826\n",
      "Score: 0.026188114657998085\n",
      "Score: 0.9880349040031433\n",
      "Score: 0.8664405345916748\n",
      "Score: 0.9813412427902222\n",
      "Score: 0.8643403649330139\n",
      "Score: 0.9090859889984131\n",
      "Score: 0.9681467413902283\n",
      "Score: 0.7955455780029297\n",
      "Score: 0.9860365390777588\n",
      "Score: 0.9863866567611694\n",
      "Score: 0.984437882900238\n",
      "Score: 0.991360604763031\n",
      "Score: 0.9907628893852234\n",
      "Score: 0.9809205532073975\n",
      "Score: 0.9119217991828918\n",
      "Score: 0.8595203161239624\n",
      "Score: 0.9890192151069641\n",
      "Score: 0.9902418255805969\n",
      "Score: 0.7162274122238159\n",
      "Score: 0.9900856018066406\n",
      "Score: 0.9894888997077942\n",
      "Score: 0.8757520318031311\n",
      "Score: 0.9891074895858765\n",
      "Score: 0.25827422738075256\n",
      "Score: 0.019474122673273087\n",
      "Score: 0.9395508766174316\n",
      "Score: 0.9875965714454651\n",
      "Score: 0.9886940121650696\n",
      "Score: 0.991530179977417\n",
      "Score: 0.9840197563171387\n",
      "Score: 0.8441419005393982\n",
      "Score: 0.9857078194618225\n",
      "Score: 0.9317112565040588\n",
      "Score: 0.8383539319038391\n",
      "Score: 0.026448676362633705\n",
      "Score: 0.9557382464408875\n",
      "Score: 0.9875198602676392\n",
      "Score: 0.9884351491928101\n",
      "Score: 0.9008759260177612\n",
      "Score: 0.9781889319419861\n",
      "Score: 0.9457539916038513\n",
      "Score: 0.9881578087806702\n",
      "Score: 0.8573932647705078\n",
      "Score: 0.9878931641578674\n",
      "Score: 0.11403641104698181\n",
      "Score: 0.7373977303504944\n",
      "Score: 0.9582809209823608\n",
      "Score: 0.10532251745462418\n",
      "Score: 0.85756915807724\n",
      "Score: 0.9863751530647278\n",
      "Score: 0.9365364909172058\n",
      "Score: 0.3841609060764313\n",
      "Score: 0.9665555357933044\n",
      "Score: 0.6833251714706421\n",
      "Score: 0.981689453125\n",
      "Score: 0.9861787557601929\n",
      "Score: 0.9852516651153564\n",
      "Score: 0.9576223492622375\n",
      "Score: 0.8264666199684143\n",
      "Score: 0.9855031967163086\n",
      "Score: 0.06463033705949783\n",
      "Score: 0.9758298397064209\n",
      "Score: 0.9277316331863403\n",
      "Score: 0.9270294308662415\n",
      "Score: 0.17901958525180817\n",
      "Score: 0.9890592098236084\n",
      "Score: 0.9908955693244934\n",
      "Score: 0.9458203315734863\n",
      "Score: 0.733485221862793\n",
      "Score: 0.9777719974517822\n",
      "Score: 0.1266613006591797\n",
      "Score: 0.8351252675056458\n",
      "Score: 0.9883954524993896\n",
      "Score: 0.11166774481534958\n",
      "Score: 0.023348301649093628\n",
      "Score: 0.9512531757354736\n",
      "Score: 0.029386993497610092\n",
      "Score: 0.990439772605896\n",
      "Score: 0.9879163503646851\n",
      "Score: 0.9829215407371521\n",
      "Score: 0.9431402087211609\n",
      "Score: 0.9888334274291992\n",
      "Score: 0.9736541509628296\n",
      "Score: 0.9908818006515503\n",
      "Score: 0.9133896231651306\n",
      "Score: 0.9817177057266235\n",
      "Score: 0.9863488674163818\n",
      "Score: 0.9836170673370361\n",
      "Score: 0.9801895022392273\n",
      "Score: 0.680381715297699\n",
      "Score: 0.9715830087661743\n",
      "Score: 0.9859532713890076\n",
      "Score: 0.787463366985321\n",
      "Score: 0.9794961810112\n",
      "Score: 0.9914237856864929\n",
      "Score: 0.9838263392448425\n",
      "Score: 0.924751341342926\n",
      "Score: 0.969167172908783\n",
      "Score: 0.9717864394187927\n",
      "Score: 0.9854090809822083\n",
      "Score: 0.9260144233703613\n",
      "Score: 0.984001636505127\n",
      "Score: 0.9698652625083923\n",
      "Score: 0.9658260941505432\n",
      "Score: 0.9699596762657166\n",
      "Score: 0.9714005589485168\n",
      "Score: 0.9694339632987976\n",
      "Score: 0.9670103788375854\n",
      "Score: 0.9412968754768372\n",
      "Score: 0.9913700819015503\n",
      "Score: 0.9899696707725525\n",
      "Score: 0.932081937789917\n",
      "Score: 0.9059028625488281\n",
      "Score: 0.9054718613624573\n",
      "Score: 0.9845651388168335\n",
      "Score: 0.9854985475540161\n",
      "Score: 0.9667825698852539\n",
      "Score: 0.198202982544899\n",
      "Score: 0.979162871837616\n",
      "Score: 0.9864428043365479\n",
      "Score: 0.902021586894989\n",
      "Score: 0.9887592792510986\n",
      "Score: 0.9835530519485474\n",
      "Score: 0.8420058488845825\n",
      "Score: 0.17835326492786407\n",
      "Score: 0.8843515515327454\n",
      "Score: 0.4002593457698822\n",
      "Score: 0.612525224685669\n",
      "Score: 0.9579566717147827\n",
      "Score: 0.07771848887205124\n",
      "Score: 0.05364036560058594\n",
      "Score: 0.9861635565757751\n",
      "Score: 0.8201407790184021\n",
      "Score: 0.8805177211761475\n",
      "Score: 0.7294436693191528\n",
      "Score: 0.3149223029613495\n",
      "Score: 0.24752195179462433\n",
      "Score: 0.09404084086418152\n",
      "Score: 0.1358712911605835\n",
      "Score: 0.983034074306488\n",
      "Score: 0.9684672951698303\n",
      "Score: 0.16782738268375397\n",
      "Score: 0.030738279223442078\n",
      "Score: 0.8018593192100525\n",
      "Score: 0.9710034132003784\n",
      "Score: 0.9496689438819885\n",
      "Score: 0.39395269751548767\n",
      "Score: 0.9845389723777771\n",
      "Score: 0.9899961948394775\n",
      "Score: 0.9880577325820923\n",
      "Score: 0.759032666683197\n",
      "Score: 0.9845389723777771\n",
      "Score: 0.9899961948394775\n",
      "Score: 0.759032666683197\n",
      "Score: 0.9743245244026184\n",
      "Score: 0.9845389723777771\n",
      "Score: 0.9899961948394775\n",
      "Score: 0.759032666683197\n",
      "Score: 0.9743245244026184\n",
      "Score: 0.9845389723777771\n",
      "Score: 0.9899961948394775\n",
      "Score: 0.759032666683197\n"
     ]
    }
   ],
   "source": [
    "sentiments = []\n",
    "indices = []\n",
    "prev = np.datetime64(\"2015-11-12 21:31:26\")\n",
    "for i in final_y.index:\n",
    "    total=0\n",
    "    cnt=0\n",
    "    for j in final_posts.itertuples():\n",
    "        k,msg,time = j\n",
    "        indices.append(k)\n",
    "        if np.datetime64(time)<np.datetime64(i) and np.datetime64(time)>prev:\n",
    "            total += tf.squeeze(sent_model.predict([msg])).numpy()\n",
    "            print(f'Score: {tf.squeeze(sent_model.predict([msg])).numpy()}')\n",
    "            cnt+=1\n",
    "    prev = np.datetime64(i)\n",
    "    if(cnt==0):\n",
    "        sentiments.append(0)\n",
    "    else:\n",
    "        sentiments.append(total/cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f54f921",
   "metadata": {},
   "source": [
    "# SOLVING THE 0 SENTIMENT SCORE PROBLEM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a184fe8",
   "metadata": {},
   "source": [
    "#### Steps to solve the problem:\n",
    "* Getting the indices where sentiment score is 0\n",
    "* Removing those values from the sentiments array and final_x\n",
    "* Removing the added index column\n",
    "* Using N-beats model to make predictions on the prepared final_x datset\n",
    "* Make the final dataset to train final model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d9237d",
   "metadata": {},
   "source": [
    "#### Getting the indices where sentiment score is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "64f20fee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5, 6, 7, 8, 9, 10, 11, 12, 57]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_index = []\n",
    "for i,j in enumerate(sentiments):\n",
    "    if(j==0):\n",
    "        zero_index.append(i)\n",
    "zero_index "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6128f9c5",
   "metadata": {},
   "source": [
    "#### Removing those values from the sentiments array and final_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3ccc1537",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_x_zeros = final_x.copy()\n",
    "final_y_zeros = final_y.copy()\n",
    "final_y_zeros = final_y_zeros.to_frame()\n",
    "final_x_zeros['removal_assist'] = np.arange(0,len(final_x),1)\n",
    "final_y_zeros['removal_assist'] = np.arange(0,len(final_x),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "08056a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_y_zeros = final_y_zeros[final_y_zeros.removal_assist.isin(zero_index)==False]\n",
    "final_x_zeros = final_x_zeros[final_x_zeros.removal_assist.isin(zero_index)==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0b4ac4e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53, 53)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_y_zeros),len(final_x_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d1084b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.93790942, 0.94711164, 0.90710024, 0.8002191 , 0.98364121,\n",
       "       0.34421938, 0.81026217, 0.93727617, 0.85286945, 0.81385339,\n",
       "       0.91488078, 0.84711482, 0.78174605, 0.78755366, 0.79193413,\n",
       "       0.88688747, 0.81942778, 0.86818131, 0.57996973, 0.94231537,\n",
       "       0.81385345, 0.71057868, 0.79632066, 0.71666523, 0.83461632,\n",
       "       0.75409228, 0.76133595, 0.64567702, 0.74668035, 0.75681433,\n",
       "       0.90270813, 0.90020501, 0.66226926, 0.91934999, 0.89887066,\n",
       "       0.89318242, 0.74312352, 0.75146003, 0.76075468, 0.77344375,\n",
       "       0.86227876, 0.9537311 , 0.79507104, 0.93639518, 0.85952354,\n",
       "       0.62649698, 0.78937954, 0.98839545, 0.87384896, 0.90128844,\n",
       "       0.65563229, 0.61601235, 0.81838229])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiments = np.delete(sentiments,zero_index)\n",
    "sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b67258",
   "metadata": {},
   "source": [
    "#### Removing the added index column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6e7d83b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_x_zeros.drop([\"removal_assist\"], axis=1,inplace=True)\n",
    "final_y_zeros.drop([\"removal_assist\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6ef214",
   "metadata": {},
   "source": [
    "#### Using N-beats model to make predictions on the prepared final_x datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0e712ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 71)]         0           []                               \n",
      "                                                                                                  \n",
      " blocks_1 (blocks)              ((None, 71),         861768      ['input_1[0][0]']                \n",
      "                                 (None, 1))                                                       \n",
      "                                                                                                  \n",
      " subtract (Subtract)            (None, 71)           0           ['input_1[0][0]',                \n",
      "                                                                  'blocks_1[0][0]']               \n",
      "                                                                                                  \n",
      " blocks_2 (blocks)              ((None, 71),         861768      ['subtract[0][0]']               \n",
      "                                 (None, 1))                                                       \n",
      "                                                                                                  \n",
      " subtract_1 (Subtract)          (None, 71)           0           ['subtract[0][0]',               \n",
      "                                                                  'blocks_2[0][0]']               \n",
      "                                                                                                  \n",
      " blocks_3 (blocks)              ((None, 71),         861768      ['subtract_1[0][0]']             \n",
      "                                 (None, 1))                                                       \n",
      "                                                                                                  \n",
      " subtract_2 (Subtract)          (None, 71)           0           ['subtract_1[0][0]',             \n",
      "                                                                  'blocks_3[0][0]']               \n",
      "                                                                                                  \n",
      " blocks_4 (blocks)              ((None, 71),         861768      ['subtract_2[0][0]']             \n",
      "                                 (None, 1))                                                       \n",
      "                                                                                                  \n",
      " subtract_3 (Subtract)          (None, 71)           0           ['subtract_2[0][0]',             \n",
      "                                                                  'blocks_4[0][0]']               \n",
      "                                                                                                  \n",
      " blocks_5 (blocks)              ((None, 71),         861768      ['subtract_3[0][0]']             \n",
      "                                 (None, 1))                                                       \n",
      "                                                                                                  \n",
      " subtract_4 (Subtract)          (None, 71)           0           ['subtract_3[0][0]',             \n",
      "                                                                  'blocks_5[0][0]']               \n",
      "                                                                                                  \n",
      " blocks_6 (blocks)              ((None, 71),         861768      ['subtract_4[0][0]']             \n",
      "                                 (None, 1))                                                       \n",
      "                                                                                                  \n",
      " subtract_5 (Subtract)          (None, 71)           0           ['subtract_4[0][0]',             \n",
      "                                                                  'blocks_6[0][0]']               \n",
      "                                                                                                  \n",
      " blocks_7 (blocks)              ((None, 71),         861768      ['subtract_5[0][0]']             \n",
      "                                 (None, 1))                                                       \n",
      "                                                                                                  \n",
      " subtract_6 (Subtract)          (None, 71)           0           ['subtract_5[0][0]',             \n",
      "                                                                  'blocks_7[0][0]']               \n",
      "                                                                                                  \n",
      " blocks_8 (blocks)              ((None, 71),         861768      ['subtract_6[0][0]']             \n",
      "                                 (None, 1))                                                       \n",
      "                                                                                                  \n",
      " subtract_7 (Subtract)          (None, 71)           0           ['subtract_6[0][0]',             \n",
      "                                                                  'blocks_8[0][0]']               \n",
      "                                                                                                  \n",
      " blocks_9 (blocks)              ((None, 71),         861768      ['subtract_7[0][0]']             \n",
      "                                 (None, 1))                                                       \n",
      "                                                                                                  \n",
      " subtract_8 (Subtract)          (None, 71)           0           ['subtract_7[0][0]',             \n",
      "                                                                  'blocks_9[0][0]']               \n",
      "                                                                                                  \n",
      " blocks_10 (blocks)             ((None, 71),         861768      ['subtract_8[0][0]']             \n",
      "                                 (None, 1))                                                       \n",
      "                                                                                                  \n",
      " subtract_9 (Subtract)          (None, 71)           0           ['subtract_8[0][0]',             \n",
      "                                                                  'blocks_10[0][0]']              \n",
      "                                                                                                  \n",
      " blocks_11 (blocks)             ((None, 71),         861768      ['subtract_9[0][0]']             \n",
      "                                 (None, 1))                                                       \n",
      "                                                                                                  \n",
      " subtract_10 (Subtract)         (None, 71)           0           ['subtract_9[0][0]',             \n",
      "                                                                  'blocks_11[0][0]']              \n",
      "                                                                                                  \n",
      " blocks_12 (blocks)             ((None, 71),         861768      ['subtract_10[0][0]']            \n",
      "                                 (None, 1))                                                       \n",
      "                                                                                                  \n",
      " subtract_11 (Subtract)         (None, 71)           0           ['subtract_10[0][0]',            \n",
      "                                                                  'blocks_12[0][0]']              \n",
      "                                                                                                  \n",
      " blocks_13 (blocks)             ((None, 71),         861768      ['subtract_11[0][0]']            \n",
      "                                 (None, 1))                                                       \n",
      "                                                                                                  \n",
      " subtract_12 (Subtract)         (None, 71)           0           ['subtract_11[0][0]',            \n",
      "                                                                  'blocks_13[0][0]']              \n",
      "                                                                                                  \n",
      " blocks_14 (blocks)             ((None, 71),         861768      ['subtract_12[0][0]']            \n",
      "                                 (None, 1))                                                       \n",
      "                                                                                                  \n",
      " subtract_13 (Subtract)         (None, 71)           0           ['subtract_12[0][0]',            \n",
      "                                                                  'blocks_14[0][0]']              \n",
      "                                                                                                  \n",
      " blocks_15 (blocks)             ((None, 71),         861768      ['subtract_13[0][0]']            \n",
      "                                 (None, 1))                                                       \n",
      "                                                                                                  \n",
      " subtract_14 (Subtract)         (None, 71)           0           ['subtract_13[0][0]',            \n",
      "                                                                  'blocks_15[0][0]']              \n",
      "                                                                                                  \n",
      " blocks_16 (blocks)             ((None, 71),         861768      ['subtract_14[0][0]']            \n",
      "                                 (None, 1))                                                       \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 1)            0           ['blocks_1[0][1]',               \n",
      "                                                                  'blocks_2[0][1]']               \n",
      "                                                                                                  \n",
      " subtract_15 (Subtract)         (None, 71)           0           ['subtract_14[0][0]',            \n",
      "                                                                  'blocks_16[0][0]']              \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 1)            0           ['add[0][0]',                    \n",
      "                                                                  'blocks_3[0][1]']               \n",
      "                                                                                                  \n",
      " blocks_17 (blocks)             ((None, 71),         861768      ['subtract_15[0][0]']            \n",
      "                                 (None, 1))                                                       \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 1)            0           ['add_1[0][0]',                  \n",
      "                                                                  'blocks_4[0][1]']               \n",
      "                                                                                                  \n",
      " subtract_16 (Subtract)         (None, 71)           0           ['subtract_15[0][0]',            \n",
      "                                                                  'blocks_17[0][0]']              \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 1)            0           ['add_2[0][0]',                  \n",
      "                                                                  'blocks_5[0][1]']               \n",
      "                                                                                                  \n",
      " blocks_18 (blocks)             ((None, 71),         861768      ['subtract_16[0][0]']            \n",
      "                                 (None, 1))                                                       \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 1)            0           ['add_3[0][0]',                  \n",
      "                                                                  'blocks_6[0][1]']               \n",
      "                                                                                                  \n",
      " subtract_17 (Subtract)         (None, 71)           0           ['subtract_16[0][0]',            \n",
      "                                                                  'blocks_18[0][0]']              \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 1)            0           ['add_4[0][0]',                  \n",
      "                                                                  'blocks_7[0][1]']               \n",
      "                                                                                                  \n",
      " blocks_19 (blocks)             ((None, 71),         861768      ['subtract_17[0][0]']            \n",
      "                                 (None, 1))                                                       \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 1)            0           ['add_5[0][0]',                  \n",
      "                                                                  'blocks_8[0][1]']               \n",
      "                                                                                                  \n",
      " subtract_18 (Subtract)         (None, 71)           0           ['subtract_17[0][0]',            \n",
      "                                                                  'blocks_19[0][0]']              \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 1)            0           ['add_6[0][0]',                  \n",
      "                                                                  'blocks_9[0][1]']               \n",
      "                                                                                                  \n",
      " blocks_20 (blocks)             ((None, 71),         861768      ['subtract_18[0][0]']            \n",
      "                                 (None, 1))                                                       \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 1)            0           ['add_7[0][0]',                  \n",
      "                                                                  'blocks_10[0][1]']              \n",
      "                                                                                                  \n",
      " subtract_19 (Subtract)         (None, 71)           0           ['subtract_18[0][0]',            \n",
      "                                                                  'blocks_20[0][0]']              \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 1)            0           ['add_8[0][0]',                  \n",
      "                                                                  'blocks_11[0][1]']              \n",
      "                                                                                                  \n",
      " blocks_21 (blocks)             ((None, 71),         861768      ['subtract_19[0][0]']            \n",
      "                                 (None, 1))                                                       \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 1)            0           ['add_9[0][0]',                  \n",
      "                                                                  'blocks_12[0][1]']              \n",
      "                                                                                                  \n",
      " subtract_20 (Subtract)         (None, 71)           0           ['subtract_19[0][0]',            \n",
      "                                                                  'blocks_21[0][0]']              \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 1)            0           ['add_10[0][0]',                 \n",
      "                                                                  'blocks_13[0][1]']              \n",
      "                                                                                                  \n",
      " blocks_22 (blocks)             ((None, 71),         861768      ['subtract_20[0][0]']            \n",
      "                                 (None, 1))                                                       \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 1)            0           ['add_11[0][0]',                 \n",
      "                                                                  'blocks_14[0][1]']              \n",
      "                                                                                                  \n",
      " subtract_21 (Subtract)         (None, 71)           0           ['subtract_20[0][0]',            \n",
      "                                                                  'blocks_22[0][0]']              \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 1)            0           ['add_12[0][0]',                 \n",
      "                                                                  'blocks_15[0][1]']              \n",
      "                                                                                                  \n",
      " blocks_23 (blocks)             ((None, 71),         861768      ['subtract_21[0][0]']            \n",
      "                                 (None, 1))                                                       \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 1)            0           ['add_13[0][0]',                 \n",
      "                                                                  'blocks_16[0][1]']              \n",
      "                                                                                                  \n",
      " subtract_22 (Subtract)         (None, 71)           0           ['subtract_21[0][0]',            \n",
      "                                                                  'blocks_23[0][0]']              \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, 1)            0           ['add_14[0][0]',                 \n",
      "                                                                  'blocks_17[0][1]']              \n",
      "                                                                                                  \n",
      " blocks_24 (blocks)             ((None, 71),         861768      ['subtract_22[0][0]']            \n",
      "                                 (None, 1))                                                       \n",
      "                                                                                                  \n",
      " add_16 (Add)                   (None, 1)            0           ['add_15[0][0]',                 \n",
      "                                                                  'blocks_18[0][1]']              \n",
      "                                                                                                  \n",
      " subtract_23 (Subtract)         (None, 71)           0           ['subtract_22[0][0]',            \n",
      "                                                                  'blocks_24[0][0]']              \n",
      "                                                                                                  \n",
      " add_17 (Add)                   (None, 1)            0           ['add_16[0][0]',                 \n",
      "                                                                  'blocks_19[0][1]']              \n",
      "                                                                                                  \n",
      " blocks_25 (blocks)             ((None, 71),         861768      ['subtract_23[0][0]']            \n",
      "                                 (None, 1))                                                       \n",
      "                                                                                                  \n",
      " add_18 (Add)                   (None, 1)            0           ['add_17[0][0]',                 \n",
      "                                                                  'blocks_20[0][1]']              \n",
      "                                                                                                  \n",
      " subtract_24 (Subtract)         (None, 71)           0           ['subtract_23[0][0]',            \n",
      "                                                                  'blocks_25[0][0]']              \n",
      "                                                                                                  \n",
      " add_19 (Add)                   (None, 1)            0           ['add_18[0][0]',                 \n",
      "                                                                  'blocks_21[0][1]']              \n",
      "                                                                                                  \n",
      " blocks_26 (blocks)             ((None, 71),         861768      ['subtract_24[0][0]']            \n",
      "                                 (None, 1))                                                       \n",
      "                                                                                                  \n",
      " add_20 (Add)                   (None, 1)            0           ['add_19[0][0]',                 \n",
      "                                                                  'blocks_22[0][1]']              \n",
      "                                                                                                  \n",
      " subtract_25 (Subtract)         (None, 71)           0           ['subtract_24[0][0]',            \n",
      "                                                                  'blocks_26[0][0]']              \n",
      "                                                                                                  \n",
      " add_21 (Add)                   (None, 1)            0           ['add_20[0][0]',                 \n",
      "                                                                  'blocks_23[0][1]']              \n",
      "                                                                                                  \n",
      " blocks_27 (blocks)             ((None, 71),         861768      ['subtract_25[0][0]']            \n",
      "                                 (None, 1))                                                       \n",
      "                                                                                                  \n",
      " add_22 (Add)                   (None, 1)            0           ['add_21[0][0]',                 \n",
      "                                                                  'blocks_24[0][1]']              \n",
      "                                                                                                  \n",
      " subtract_26 (Subtract)         (None, 71)           0           ['subtract_25[0][0]',            \n",
      "                                                                  'blocks_27[0][0]']              \n",
      "                                                                                                  \n",
      " add_23 (Add)                   (None, 1)            0           ['add_22[0][0]',                 \n",
      "                                                                  'blocks_25[0][1]']              \n",
      "                                                                                                  \n",
      " blocks_28 (blocks)             ((None, 71),         861768      ['subtract_26[0][0]']            \n",
      "                                 (None, 1))                                                       \n",
      "                                                                                                  \n",
      " add_24 (Add)                   (None, 1)            0           ['add_23[0][0]',                 \n",
      "                                                                  'blocks_26[0][1]']              \n",
      "                                                                                                  \n",
      " subtract_27 (Subtract)         (None, 71)           0           ['subtract_26[0][0]',            \n",
      "                                                                  'blocks_28[0][0]']              \n",
      "                                                                                                  \n",
      " add_25 (Add)                   (None, 1)            0           ['add_24[0][0]',                 \n",
      "                                                                  'blocks_27[0][1]']              \n",
      "                                                                                                  \n",
      " blocks_29 (blocks)             ((None, 71),         861768      ['subtract_27[0][0]']            \n",
      "                                 (None, 1))                                                       \n",
      "                                                                                                  \n",
      " add_26 (Add)                   (None, 1)            0           ['add_25[0][0]',                 \n",
      "                                                                  'blocks_28[0][1]']              \n",
      "                                                                                                  \n",
      " subtract_28 (Subtract)         (None, 71)           0           ['subtract_27[0][0]',            \n",
      "                                                                  'blocks_29[0][0]']              \n",
      "                                                                                                  \n",
      " add_27 (Add)                   (None, 1)            0           ['add_26[0][0]',                 \n",
      "                                                                  'blocks_29[0][1]']              \n",
      "                                                                                                  \n",
      " blocks_30 (blocks)             ((None, 71),         861768      ['subtract_28[0][0]']            \n",
      "                                 (None, 1))                                                       \n",
      "                                                                                                  \n",
      " add_28 (Add)                   (None, 1)            0           ['add_27[0][0]',                 \n",
      "                                                                  'blocks_30[0][1]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 25,853,040\n",
      "Trainable params: 25,853,040\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_nbeats = tf.keras.models.load_model(\"nbeats_itc\")\n",
    "model_nbeats.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d606d7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = tf.squeeze(model_nbeats.predict(final_x_zeros)).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b6ebb6",
   "metadata": {},
   "source": [
    "#### Make the final dataset to train final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "97da63af",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame({\"Predictions\": preds,\n",
    "                        \"Sentiments\":sentiments})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e75a61c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53, 53)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_df),len(final_y_zeros)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f910d9db",
   "metadata": {},
   "source": [
    "# BUILDING AND TRAINING THE FINAL MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "985a5826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 2)]               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 8)                 24        \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 8)                 72        \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 8)                 72        \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 177\n",
      "Trainable params: 177\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "inputs = tf.keras.Input(shape=(2))\n",
    "x = tf.keras.layers.Dense(8, activation=\"selu\", kernel_initializer=\"lecun_normal\")(inputs)\n",
    "x = tf.keras.layers.Dense(8, activation=\"selu\", kernel_initializer=\"lecun_normal\")(x)\n",
    "x = tf.keras.layers.Dense(8, activation=\"selu\", kernel_initializer=\"lecun_normal\")(x)\n",
    "outputs = tf.keras.layers.Dense(1)(x)\n",
    "model = tf.keras.models.Model(inputs=inputs,outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "618e6958",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain,ytrain,xtest,ytest = final_df[:42],final_y_zeros[:42],final_df[42:],final_y_zeros[42:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7f654aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "2/2 [==============================] - 0s 124ms/step - loss: 206.3576 - mae: 206.3576 - mse: 42631.5312 - val_loss: 208.8917 - val_mae: 208.8917 - val_mse: 43671.8438 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 203.6895 - mae: 203.6895 - mse: 41538.1055 - val_loss: 206.1554 - val_mae: 206.1554 - val_mse: 42535.3555 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 201.0051 - mae: 201.0051 - mse: 40451.4102 - val_loss: 203.4059 - val_mae: 203.4059 - val_mse: 41408.4492 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 198.3138 - mae: 198.3138 - mse: 39376.0078 - val_loss: 200.6421 - val_mae: 200.6421 - val_mse: 40290.9766 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 195.6032 - mae: 195.6032 - mse: 38306.2266 - val_loss: 197.8635 - val_mae: 197.8635 - val_mse: 39182.9219 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 192.8760 - mae: 192.8760 - mse: 37245.5078 - val_loss: 195.0682 - val_mae: 195.0682 - val_mse: 38083.7500 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 190.1421 - mae: 190.1421 - mse: 36198.5234 - val_loss: 192.2549 - val_mae: 192.2549 - val_mse: 36993.3398 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 187.3811 - mae: 187.3811 - mse: 35154.8359 - val_loss: 189.4235 - val_mae: 189.4235 - val_mse: 35911.8594 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 184.6096 - mae: 184.6096 - mse: 34124.8281 - val_loss: 186.5718 - val_mae: 186.5718 - val_mse: 34838.9023 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 181.8139 - mae: 181.8139 - mse: 33098.9492 - val_loss: 183.6994 - val_mae: 183.6994 - val_mse: 33774.5430 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 178.9965 - mae: 178.9965 - mse: 32080.3984 - val_loss: 180.8038 - val_mae: 180.8038 - val_mse: 32718.3359 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 176.1615 - mae: 176.1615 - mse: 31075.6777 - val_loss: 177.8833 - val_mae: 177.8833 - val_mse: 31670.0625 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 173.2940 - mae: 173.2940 - mse: 30070.3359 - val_loss: 174.9364 - val_mae: 174.9364 - val_mse: 30629.5957 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 170.4060 - mae: 170.4060 - mse: 29078.2324 - val_loss: 171.9600 - val_mae: 171.9600 - val_mse: 29596.3516 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 167.4867 - mae: 167.4867 - mse: 28090.4414 - val_loss: 168.9524 - val_mae: 168.9524 - val_mse: 28570.2754 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 164.5384 - mae: 164.5384 - mse: 27111.6641 - val_loss: 165.9113 - val_mae: 165.9113 - val_mse: 27551.1855 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 161.5564 - mae: 161.5564 - mse: 26137.4414 - val_loss: 162.8347 - val_mae: 162.8347 - val_mse: 26539.0391 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 158.5373 - mae: 158.5373 - mse: 25169.4375 - val_loss: 159.7200 - val_mae: 159.7200 - val_mse: 25533.6445 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 155.4788 - mae: 155.4788 - mse: 24207.4180 - val_loss: 156.5636 - val_mae: 156.5636 - val_mse: 24534.6074 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 152.3830 - mae: 152.3830 - mse: 23254.4727 - val_loss: 153.3621 - val_mae: 153.3621 - val_mse: 23541.6641 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 149.2452 - mae: 149.2452 - mse: 22308.1562 - val_loss: 150.1136 - val_mae: 150.1136 - val_mse: 22555.0938 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 146.0607 - mae: 146.0607 - mse: 21369.6094 - val_loss: 146.8163 - val_mae: 146.8163 - val_mse: 21575.3203 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 142.8246 - mae: 142.8246 - mse: 20432.0508 - val_loss: 143.4675 - val_mae: 143.4675 - val_mse: 20602.5020 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 139.5344 - mae: 139.5344 - mse: 19500.8105 - val_loss: 140.0626 - val_mae: 140.0626 - val_mse: 19636.3945 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 136.1981 - mae: 136.1981 - mse: 18582.9551 - val_loss: 136.5974 - val_mae: 136.5974 - val_mse: 18677.0078 - lr: 0.0010\n",
      "Epoch 26/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 132.7925 - mae: 132.7925 - mse: 17664.1934 - val_loss: 133.0692 - val_mae: 133.0692 - val_mse: 17724.8555 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 129.3307 - mae: 129.3307 - mse: 16755.2676 - val_loss: 129.4725 - val_mae: 129.4725 - val_mse: 16779.8672 - lr: 0.0010\n",
      "Epoch 28/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 125.8029 - mae: 125.8029 - mse: 15856.0391 - val_loss: 125.8041 - val_mae: 125.8041 - val_mse: 15842.7100 - lr: 0.0010\n",
      "Epoch 29/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 122.2010 - mae: 122.2010 - mse: 14961.4570 - val_loss: 122.0603 - val_mae: 122.0603 - val_mse: 14914.0771 - lr: 0.0010\n",
      "Epoch 30/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 118.5260 - mae: 118.5260 - mse: 14075.8643 - val_loss: 118.2363 - val_mae: 118.2363 - val_mse: 13994.4775 - lr: 0.0010\n",
      "Epoch 31/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 114.7653 - mae: 114.7653 - mse: 13195.8291 - val_loss: 114.3260 - val_mae: 114.3260 - val_mse: 13084.4160 - lr: 0.0010\n",
      "Epoch 32/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 110.9256 - mae: 110.9256 - mse: 12329.5039 - val_loss: 110.5457 - val_mae: 110.5457 - val_mse: 12233.6963 - lr: 0.0010\n",
      "Epoch 33/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 107.2800 - mae: 107.2800 - mse: 11535.0039 - val_loss: 107.0284 - val_mae: 107.0284 - val_mse: 11467.8438 - lr: 0.0010\n",
      "Epoch 34/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 103.8230 - mae: 103.8230 - mse: 10804.5908 - val_loss: 103.3991 - val_mae: 103.3991 - val_mse: 10703.5615 - lr: 0.0010\n",
      "Epoch 35/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 100.2555 - mae: 100.2555 - mse: 10077.2412 - val_loss: 99.6402 - val_mae: 99.6402 - val_mse: 9939.7695 - lr: 0.0010\n",
      "Epoch 36/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 96.5504 - mae: 96.5504 - mse: 9343.9893 - val_loss: 95.7565 - val_mae: 95.7565 - val_mse: 9180.3066 - lr: 0.0010\n",
      "Epoch 37/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 92.7395 - mae: 92.7395 - mse: 8625.5957 - val_loss: 91.7501 - val_mae: 91.7501 - val_mse: 8428.4932 - lr: 0.0010\n",
      "Epoch 38/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 88.8030 - mae: 88.8030 - mse: 7909.0991 - val_loss: 87.6256 - val_mae: 87.6256 - val_mse: 7688.0864 - lr: 0.0010\n",
      "Epoch 39/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 84.7442 - mae: 84.7442 - mse: 7201.3750 - val_loss: 83.3821 - val_mae: 83.3821 - val_mse: 6961.8364 - lr: 0.0010\n",
      "Epoch 40/1000\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 80.5804 - mae: 80.5804 - mse: 6514.9814 - val_loss: 79.0170 - val_mae: 79.0170 - val_mse: 6252.3691 - lr: 0.0010\n",
      "Epoch 41/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 76.2935 - mae: 76.2935 - mse: 5843.3281 - val_loss: 74.5302 - val_mae: 74.5302 - val_mse: 5562.8701 - lr: 0.0010\n",
      "Epoch 42/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 71.8782 - mae: 71.8782 - mse: 5183.9951 - val_loss: 69.9177 - val_mae: 69.9177 - val_mse: 4896.0596 - lr: 0.0010\n",
      "Epoch 43/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 67.3459 - mae: 67.3459 - mse: 4553.4150 - val_loss: 65.1733 - val_mae: 65.1733 - val_mse: 4254.6021 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 62.6880 - mae: 62.6880 - mse: 3949.2109 - val_loss: 60.2938 - val_mae: 60.2938 - val_mse: 3641.8689 - lr: 0.0010\n",
      "Epoch 45/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 57.8907 - mae: 57.8907 - mse: 3368.8169 - val_loss: 55.2758 - val_mae: 55.2758 - val_mse: 3061.4333 - lr: 0.0010\n",
      "Epoch 46/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 52.9622 - mae: 52.9622 - mse: 2823.2083 - val_loss: 50.1139 - val_mae: 50.1139 - val_mse: 2516.9324 - lr: 0.0010\n",
      "Epoch 47/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 47.8916 - mae: 47.8916 - mse: 2310.4363 - val_loss: 44.8042 - val_mae: 44.8042 - val_mse: 2012.4872 - lr: 0.0010\n",
      "Epoch 48/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 42.6771 - mae: 42.6771 - mse: 1840.8806 - val_loss: 39.3425 - val_mae: 39.3425 - val_mse: 1552.4679 - lr: 0.0010\n",
      "Epoch 49/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 37.3093 - mae: 37.3093 - mse: 1409.1299 - val_loss: 33.7240 - val_mae: 33.7240 - val_mse: 1141.5315 - lr: 0.0010\n",
      "Epoch 50/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 31.7831 - mae: 31.7831 - mse: 1025.5615 - val_loss: 27.9414 - val_mae: 27.9414 - val_mse: 784.5680 - lr: 0.0010\n",
      "Epoch 51/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 26.0960 - mae: 26.0960 - mse: 694.8987 - val_loss: 21.9865 - val_mae: 21.9865 - val_mse: 486.9115 - lr: 0.0010\n",
      "Epoch 52/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 20.2452 - mae: 20.2452 - mse: 426.3846 - val_loss: 15.8526 - val_mae: 15.8526 - val_mse: 254.5027 - lr: 0.0010\n",
      "Epoch 53/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 14.2232 - mae: 14.2232 - mse: 217.6457 - val_loss: 9.5357 - val_mae: 9.5357 - val_mse: 93.8626 - lr: 0.0010\n",
      "Epoch 54/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 8.1226 - mae: 8.1226 - mse: 75.7948 - val_loss: 3.0643 - val_mae: 3.0643 - val_mse: 12.1072 - lr: 0.0010\n",
      "Epoch 55/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 3.1427 - mae: 3.1427 - mse: 15.7965 - val_loss: 3.0564 - val_mae: 3.0564 - val_mse: 11.9042 - lr: 0.0010\n",
      "Epoch 56/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 4.4801 - mae: 4.4801 - mse: 28.2649 - val_loss: 7.1600 - val_mae: 7.1600 - val_mse: 53.7516 - lr: 0.0010\n",
      "Epoch 57/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 7.8582 - mae: 7.8582 - mse: 73.0398 - val_loss: 9.0021 - val_mae: 9.0021 - val_mse: 83.4965 - lr: 0.0010\n",
      "Epoch 58/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 9.4376 - mae: 9.4376 - mse: 100.3588 - val_loss: 9.0182 - val_mae: 9.0182 - val_mse: 83.7877 - lr: 0.0010\n",
      "Epoch 59/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 9.2741 - mae: 9.2741 - mse: 96.9403 - val_loss: 7.5936 - val_mae: 7.5936 - val_mse: 60.1427 - lr: 0.0010\n",
      "Epoch 60/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 7.7406 - mae: 7.7406 - mse: 71.9088 - val_loss: 5.0577 - val_mae: 5.0577 - val_mse: 28.1031 - lr: 0.0010\n",
      "Epoch 61/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 5.3029 - mae: 5.3029 - mse: 37.5319 - val_loss: 2.0192 - val_mae: 2.0192 - val_mse: 5.8497 - lr: 0.0010\n",
      "Epoch 62/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 3.0789 - mae: 3.0789 - mse: 16.0492 - val_loss: 2.0319 - val_mae: 2.0319 - val_mse: 5.4396 - lr: 0.0010\n",
      "Epoch 63/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2.9146 - mae: 2.9146 - mse: 14.9323 - val_loss: 4.0888 - val_mae: 4.0888 - val_mse: 19.4672 - lr: 0.0010\n",
      "Epoch 64/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 4.2363 - mae: 4.2363 - mse: 25.2063 - val_loss: 5.0087 - val_mae: 5.0087 - val_mse: 27.8640 - lr: 0.0010\n",
      "Epoch 65/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 4.8095 - mae: 4.8095 - mse: 30.5956 - val_loss: 4.6427 - val_mae: 4.6427 - val_mse: 24.3207 - lr: 0.0010\n",
      "Epoch 66/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 4.3983 - mae: 4.3983 - mse: 26.6761 - val_loss: 3.3408 - val_mae: 3.3408 - val_mse: 13.8871 - lr: 0.0010\n",
      "Epoch 67/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 3.4255 - mae: 3.4255 - mse: 17.7615 - val_loss: 1.9064 - val_mae: 1.9064 - val_mse: 4.6081 - lr: 0.0010\n",
      "Epoch 68/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.6358 - mae: 2.6358 - mse: 12.9478 - val_loss: 1.2398 - val_mae: 1.2398 - val_mse: 2.7875 - lr: 0.0010\n",
      "Epoch 69/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2.6300 - mae: 2.6300 - mse: 12.6794 - val_loss: 1.6840 - val_mae: 1.6840 - val_mse: 4.5513 - lr: 0.0010\n",
      "Epoch 70/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 3.0499 - mae: 3.0499 - mse: 15.7063 - val_loss: 1.8540 - val_mae: 1.8540 - val_mse: 5.1679 - lr: 0.0010\n",
      "Epoch 71/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 3.0972 - mae: 3.0972 - mse: 16.1150 - val_loss: 1.3931 - val_mae: 1.3931 - val_mse: 3.6768 - lr: 0.0010\n",
      "Epoch 72/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.8152 - mae: 2.8152 - mse: 13.9624 - val_loss: 1.3589 - val_mae: 1.3589 - val_mse: 2.6353 - lr: 0.0010\n",
      "Epoch 73/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.5288 - mae: 2.5288 - mse: 12.2699 - val_loss: 1.7041 - val_mae: 1.7041 - val_mse: 3.5749 - lr: 0.0010\n",
      "Epoch 74/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.5531 - mae: 2.5531 - mse: 12.1696 - val_loss: 1.8367 - val_mae: 1.8367 - val_mse: 4.2092 - lr: 0.0010\n",
      "Epoch 75/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.5896 - mae: 2.5896 - mse: 12.1925 - val_loss: 1.6937 - val_mae: 1.6937 - val_mse: 3.5322 - lr: 0.0010\n",
      "Epoch 76/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4749 - mae: 2.4749 - mse: 11.9194 - val_loss: 1.4332 - val_mae: 1.4332 - val_mse: 2.7236 - lr: 0.0010\n",
      "Epoch 77/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.4645 - mae: 2.4645 - mse: 11.7791 - val_loss: 1.2992 - val_mae: 1.2992 - val_mse: 2.6664 - lr: 0.0010\n",
      "Epoch 78/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.5641 - mae: 2.5641 - mse: 12.3458 - val_loss: 1.2665 - val_mae: 1.2665 - val_mse: 2.7221 - lr: 0.0010\n",
      "Epoch 79/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 2.6022 - mae: 2.6022 - mse: 12.5012 - val_loss: 1.3314 - val_mae: 1.3314 - val_mse: 2.6383 - lr: 0.0010\n",
      "Epoch 80/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.5312 - mae: 2.5312 - mse: 12.1452 - val_loss: 1.4027 - val_mae: 1.4027 - val_mse: 2.6704 - lr: 0.0010\n",
      "Epoch 81/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2.4881 - mae: 2.4881 - mse: 11.9042 - val_loss: 1.4694 - val_mae: 1.4694 - val_mse: 2.8179 - lr: 0.0010\n",
      "Epoch 82/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4560 - mae: 2.4560 - mse: 11.7907 - val_loss: 1.5351 - val_mae: 1.5351 - val_mse: 3.0020 - lr: 0.0010\n",
      "Epoch 83/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4439 - mae: 2.4439 - mse: 11.7579 - val_loss: 1.5323 - val_mae: 1.5323 - val_mse: 2.9946 - lr: 0.0010\n",
      "Epoch 84/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.4422 - mae: 2.4422 - mse: 11.7565 - val_loss: 1.4996 - val_mae: 1.4996 - val_mse: 2.9153 - lr: 0.0010\n",
      "Epoch 85/1000\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 2.4481 - mae: 2.4481 - mse: 11.7933 - val_loss: 1.4814 - val_mae: 1.4814 - val_mse: 2.8563 - lr: 0.0010\n",
      "Epoch 86/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4486 - mae: 2.4486 - mse: 11.7784 - val_loss: 1.5281 - val_mae: 1.5281 - val_mse: 2.9841 - lr: 0.0010\n",
      "Epoch 87/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4379 - mae: 2.4379 - mse: 11.7185 - val_loss: 1.6266 - val_mae: 1.6266 - val_mse: 3.2796 - lr: 0.0010\n",
      "Epoch 88/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4760 - mae: 2.4760 - mse: 11.8219 - val_loss: 1.6145 - val_mae: 1.6145 - val_mse: 3.2385 - lr: 0.0010\n",
      "Epoch 89/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 28ms/step - loss: 2.4542 - mae: 2.4542 - mse: 11.7311 - val_loss: 1.4395 - val_mae: 1.4395 - val_mse: 2.7378 - lr: 0.0010\n",
      "Epoch 90/1000\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 2.4922 - mae: 2.4922 - mse: 11.8467 - val_loss: 1.3112 - val_mae: 1.3112 - val_mse: 2.6529 - lr: 0.0010\n",
      "Epoch 91/1000\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 2.5483 - mae: 2.5483 - mse: 12.2336 - val_loss: 1.3298 - val_mae: 1.3298 - val_mse: 2.6391 - lr: 0.0010\n",
      "Epoch 92/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 2.5274 - mae: 2.5274 - mse: 12.1357 - val_loss: 1.4265 - val_mae: 1.4265 - val_mse: 2.7100 - lr: 0.0010\n",
      "Epoch 93/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4606 - mae: 2.4606 - mse: 11.8104 - val_loss: 1.5854 - val_mae: 1.5854 - val_mse: 3.1449 - lr: 0.0010\n",
      "Epoch 94/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4670 - mae: 2.4670 - mse: 11.7892 - val_loss: 1.6412 - val_mae: 1.6412 - val_mse: 3.3310 - lr: 0.0010\n",
      "Epoch 95/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.4902 - mae: 2.4902 - mse: 11.8563 - val_loss: 1.5235 - val_mae: 1.5235 - val_mse: 2.9725 - lr: 0.0010\n",
      "Epoch 96/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4416 - mae: 2.4416 - mse: 11.7461 - val_loss: 1.4761 - val_mae: 1.4761 - val_mse: 2.8390 - lr: 0.0010\n",
      "Epoch 97/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2.4525 - mae: 2.4525 - mse: 11.8045 - val_loss: 1.4453 - val_mae: 1.4453 - val_mse: 2.7515 - lr: 0.0010\n",
      "Epoch 98/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4747 - mae: 2.4747 - mse: 11.8557 - val_loss: 1.5429 - val_mae: 1.5429 - val_mse: 3.0228 - lr: 0.0010\n",
      "Epoch 99/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2.5010 - mae: 2.5010 - mse: 11.9258 - val_loss: 1.6865 - val_mae: 1.6865 - val_mse: 3.5032 - lr: 0.0010\n",
      "Epoch 100/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4959 - mae: 2.4959 - mse: 11.7961 - val_loss: 1.5234 - val_mae: 1.5234 - val_mse: 2.9721 - lr: 0.0010\n",
      "Epoch 101/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2.4439 - mae: 2.4439 - mse: 11.7061 - val_loss: 1.4253 - val_mae: 1.4253 - val_mse: 2.7075 - lr: 0.0010\n",
      "Epoch 102/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2.4695 - mae: 2.4695 - mse: 11.8360 - val_loss: 1.4135 - val_mae: 1.4135 - val_mse: 2.6865 - lr: 0.0010\n",
      "Epoch 103/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2.4791 - mae: 2.4791 - mse: 11.8903 - val_loss: 1.4325 - val_mae: 1.4325 - val_mse: 2.7221 - lr: 0.0010\n",
      "Epoch 104/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4702 - mae: 2.4702 - mse: 11.8469 - val_loss: 1.4556 - val_mae: 1.4556 - val_mse: 2.7781 - lr: 0.0010\n",
      "Epoch 105/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2.4599 - mae: 2.4599 - mse: 11.8128 - val_loss: 1.4825 - val_mae: 1.4825 - val_mse: 2.8600 - lr: 0.0010\n",
      "Epoch 106/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4498 - mae: 2.4498 - mse: 11.7697 - val_loss: 1.5618 - val_mae: 1.5618 - val_mse: 3.0750 - lr: 0.0010\n",
      "Epoch 107/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4548 - mae: 2.4548 - mse: 11.8500 - val_loss: 1.6470 - val_mae: 1.6470 - val_mse: 3.3519 - lr: 0.0010\n",
      "Epoch 108/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4817 - mae: 2.4817 - mse: 11.8275 - val_loss: 1.5448 - val_mae: 1.5448 - val_mse: 3.0279 - lr: 0.0010\n",
      "Epoch 109/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4535 - mae: 2.4535 - mse: 11.7944 - val_loss: 1.4325 - val_mae: 1.4325 - val_mse: 2.7222 - lr: 0.0010\n",
      "Epoch 110/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.4660 - mae: 2.4660 - mse: 11.8261 - val_loss: 1.4073 - val_mae: 1.4073 - val_mse: 2.6768 - lr: 0.0010\n",
      "Epoch 111/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4793 - mae: 2.4793 - mse: 11.8893 - val_loss: 1.4131 - val_mae: 1.4131 - val_mse: 2.6858 - lr: 0.0010\n",
      "Epoch 112/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4710 - mae: 2.4710 - mse: 11.8784 - val_loss: 1.4872 - val_mae: 1.4872 - val_mse: 2.8762 - lr: 0.0010\n",
      "Epoch 113/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.4593 - mae: 2.4593 - mse: 11.6389 - val_loss: 1.5807 - val_mae: 1.5807 - val_mse: 3.1309 - lr: 0.0010\n",
      "Epoch 114/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4579 - mae: 2.4579 - mse: 11.8035 - val_loss: 1.5314 - val_mae: 1.5314 - val_mse: 2.9927 - lr: 0.0010\n",
      "Epoch 115/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2.4517 - mae: 2.4517 - mse: 11.8086 - val_loss: 1.4826 - val_mae: 1.4826 - val_mse: 2.8604 - lr: 0.0010\n",
      "Epoch 116/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2.4547 - mae: 2.4547 - mse: 11.7864 - val_loss: 1.4112 - val_mae: 1.4112 - val_mse: 2.6828 - lr: 0.0010\n",
      "Epoch 117/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2.4778 - mae: 2.4778 - mse: 11.8670 - val_loss: 1.4046 - val_mae: 1.4046 - val_mse: 2.6730 - lr: 0.0010\n",
      "Epoch 118/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2.4820 - mae: 2.4820 - mse: 11.9194 - val_loss: 1.4377 - val_mae: 1.4377 - val_mse: 2.7334 - lr: 0.0010\n",
      "Epoch 119/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2.4493 - mae: 2.4493 - mse: 11.6961 - val_loss: 1.6177 - val_mae: 1.6177 - val_mse: 3.2492 - lr: 0.0010\n",
      "Epoch 120/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2.4782 - mae: 2.4782 - mse: 11.8231 - val_loss: 1.7246 - val_mae: 1.7246 - val_mse: 3.6628 - lr: 0.0010\n",
      "Epoch 121/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.5230 - mae: 2.5230 - mse: 11.9188 - val_loss: 1.5923 - val_mae: 1.5923 - val_mse: 3.1666 - lr: 0.0010\n",
      "Epoch 122/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4561 - mae: 2.4561 - mse: 11.7206 - val_loss: 1.3966 - val_mae: 1.3966 - val_mse: 2.6624 - lr: 0.0010\n",
      "Epoch 123/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.5033 - mae: 2.5033 - mse: 12.1235 - val_loss: 1.3290 - val_mae: 1.3290 - val_mse: 2.6393 - lr: 0.0010\n",
      "Epoch 124/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2.5225 - mae: 2.5225 - mse: 12.1177 - val_loss: 1.4244 - val_mae: 1.4244 - val_mse: 2.7058 - lr: 0.0010\n",
      "Epoch 125/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4696 - mae: 2.4696 - mse: 11.7737 - val_loss: 1.6482 - val_mae: 1.6482 - val_mse: 3.3565 - lr: 0.0010\n",
      "Epoch 126/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.5132 - mae: 2.5132 - mse: 11.8863 - val_loss: 1.6910 - val_mae: 1.6910 - val_mse: 3.5214 - lr: 0.0010\n",
      "Epoch 127/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4982 - mae: 2.4982 - mse: 11.8510 - val_loss: 1.4790 - val_mae: 1.4790 - val_mse: 2.8483 - lr: 0.0010\n",
      "Epoch 128/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2.4733 - mae: 2.4733 - mse: 12.0574 - val_loss: 1.3809 - val_mae: 1.3809 - val_mse: 2.6467 - lr: 0.0010\n",
      "Epoch 129/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4908 - mae: 2.4908 - mse: 11.9670 - val_loss: 1.4136 - val_mae: 1.4136 - val_mse: 2.6866 - lr: 0.0010\n",
      "Epoch 130/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4644 - mae: 2.4644 - mse: 11.7970 - val_loss: 1.5435 - val_mae: 1.5435 - val_mse: 3.0245 - lr: 0.0010\n",
      "Epoch 131/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.4436 - mae: 2.4436 - mse: 11.6569 - val_loss: 1.7136 - val_mae: 1.7136 - val_mse: 3.6154 - lr: 0.0010\n",
      "Epoch 132/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.5316 - mae: 2.5316 - mse: 11.9713 - val_loss: 1.6453 - val_mae: 1.6453 - val_mse: 3.3460 - lr: 0.0010\n",
      "Epoch 133/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.4614 - mae: 2.4614 - mse: 11.7660 - val_loss: 1.4345 - val_mae: 1.4345 - val_mse: 2.7263 - lr: 0.0010\n",
      "Epoch 134/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2.4591 - mae: 2.4591 - mse: 11.7884 - val_loss: 1.3437 - val_mae: 1.3437 - val_mse: 2.6344 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 135/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2.5161 - mae: 2.5161 - mse: 12.0702 - val_loss: 1.3191 - val_mae: 1.3191 - val_mse: 2.6456 - lr: 0.0010\n",
      "Epoch 136/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.5361 - mae: 2.5361 - mse: 12.2065 - val_loss: 1.3716 - val_mae: 1.3716 - val_mse: 2.6402 - lr: 0.0010\n",
      "Epoch 137/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4901 - mae: 2.4901 - mse: 11.8915 - val_loss: 1.4830 - val_mae: 1.4830 - val_mse: 2.8619 - lr: 0.0010\n",
      "Epoch 138/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2.4592 - mae: 2.4592 - mse: 11.8359 - val_loss: 1.6679 - val_mae: 1.6679 - val_mse: 3.4304 - lr: 0.0010\n",
      "Epoch 139/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2.5058 - mae: 2.5058 - mse: 11.9032 - val_loss: 1.6016 - val_mae: 1.6016 - val_mse: 3.1963 - lr: 0.0010\n",
      "Epoch 140/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.4288 - mae: 2.4288 - mse: 11.5567 - val_loss: 1.3603 - val_mae: 1.3603 - val_mse: 2.6353 - lr: 0.0010\n",
      "Epoch 141/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 2.5375 - mae: 2.5375 - mse: 12.3687 - val_loss: 1.2496 - val_mae: 1.2496 - val_mse: 2.7608 - lr: 0.0010\n",
      "Epoch 142/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2.6336 - mae: 2.6336 - mse: 12.7423 - val_loss: 1.3146 - val_mae: 1.3146 - val_mse: 2.6492 - lr: 0.0010\n",
      "Epoch 143/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.5331 - mae: 2.5331 - mse: 12.1821 - val_loss: 1.4310 - val_mae: 1.4310 - val_mse: 2.7189 - lr: 0.0010\n",
      "Epoch 144/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.5058 - mae: 2.5058 - mse: 12.1402 - val_loss: 1.6604 - val_mae: 1.6604 - val_mse: 3.4020 - lr: 0.0010\n",
      "Epoch 145/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.5053 - mae: 2.5053 - mse: 11.9191 - val_loss: 1.6150 - val_mae: 1.6150 - val_mse: 3.2403 - lr: 0.0010\n",
      "Epoch 146/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4590 - mae: 2.4590 - mse: 11.7336 - val_loss: 1.4512 - val_mae: 1.4512 - val_mse: 2.7662 - lr: 0.0010\n",
      "Epoch 147/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4671 - mae: 2.4671 - mse: 11.8141 - val_loss: 1.4456 - val_mae: 1.4456 - val_mse: 2.7521 - lr: 0.0010\n",
      "Epoch 148/1000\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 2.4628 - mae: 2.4628 - mse: 11.8357 - val_loss: 1.5153 - val_mae: 1.5153 - val_mse: 2.9524 - lr: 0.0010\n",
      "Epoch 149/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4433 - mae: 2.4433 - mse: 11.7718 - val_loss: 1.5801 - val_mae: 1.5801 - val_mse: 3.1291 - lr: 0.0010\n",
      "Epoch 150/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2.4561 - mae: 2.4561 - mse: 11.7889 - val_loss: 1.5657 - val_mae: 1.5657 - val_mse: 3.0865 - lr: 0.0010\n",
      "Epoch 151/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4627 - mae: 2.4627 - mse: 11.8477 - val_loss: 1.5902 - val_mae: 1.5902 - val_mse: 3.1601 - lr: 0.0010\n",
      "Epoch 152/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4693 - mae: 2.4693 - mse: 11.8209 - val_loss: 1.5411 - val_mae: 1.5411 - val_mse: 3.0182 - lr: 0.0010\n",
      "Epoch 153/1000\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.4434 - mae: 2.4434 - mse: 11.7563 - val_loss: 1.4186 - val_mae: 1.4186 - val_mse: 2.6951 - lr: 0.0010\n",
      "Epoch 154/1000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 2.4724 - mae: 2.4724 - mse: 11.8526 - val_loss: 1.3909 - val_mae: 1.3909 - val_mse: 2.6558 - lr: 0.0010\n",
      "Epoch 155/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.4855 - mae: 2.4855 - mse: 11.9226 - val_loss: 1.3882 - val_mae: 1.3882 - val_mse: 2.6531 - lr: 0.0010\n",
      "Epoch 156/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.4845 - mae: 2.4845 - mse: 11.9205 - val_loss: 1.4415 - val_mae: 1.4415 - val_mse: 2.7421 - lr: 0.0010\n",
      "Epoch 157/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.4471 - mae: 2.4471 - mse: 11.7146 - val_loss: 1.5561 - val_mae: 1.5561 - val_mse: 3.0592 - lr: 0.0010\n",
      "Epoch 158/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.4569 - mae: 2.4569 - mse: 11.7791 - val_loss: 1.5968 - val_mae: 1.5968 - val_mse: 3.1811 - lr: 0.0010\n",
      "Epoch 159/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4754 - mae: 2.4754 - mse: 11.9615 - val_loss: 1.4821 - val_mae: 1.4821 - val_mse: 2.8588 - lr: 0.0010\n",
      "Epoch 160/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4517 - mae: 2.4517 - mse: 11.7608 - val_loss: 1.5125 - val_mae: 1.5125 - val_mse: 2.9457 - lr: 0.0010\n",
      "Epoch 161/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4394 - mae: 2.4394 - mse: 11.7844 - val_loss: 1.5910 - val_mae: 1.5910 - val_mse: 3.1629 - lr: 0.0010\n",
      "Epoch 162/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.4577 - mae: 2.4577 - mse: 11.7844 - val_loss: 1.5013 - val_mae: 1.5013 - val_mse: 2.9194 - lr: 0.0010\n",
      "Epoch 163/1000\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 2.4529 - mae: 2.4529 - mse: 11.7539 - val_loss: 1.4284 - val_mae: 1.4284 - val_mse: 2.7135 - lr: 0.0010\n",
      "Epoch 164/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 2.4706 - mae: 2.4706 - mse: 11.8215 - val_loss: 1.4310 - val_mae: 1.4310 - val_mse: 2.7188 - lr: 0.0010\n",
      "Epoch 165/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.4681 - mae: 2.4681 - mse: 11.8348 - val_loss: 1.4760 - val_mae: 1.4760 - val_mse: 2.8384 - lr: 0.0010\n",
      "Epoch 166/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4880 - mae: 2.4880 - mse: 12.0144 - val_loss: 1.5723 - val_mae: 1.5723 - val_mse: 3.1061 - lr: 0.0010\n",
      "Epoch 167/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.4583 - mae: 2.4583 - mse: 11.7352 - val_loss: 1.4771 - val_mae: 1.4771 - val_mse: 2.8419 - lr: 0.0010\n",
      "Epoch 168/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.3926 - mae: 2.3926 - mse: 11.3244\n",
      "Epoch 00168: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.4478 - mae: 2.4478 - mse: 11.7672 - val_loss: 1.4501 - val_mae: 1.4501 - val_mse: 2.7634 - lr: 0.0010\n",
      "Epoch 169/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.4574 - mae: 2.4574 - mse: 11.7937 - val_loss: 1.4510 - val_mae: 1.4510 - val_mse: 2.7656 - lr: 1.0000e-04\n",
      "Epoch 170/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 2.4577 - mae: 2.4577 - mse: 11.7987 - val_loss: 1.4576 - val_mae: 1.4576 - val_mse: 2.7833 - lr: 1.0000e-04\n",
      "Epoch 171/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 2.4546 - mae: 2.4546 - mse: 11.7949 - val_loss: 1.4639 - val_mae: 1.4639 - val_mse: 2.8012 - lr: 1.0000e-04\n",
      "Epoch 172/1000\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 2.4506 - mae: 2.4506 - mse: 11.7766 - val_loss: 1.4705 - val_mae: 1.4705 - val_mse: 2.8213 - lr: 1.0000e-04\n",
      "Epoch 173/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.4481 - mae: 2.4481 - mse: 11.7629 - val_loss: 1.4827 - val_mae: 1.4827 - val_mse: 2.8607 - lr: 1.0000e-04\n",
      "Epoch 174/1000\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 2.4456 - mae: 2.4456 - mse: 11.7576 - val_loss: 1.4990 - val_mae: 1.4990 - val_mse: 2.9143 - lr: 1.0000e-04\n",
      "Epoch 175/1000\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 2.4432 - mae: 2.4432 - mse: 11.7418 - val_loss: 1.5277 - val_mae: 1.5277 - val_mse: 2.9833 - lr: 1.0000e-04\n",
      "Epoch 176/1000\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 2.4398 - mae: 2.4398 - mse: 11.7437 - val_loss: 1.5566 - val_mae: 1.5566 - val_mse: 3.0608 - lr: 1.0000e-04\n",
      "Epoch 177/1000\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 2.4462 - mae: 2.4462 - mse: 11.7694 - val_loss: 1.5782 - val_mae: 1.5782 - val_mse: 3.1235 - lr: 1.0000e-04\n",
      "Epoch 178/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 2.4526 - mae: 2.4526 - mse: 11.7691 - val_loss: 1.5864 - val_mae: 1.5864 - val_mse: 3.1487 - lr: 1.0000e-04\n",
      "Epoch 179/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 44ms/step - loss: 2.4555 - mae: 2.4555 - mse: 11.7760 - val_loss: 1.5860 - val_mae: 1.5860 - val_mse: 3.1475 - lr: 1.0000e-04\n",
      "Epoch 180/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.4551 - mae: 2.4551 - mse: 11.7752 - val_loss: 1.5806 - val_mae: 1.5806 - val_mse: 3.1309 - lr: 1.0000e-04\n",
      "Epoch 181/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4525 - mae: 2.4525 - mse: 11.7638 - val_loss: 1.5629 - val_mae: 1.5629 - val_mse: 3.0786 - lr: 1.0000e-04\n",
      "Epoch 182/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4463 - mae: 2.4463 - mse: 11.7690 - val_loss: 1.5289 - val_mae: 1.5289 - val_mse: 2.9865 - lr: 1.0000e-04\n",
      "Epoch 183/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4456 - mae: 2.4456 - mse: 11.7867 - val_loss: 1.5036 - val_mae: 1.5036 - val_mse: 2.9248 - lr: 1.0000e-04\n",
      "Epoch 184/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2.4446 - mae: 2.4446 - mse: 11.7545 - val_loss: 1.4963 - val_mae: 1.4963 - val_mse: 2.9080 - lr: 1.0000e-04\n",
      "Epoch 185/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2.4447 - mae: 2.4447 - mse: 11.7553 - val_loss: 1.4968 - val_mae: 1.4968 - val_mse: 2.9093 - lr: 1.0000e-04\n",
      "Epoch 186/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4450 - mae: 2.4450 - mse: 11.7580 - val_loss: 1.4977 - val_mae: 1.4977 - val_mse: 2.9112 - lr: 1.0000e-04\n",
      "Epoch 187/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4447 - mae: 2.4447 - mse: 11.7560 - val_loss: 1.4947 - val_mae: 1.4947 - val_mse: 2.9034 - lr: 1.0000e-04\n",
      "Epoch 188/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4449 - mae: 2.4449 - mse: 11.7534 - val_loss: 1.4916 - val_mae: 1.4916 - val_mse: 2.8921 - lr: 1.0000e-04\n",
      "Epoch 189/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4458 - mae: 2.4458 - mse: 11.7592 - val_loss: 1.4880 - val_mae: 1.4880 - val_mse: 2.8792 - lr: 1.0000e-04\n",
      "Epoch 190/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4458 - mae: 2.4458 - mse: 11.7645 - val_loss: 1.4811 - val_mae: 1.4811 - val_mse: 2.8554 - lr: 1.0000e-04\n",
      "Epoch 191/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.4468 - mae: 2.4468 - mse: 11.7598 - val_loss: 1.4770 - val_mae: 1.4770 - val_mse: 2.8418 - lr: 1.0000e-04\n",
      "Epoch 192/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2.4474 - mae: 2.4474 - mse: 11.7638 - val_loss: 1.4752 - val_mae: 1.4752 - val_mse: 2.8358 - lr: 1.0000e-04\n",
      "Epoch 193/1000\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 2.4477 - mae: 2.4477 - mse: 11.7668 - val_loss: 1.4751 - val_mae: 1.4751 - val_mse: 2.8355 - lr: 1.0000e-04\n",
      "Epoch 194/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 2.4480 - mae: 2.4480 - mse: 11.7709 - val_loss: 1.4751 - val_mae: 1.4751 - val_mse: 2.8358 - lr: 1.0000e-04\n",
      "Epoch 195/1000\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 2.4479 - mae: 2.4479 - mse: 11.7656 - val_loss: 1.4750 - val_mae: 1.4750 - val_mse: 2.8354 - lr: 1.0000e-04\n",
      "Epoch 196/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.4475 - mae: 2.4475 - mse: 11.7641 - val_loss: 1.4803 - val_mae: 1.4803 - val_mse: 2.8526 - lr: 1.0000e-04\n",
      "Epoch 197/1000\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.4464 - mae: 2.4464 - mse: 11.7657 - val_loss: 1.4899 - val_mae: 1.4899 - val_mse: 2.8861 - lr: 1.0000e-04\n",
      "Epoch 198/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2.4447 - mae: 2.4447 - mse: 11.7484 - val_loss: 1.5082 - val_mae: 1.5082 - val_mse: 2.9357 - lr: 1.0000e-04\n",
      "Epoch 199/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4470 - mae: 2.4470 - mse: 11.7739 - val_loss: 1.5289 - val_mae: 1.5289 - val_mse: 2.9865 - lr: 1.0000e-04\n",
      "Epoch 200/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2.4416 - mae: 2.4416 - mse: 11.7506 - val_loss: 1.5360 - val_mae: 1.5360 - val_mse: 3.0049 - lr: 1.0000e-04\n",
      "Epoch 201/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4418 - mae: 2.4418 - mse: 11.7553 - val_loss: 1.5404 - val_mae: 1.5404 - val_mse: 3.0163 - lr: 1.0000e-04\n",
      "Epoch 202/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4438 - mae: 2.4438 - mse: 11.7628 - val_loss: 1.5479 - val_mae: 1.5479 - val_mse: 3.0366 - lr: 1.0000e-04\n",
      "Epoch 203/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4437 - mae: 2.4437 - mse: 11.7507 - val_loss: 1.5632 - val_mae: 1.5632 - val_mse: 3.0796 - lr: 1.0000e-04\n",
      "Epoch 204/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4479 - mae: 2.4479 - mse: 11.7641 - val_loss: 1.5707 - val_mae: 1.5707 - val_mse: 3.1013 - lr: 1.0000e-04\n",
      "Epoch 205/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4503 - mae: 2.4503 - mse: 11.7679 - val_loss: 1.5670 - val_mae: 1.5670 - val_mse: 3.0906 - lr: 1.0000e-04\n",
      "Epoch 206/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.4486 - mae: 2.4486 - mse: 11.7614 - val_loss: 1.5525 - val_mae: 1.5525 - val_mse: 3.0494 - lr: 1.0000e-04\n",
      "Epoch 207/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 2.4467 - mae: 2.4467 - mse: 11.7851 - val_loss: 1.5315 - val_mae: 1.5315 - val_mse: 2.9931 - lr: 1.0000e-04\n",
      "Epoch 208/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 2.4412 - mae: 2.4412 - mse: 11.7521 - val_loss: 1.5176 - val_mae: 1.5176 - val_mse: 2.9581 - lr: 1.0000e-04\n",
      "Epoch 209/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 2.4428 - mae: 2.4428 - mse: 11.7486 - val_loss: 1.4996 - val_mae: 1.4996 - val_mse: 2.9156 - lr: 1.0000e-04\n",
      "Epoch 210/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.4452 - mae: 2.4452 - mse: 11.7609 - val_loss: 1.4919 - val_mae: 1.4919 - val_mse: 2.8932 - lr: 1.0000e-04\n",
      "Epoch 211/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.4454 - mae: 2.4454 - mse: 11.7569 - val_loss: 1.4902 - val_mae: 1.4902 - val_mse: 2.8869 - lr: 1.0000e-04\n",
      "Epoch 212/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4459 - mae: 2.4459 - mse: 11.7585 - val_loss: 1.4889 - val_mae: 1.4889 - val_mse: 2.8822 - lr: 1.0000e-04\n",
      "Epoch 213/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4456 - mae: 2.4456 - mse: 11.7573 - val_loss: 1.4934 - val_mae: 1.4934 - val_mse: 2.8988 - lr: 1.0000e-04\n",
      "Epoch 214/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.4445 - mae: 2.4445 - mse: 11.7611 - val_loss: 1.5077 - val_mae: 1.5077 - val_mse: 2.9345 - lr: 1.0000e-04\n",
      "Epoch 215/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 2.4437 - mae: 2.4437 - mse: 11.7527 - val_loss: 1.5275 - val_mae: 1.5275 - val_mse: 2.9829 - lr: 1.0000e-04\n",
      "Epoch 216/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 2.4421 - mae: 2.4421 - mse: 11.7641 - val_loss: 1.5405 - val_mae: 1.5405 - val_mse: 3.0168 - lr: 1.0000e-04\n",
      "Epoch 217/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.4432 - mae: 2.4432 - mse: 11.7583 - val_loss: 1.5347 - val_mae: 1.5347 - val_mse: 3.0015 - lr: 1.0000e-04\n",
      "Epoch 218/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4410 - mae: 2.4410 - mse: 11.7574 - val_loss: 1.5175 - val_mae: 1.5175 - val_mse: 2.9579 - lr: 1.0000e-04\n",
      "Epoch 219/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.4421 - mae: 2.4421 - mse: 11.7505 - val_loss: 1.4991 - val_mae: 1.4991 - val_mse: 2.9144 - lr: 1.0000e-04\n",
      "Epoch 220/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 2.4436 - mae: 2.4436 - mse: 11.7541 - val_loss: 1.4859 - val_mae: 1.4859 - val_mse: 2.8717 - lr: 1.0000e-04\n",
      "Epoch 221/1000\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 2.4453 - mae: 2.4453 - mse: 11.7559 - val_loss: 1.4735 - val_mae: 1.4735 - val_mse: 2.8306 - lr: 1.0000e-04\n",
      "Epoch 222/1000\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 2.4502 - mae: 2.4502 - mse: 11.7682 - val_loss: 1.4663 - val_mae: 1.4663 - val_mse: 2.8085 - lr: 1.0000e-04\n",
      "Epoch 223/1000\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 2.4501 - mae: 2.4501 - mse: 11.7747 - val_loss: 1.4709 - val_mae: 1.4709 - val_mse: 2.8223 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 224/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4488 - mae: 2.4488 - mse: 11.7690 - val_loss: 1.4809 - val_mae: 1.4809 - val_mse: 2.8546 - lr: 1.0000e-04\n",
      "Epoch 225/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.4469 - mae: 2.4469 - mse: 11.7616 - val_loss: 1.4906 - val_mae: 1.4906 - val_mse: 2.8886 - lr: 1.0000e-04\n",
      "Epoch 226/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.4448 - mae: 2.4448 - mse: 11.7557 - val_loss: 1.5055 - val_mae: 1.5055 - val_mse: 2.9292 - lr: 1.0000e-04\n",
      "Epoch 227/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 2.4452 - mae: 2.4452 - mse: 11.7407 - val_loss: 1.5255 - val_mae: 1.5255 - val_mse: 2.9778 - lr: 1.0000e-04\n",
      "Epoch 228/1000\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 2.4413 - mae: 2.4413 - mse: 11.7443 - val_loss: 1.5441 - val_mae: 1.5441 - val_mse: 3.0263 - lr: 1.0000e-04\n",
      "Epoch 229/1000\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 2.4460 - mae: 2.4460 - mse: 11.7491 - val_loss: 1.5576 - val_mae: 1.5576 - val_mse: 3.0637 - lr: 1.0000e-04\n",
      "Epoch 230/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.4463 - mae: 2.4463 - mse: 11.7621 - val_loss: 1.5520 - val_mae: 1.5520 - val_mse: 3.0481 - lr: 1.0000e-04\n",
      "Epoch 231/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 2.4459 - mae: 2.4459 - mse: 11.7790 - val_loss: 1.5429 - val_mae: 1.5429 - val_mse: 3.0231 - lr: 1.0000e-04\n",
      "Epoch 232/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4438 - mae: 2.4438 - mse: 11.7577 - val_loss: 1.5356 - val_mae: 1.5356 - val_mse: 3.0039 - lr: 1.0000e-04\n",
      "Epoch 233/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 2.4416 - mae: 2.4416 - mse: 11.7544 - val_loss: 1.5284 - val_mae: 1.5284 - val_mse: 2.9852 - lr: 1.0000e-04\n",
      "Epoch 234/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4416 - mae: 2.4416 - mse: 11.7539 - val_loss: 1.5177 - val_mae: 1.5177 - val_mse: 2.9586 - lr: 1.0000e-04\n",
      "Epoch 235/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4435 - mae: 2.4435 - mse: 11.7501 - val_loss: 1.5088 - val_mae: 1.5088 - val_mse: 2.9371 - lr: 1.0000e-04\n",
      "Epoch 236/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.4436 - mae: 2.4436 - mse: 11.7537 - val_loss: 1.5089 - val_mae: 1.5089 - val_mse: 2.9373 - lr: 1.0000e-04\n",
      "Epoch 237/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4436 - mae: 2.4436 - mse: 11.7539 - val_loss: 1.5116 - val_mae: 1.5116 - val_mse: 2.9437 - lr: 1.0000e-04\n",
      "Epoch 238/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.4432 - mae: 2.4432 - mse: 11.7534 - val_loss: 1.5188 - val_mae: 1.5188 - val_mse: 2.9611 - lr: 1.0000e-04\n",
      "Epoch 239/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4427 - mae: 2.4427 - mse: 11.7486 - val_loss: 1.5323 - val_mae: 1.5323 - val_mse: 2.9952 - lr: 1.0000e-04\n",
      "Epoch 240/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4435 - mae: 2.4435 - mse: 11.7763 - val_loss: 1.5379 - val_mae: 1.5379 - val_mse: 3.0098 - lr: 1.0000e-04\n",
      "Epoch 241/1000\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.4421 - mae: 2.4421 - mse: 11.7628 - val_loss: 1.5303 - val_mae: 1.5303 - val_mse: 2.9901 - lr: 1.0000e-04\n",
      "Epoch 242/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4418 - mae: 2.4418 - mse: 11.7555 - val_loss: 1.5342 - val_mae: 1.5342 - val_mse: 3.0002 - lr: 1.0000e-04\n",
      "Epoch 243/1000\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 2.4431 - mae: 2.4431 - mse: 11.7495 - val_loss: 1.5373 - val_mae: 1.5373 - val_mse: 3.0084 - lr: 1.0000e-04\n",
      "Epoch 244/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 2.4420 - mae: 2.4420 - mse: 11.7582 - val_loss: 1.5226 - val_mae: 1.5226 - val_mse: 2.9706 - lr: 1.0000e-04\n",
      "Epoch 245/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 2.4439 - mae: 2.4439 - mse: 11.7628 - val_loss: 1.5180 - val_mae: 1.5180 - val_mse: 2.9593 - lr: 1.0000e-04\n",
      "Epoch 246/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4428 - mae: 2.4428 - mse: 11.7521 - val_loss: 1.5269 - val_mae: 1.5269 - val_mse: 2.9815 - lr: 1.0000e-04\n",
      "Epoch 247/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4430 - mae: 2.4430 - mse: 11.7602 - val_loss: 1.5289 - val_mae: 1.5289 - val_mse: 2.9864 - lr: 1.0000e-04\n",
      "Epoch 248/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 2.4418 - mae: 2.4418 - mse: 11.7565 - val_loss: 1.5229 - val_mae: 1.5229 - val_mse: 2.9713 - lr: 1.0000e-04\n",
      "Epoch 249/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.4424 - mae: 2.4424 - mse: 11.7540 - val_loss: 1.5184 - val_mae: 1.5184 - val_mse: 2.9601 - lr: 1.0000e-04\n",
      "Epoch 250/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 2.4468 - mae: 2.4468 - mse: 11.7747 - val_loss: 1.5215 - val_mae: 1.5215 - val_mse: 2.9679 - lr: 1.0000e-04\n",
      "Epoch 251/1000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 2.4413 - mae: 2.4413 - mse: 11.7436 - val_loss: 1.5494 - val_mae: 1.5494 - val_mse: 3.0409 - lr: 1.0000e-04\n",
      "Epoch 252/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4446 - mae: 2.4446 - mse: 11.7615 - val_loss: 1.5695 - val_mae: 1.5695 - val_mse: 3.0977 - lr: 1.0000e-04\n",
      "Epoch 253/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4503 - mae: 2.4503 - mse: 11.7647 - val_loss: 1.5725 - val_mae: 1.5725 - val_mse: 3.1068 - lr: 1.0000e-04\n",
      "Epoch 254/1000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 2.4516 - mae: 2.4516 - mse: 11.7666 - val_loss: 1.5614 - val_mae: 1.5614 - val_mse: 3.0744 - lr: 1.0000e-04\n",
      "Epoch 255/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4495 - mae: 2.4495 - mse: 11.7748 - val_loss: 1.5511 - val_mae: 1.5511 - val_mse: 3.0454 - lr: 1.0000e-04\n",
      "Epoch 256/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4449 - mae: 2.4449 - mse: 11.7583 - val_loss: 1.5453 - val_mae: 1.5453 - val_mse: 3.0295 - lr: 1.0000e-04\n",
      "Epoch 257/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.4436 - mae: 2.4436 - mse: 11.7647 - val_loss: 1.5330 - val_mae: 1.5330 - val_mse: 2.9970 - lr: 1.0000e-04\n",
      "Epoch 258/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.4416 - mae: 2.4416 - mse: 11.7617 - val_loss: 1.5224 - val_mae: 1.5224 - val_mse: 2.9701 - lr: 1.0000e-04\n",
      "Epoch 259/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4425 - mae: 2.4425 - mse: 11.7538 - val_loss: 1.5121 - val_mae: 1.5121 - val_mse: 2.9449 - lr: 1.0000e-04\n",
      "Epoch 260/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 2.4442 - mae: 2.4442 - mse: 11.7583 - val_loss: 1.4984 - val_mae: 1.4984 - val_mse: 2.9129 - lr: 1.0000e-04\n",
      "Epoch 261/1000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 2.4445 - mae: 2.4445 - mse: 11.7568 - val_loss: 1.4954 - val_mae: 1.4954 - val_mse: 2.9061 - lr: 1.0000e-04\n",
      "Epoch 262/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.4462 - mae: 2.4462 - mse: 11.7595 - val_loss: 1.5031 - val_mae: 1.5031 - val_mse: 2.9236 - lr: 1.0000e-04\n",
      "Epoch 263/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.4457 - mae: 2.4457 - mse: 11.7831 - val_loss: 1.5250 - val_mae: 1.5250 - val_mse: 2.9768 - lr: 1.0000e-04\n",
      "Epoch 264/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.4431 - mae: 2.4431 - mse: 11.7578 - val_loss: 1.5376 - val_mae: 1.5376 - val_mse: 3.0091 - lr: 1.0000e-04\n",
      "Epoch 265/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.4420 - mae: 2.4420 - mse: 11.7555 - val_loss: 1.5326 - val_mae: 1.5326 - val_mse: 2.9962 - lr: 1.0000e-04\n",
      "Epoch 266/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 2.4415 - mae: 2.4415 - mse: 11.7491 - val_loss: 1.5262 - val_mae: 1.5262 - val_mse: 2.9796 - lr: 1.0000e-04\n",
      "Epoch 267/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.4426 - mae: 2.4426 - mse: 11.7570 - val_loss: 1.5188 - val_mae: 1.5188 - val_mse: 2.9613 - lr: 1.0000e-04\n",
      "Epoch 268/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5061 - mae: 2.5061 - mse: 12.1755\n",
      "Epoch 00268: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 2.4421 - mae: 2.4421 - mse: 11.7468 - val_loss: 1.5026 - val_mae: 1.5026 - val_mse: 2.9227 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss = \"mae\", optimizer = tf.keras.optimizers.Adam(), metrics=[\"mae\",\"mse\"])\n",
    "history = model.fit(xtrain,ytrain,\n",
    "                    epochs = 1000,\n",
    "                    validation_data=(xtest,ytest), \n",
    "                    callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", \n",
    "                                                            patience=200, \n",
    "                                                            restore_best_weights=True),\n",
    "                          tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", \n",
    "                                                               patience=100, \n",
    "                                                               verbose=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f399880b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 66ms/step - loss: 1.2398 - mae: 1.2398 - mse: 2.7875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.2398127317428589, 1.2398127317428589, 2.7874674797058105]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(xtest,ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae74616",
   "metadata": {},
   "source": [
    "# Training a conv1d model on the same data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "52ca0d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " inputs (InputLayer)         [(None, 71, 1)]           0         \n",
      "                                                                 \n",
      " conv1d_8 (Conv1D)           (None, 71, 64)            256       \n",
      "                                                                 \n",
      " max_pooling1d_6 (MaxPooling  (None, 35, 64)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_9 (Conv1D)           (None, 35, 64)            12352     \n",
      "                                                                 \n",
      " max_pooling1d_7 (MaxPooling  (None, 17, 64)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_10 (Conv1D)          (None, 17, 64)            12352     \n",
      "                                                                 \n",
      " max_pooling1d_8 (MaxPooling  (None, 8, 64)            0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_11 (Conv1D)          (None, 8, 64)             12352     \n",
      "                                                                 \n",
      " global_max_pooling1d_2 (Glo  (None, 64)               0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 37,377\n",
      "Trainable params: 37,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "inputs = tf.keras.Input(shape=(71,1), name=\"inputs\")\n",
    "x = tf.keras.layers.Conv1D(64, 3, activation=\"relu\", padding=\"causal\")(inputs)\n",
    "x = tf.keras.layers.MaxPooling1D()(x)\n",
    "x = tf.keras.layers.Conv1D(64, 3, activation=\"relu\", padding=\"causal\")(x)\n",
    "x = tf.keras.layers.MaxPooling1D()(x)\n",
    "x = tf.keras.layers.Conv1D(64, 3, activation=\"relu\", padding=\"causal\")(x)\n",
    "x = tf.keras.layers.MaxPooling1D()(x)\n",
    "x = tf.keras.layers.Conv1D(64, 3, activation=\"relu\", padding=\"causal\")(x)\n",
    "x = tf.keras.layers.GlobalMaxPooling1D()(x)\n",
    "outputs = tf.keras.layers.Dense(1)(x)\n",
    "model = tf.keras.models.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1c2ca963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "2/2 [==============================] - 1s 617ms/step - loss: 216.8970 - mae: 216.8970 - mse: 47096.8438 - val_loss: 214.8625 - val_mae: 214.8625 - val_mse: 46198.5664 - lr: 1.0000e-04\n",
      "Epoch 2/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 208.8444 - mae: 208.8444 - mse: 43668.6562 - val_loss: 207.1177 - val_mae: 207.1177 - val_mse: 42931.1602 - lr: 1.0000e-04\n",
      "Epoch 3/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 200.8758 - mae: 200.8758 - mse: 40403.9414 - val_loss: 199.3156 - val_mae: 199.3156 - val_mse: 39760.0391 - lr: 1.0000e-04\n",
      "Epoch 4/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 192.9569 - mae: 192.9569 - mse: 37283.7266 - val_loss: 191.7895 - val_mae: 191.7895 - val_mse: 36817.5898 - lr: 1.0000e-04\n",
      "Epoch 5/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 185.2302 - mae: 185.2302 - mse: 34358.7188 - val_loss: 184.5834 - val_mae: 184.5834 - val_mse: 34107.8711 - lr: 1.0000e-04\n",
      "Epoch 6/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 177.7157 - mae: 177.7157 - mse: 31626.7773 - val_loss: 177.3840 - val_mae: 177.3840 - val_mse: 31504.3555 - lr: 1.0000e-04\n",
      "Epoch 7/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 170.1313 - mae: 170.1313 - mse: 28991.6777 - val_loss: 170.2191 - val_mae: 170.2191 - val_mse: 29017.2930 - lr: 1.0000e-04\n",
      "Epoch 8/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 162.6907 - mae: 162.6907 - mse: 26511.8301 - val_loss: 163.1609 - val_mae: 163.1609 - val_mse: 26664.3242 - lr: 1.0000e-04\n",
      "Epoch 9/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 155.4401 - mae: 155.4401 - mse: 24208.0000 - val_loss: 156.1454 - val_mae: 156.1454 - val_mse: 24423.9395 - lr: 1.0000e-04\n",
      "Epoch 10/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 148.2156 - mae: 148.2156 - mse: 22011.7734 - val_loss: 148.9245 - val_mae: 148.9245 - val_mse: 22220.6152 - lr: 1.0000e-04\n",
      "Epoch 11/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 140.8580 - mae: 140.8580 - mse: 19880.3086 - val_loss: 141.6064 - val_mae: 141.6064 - val_mse: 20093.3281 - lr: 1.0000e-04\n",
      "Epoch 12/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 133.4635 - mae: 133.4635 - mse: 17858.5586 - val_loss: 134.1803 - val_mae: 134.1803 - val_mse: 18044.0312 - lr: 1.0000e-04\n",
      "Epoch 13/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 125.9029 - mae: 125.9029 - mse: 15888.1934 - val_loss: 126.3848 - val_mae: 126.3848 - val_mse: 16011.1709 - lr: 1.0000e-04\n",
      "Epoch 14/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 117.9900 - mae: 117.9900 - mse: 13961.8486 - val_loss: 118.2156 - val_mae: 118.2156 - val_mse: 14011.0879 - lr: 1.0000e-04\n",
      "Epoch 15/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 109.6654 - mae: 109.6654 - mse: 12064.0508 - val_loss: 109.6087 - val_mae: 109.6087 - val_mse: 12049.1787 - lr: 1.0000e-04\n",
      "Epoch 16/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 100.8712 - mae: 100.8712 - mse: 10212.9521 - val_loss: 100.5327 - val_mae: 100.5327 - val_mse: 10141.4746 - lr: 1.0000e-04\n",
      "Epoch 17/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 91.5198 - mae: 91.5198 - mse: 8410.9355 - val_loss: 90.9519 - val_mae: 90.9519 - val_mse: 8307.1631 - lr: 1.0000e-04\n",
      "Epoch 18/1000\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 81.6487 - mae: 81.6487 - mse: 6706.7397 - val_loss: 81.0439 - val_mae: 81.0439 - val_mse: 6603.9761 - lr: 1.0000e-04\n",
      "Epoch 19/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 71.4123 - mae: 71.4123 - mse: 5136.8564 - val_loss: 70.5985 - val_mae: 70.5985 - val_mse: 5019.3013 - lr: 1.0000e-04\n",
      "Epoch 20/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 60.6493 - mae: 60.6493 - mse: 3711.9419 - val_loss: 59.4314 - val_mae: 59.4314 - val_mse: 3566.9424 - lr: 1.0000e-04\n",
      "Epoch 21/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 49.1846 - mae: 49.1846 - mse: 2464.4871 - val_loss: 47.7749 - val_mae: 47.7749 - val_mse: 2317.3530 - lr: 1.0000e-04\n",
      "Epoch 22/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 37.1731 - mae: 37.1731 - mse: 1432.7838 - val_loss: 35.5265 - val_mae: 35.5265 - val_mse: 1297.0214 - lr: 1.0000e-04\n",
      "Epoch 23/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 24.5742 - mae: 24.5742 - mse: 639.0329 - val_loss: 22.4766 - val_mae: 22.4766 - val_mse: 540.1570 - lr: 1.0000e-04\n",
      "Epoch 24/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 11.2292 - mae: 11.2292 - mse: 168.4717 - val_loss: 9.1683 - val_mae: 9.1683 - val_mse: 109.8153 - lr: 1.0000e-04\n",
      "Epoch 25/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 6.2136 - mae: 6.2136 - mse: 58.9385 - val_loss: 4.3033 - val_mae: 4.3033 - val_mse: 40.9895 - lr: 1.0000e-04\n",
      "Epoch 26/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 13.6606 - mae: 13.6606 - mse: 231.3558 - val_loss: 8.5467 - val_mae: 8.5467 - val_mse: 107.2168 - lr: 1.0000e-04\n",
      "Epoch 27/1000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 19.0064 - mae: 19.0064 - mse: 406.6169 - val_loss: 9.6488 - val_mae: 9.6488 - val_mse: 127.7891 - lr: 1.0000e-04\n",
      "Epoch 28/1000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 19.5406 - mae: 19.5406 - mse: 429.0522 - val_loss: 7.2456 - val_mae: 7.2456 - val_mse: 85.8981 - lr: 1.0000e-04\n",
      "Epoch 29/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 16.5077 - mae: 16.5077 - mse: 318.1418 - val_loss: 4.4955 - val_mae: 4.4955 - val_mse: 39.6216 - lr: 1.0000e-04\n",
      "Epoch 30/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 11.0039 - mae: 11.0039 - mse: 166.6877 - val_loss: 7.3037 - val_mae: 7.3037 - val_mse: 61.4179 - lr: 1.0000e-04\n",
      "Epoch 31/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 7.0777 - mae: 7.0777 - mse: 70.0698 - val_loss: 11.5220 - val_mae: 11.5220 - val_mse: 168.8371 - lr: 1.0000e-04\n",
      "Epoch 32/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 5.6138 - mae: 5.6138 - mse: 48.7573 - val_loss: 16.1223 - val_mae: 16.1223 - val_mse: 296.2644 - lr: 1.0000e-04\n",
      "Epoch 33/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 7.9125 - mae: 7.9125 - mse: 90.6935 - val_loss: 18.1923 - val_mae: 18.1923 - val_mse: 366.6171 - lr: 1.0000e-04\n",
      "Epoch 34/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 9.2481 - mae: 9.2481 - mse: 117.9966 - val_loss: 17.7841 - val_mae: 17.7841 - val_mse: 351.1656 - lr: 1.0000e-04\n",
      "Epoch 35/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 8.7975 - mae: 8.7975 - mse: 108.6801 - val_loss: 15.4286 - val_mae: 15.4286 - val_mse: 272.0716 - lr: 1.0000e-04\n",
      "Epoch 36/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 6.9698 - mae: 6.9698 - mse: 71.1887 - val_loss: 11.7943 - val_mae: 11.7943 - val_mse: 172.1478 - lr: 1.0000e-04\n",
      "Epoch 37/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 5.2970 - mae: 5.2970 - mse: 43.2273 - val_loss: 8.7065 - val_mae: 8.7065 - val_mse: 98.5391 - lr: 1.0000e-04\n",
      "Epoch 38/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 5.2720 - mae: 5.2720 - mse: 38.7723 - val_loss: 7.0468 - val_mae: 7.0468 - val_mse: 61.6766 - lr: 1.0000e-04\n",
      "Epoch 39/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 6.0759 - mae: 6.0759 - mse: 52.6555 - val_loss: 6.3709 - val_mae: 6.3709 - val_mse: 48.1893 - lr: 1.0000e-04\n",
      "Epoch 40/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 6.3674 - mae: 6.3674 - mse: 60.2057 - val_loss: 6.5177 - val_mae: 6.5177 - val_mse: 52.1089 - lr: 1.0000e-04\n",
      "Epoch 41/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 5.9032 - mae: 5.9032 - mse: 51.1039 - val_loss: 7.5532 - val_mae: 7.5532 - val_mse: 73.3170 - lr: 1.0000e-04\n",
      "Epoch 42/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 5.2421 - mae: 5.2421 - mse: 37.6450 - val_loss: 9.3300 - val_mae: 9.3300 - val_mse: 109.5641 - lr: 1.0000e-04\n",
      "Epoch 43/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 4.7072 - mae: 4.7072 - mse: 32.1384 - val_loss: 10.7023 - val_mae: 10.7023 - val_mse: 140.6899 - lr: 1.0000e-04\n",
      "Epoch 44/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 36ms/step - loss: 4.7773 - mae: 4.7773 - mse: 33.1163 - val_loss: 11.1884 - val_mae: 11.1884 - val_mse: 150.5921 - lr: 1.0000e-04\n",
      "Epoch 45/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 4.7716 - mae: 4.7716 - mse: 33.7467 - val_loss: 10.5028 - val_mae: 10.5028 - val_mse: 134.9721 - lr: 1.0000e-04\n",
      "Epoch 46/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 4.5159 - mae: 4.5159 - mse: 29.7692 - val_loss: 9.1876 - val_mae: 9.1876 - val_mse: 105.3963 - lr: 1.0000e-04\n",
      "Epoch 47/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 4.2299 - mae: 4.2299 - mse: 25.6787 - val_loss: 8.0805 - val_mae: 8.0805 - val_mse: 82.6240 - lr: 1.0000e-04\n",
      "Epoch 48/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 4.2612 - mae: 4.2612 - mse: 27.0060 - val_loss: 7.6508 - val_mae: 7.6508 - val_mse: 74.6650 - lr: 1.0000e-04\n",
      "Epoch 49/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 4.1218 - mae: 4.1218 - mse: 25.7475 - val_loss: 8.0379 - val_mae: 8.0379 - val_mse: 81.6496 - lr: 1.0000e-04\n",
      "Epoch 50/1000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 3.9527 - mae: 3.9527 - mse: 23.8654 - val_loss: 8.7062 - val_mae: 8.7062 - val_mse: 94.8548 - lr: 1.0000e-04\n",
      "Epoch 51/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 3.9595 - mae: 3.9595 - mse: 23.5324 - val_loss: 9.2156 - val_mae: 9.2156 - val_mse: 105.8636 - lr: 1.0000e-04\n",
      "Epoch 52/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 3.9195 - mae: 3.9195 - mse: 23.2373 - val_loss: 8.8675 - val_mae: 8.8675 - val_mse: 98.2772 - lr: 1.0000e-04\n",
      "Epoch 53/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 3.7950 - mae: 3.7950 - mse: 22.0942 - val_loss: 8.0315 - val_mae: 8.0315 - val_mse: 81.4138 - lr: 1.0000e-04\n",
      "Epoch 54/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 3.7066 - mae: 3.7066 - mse: 21.5260 - val_loss: 7.4781 - val_mae: 7.4781 - val_mse: 71.2998 - lr: 1.0000e-04\n",
      "Epoch 55/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 3.7531 - mae: 3.7531 - mse: 22.1152 - val_loss: 7.4240 - val_mae: 7.4240 - val_mse: 70.3047 - lr: 1.0000e-04\n",
      "Epoch 56/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 3.7104 - mae: 3.7104 - mse: 21.6921 - val_loss: 7.9763 - val_mae: 7.9763 - val_mse: 80.2538 - lr: 1.0000e-04\n",
      "Epoch 57/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 3.5924 - mae: 3.5924 - mse: 20.6581 - val_loss: 8.5053 - val_mae: 8.5053 - val_mse: 90.6616 - lr: 1.0000e-04\n",
      "Epoch 58/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 3.5378 - mae: 3.5378 - mse: 20.2113 - val_loss: 8.6039 - val_mae: 8.6039 - val_mse: 92.6771 - lr: 1.0000e-04\n",
      "Epoch 59/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 3.5219 - mae: 3.5219 - mse: 20.0957 - val_loss: 8.2629 - val_mae: 8.2629 - val_mse: 85.6820 - lr: 1.0000e-04\n",
      "Epoch 60/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 3.4844 - mae: 3.4844 - mse: 19.5245 - val_loss: 7.8597 - val_mae: 7.8597 - val_mse: 77.8194 - lr: 1.0000e-04\n",
      "Epoch 61/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 3.4838 - mae: 3.4838 - mse: 19.2280 - val_loss: 7.9781 - val_mae: 7.9781 - val_mse: 79.9791 - lr: 1.0000e-04\n",
      "Epoch 62/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 3.4362 - mae: 3.4362 - mse: 18.9393 - val_loss: 8.3370 - val_mae: 8.3370 - val_mse: 86.9682 - lr: 1.0000e-04\n",
      "Epoch 63/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 3.4422 - mae: 3.4422 - mse: 19.1400 - val_loss: 8.3591 - val_mae: 8.3591 - val_mse: 87.3756 - lr: 1.0000e-04\n",
      "Epoch 64/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 3.3519 - mae: 3.3519 - mse: 18.3529 - val_loss: 7.7398 - val_mae: 7.7398 - val_mse: 75.2950 - lr: 1.0000e-04\n",
      "Epoch 65/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 3.3994 - mae: 3.3994 - mse: 18.2793 - val_loss: 7.3098 - val_mae: 7.3098 - val_mse: 67.5360 - lr: 1.0000e-04\n",
      "Epoch 66/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 3.4925 - mae: 3.4925 - mse: 18.8833 - val_loss: 7.3455 - val_mae: 7.3455 - val_mse: 68.1249 - lr: 1.0000e-04\n",
      "Epoch 67/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 3.4495 - mae: 3.4495 - mse: 18.8234 - val_loss: 8.0811 - val_mae: 8.0811 - val_mse: 81.7980 - lr: 1.0000e-04\n",
      "Epoch 68/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 3.2811 - mae: 3.2811 - mse: 17.3206 - val_loss: 9.1473 - val_mae: 9.1473 - val_mse: 100.0188 - lr: 1.0000e-04\n",
      "Epoch 69/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 3.4762 - mae: 3.4762 - mse: 19.0572 - val_loss: 8.9896 - val_mae: 8.9896 - val_mse: 96.9466 - lr: 1.0000e-04\n",
      "Epoch 70/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 3.3938 - mae: 3.3938 - mse: 18.3491 - val_loss: 8.4322 - val_mae: 8.4322 - val_mse: 87.0148 - lr: 1.0000e-04\n",
      "Epoch 71/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 3.2517 - mae: 3.2517 - mse: 17.1094 - val_loss: 8.2005 - val_mae: 8.2005 - val_mse: 82.9761 - lr: 1.0000e-04\n",
      "Epoch 72/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 3.2226 - mae: 3.2226 - mse: 16.7283 - val_loss: 7.7538 - val_mae: 7.7538 - val_mse: 75.6019 - lr: 1.0000e-04\n",
      "Epoch 73/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 3.2501 - mae: 3.2501 - mse: 16.7292 - val_loss: 7.7466 - val_mae: 7.7466 - val_mse: 75.2984 - lr: 1.0000e-04\n",
      "Epoch 74/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 3.2730 - mae: 3.2730 - mse: 16.9395 - val_loss: 7.7240 - val_mae: 7.7240 - val_mse: 74.7284 - lr: 1.0000e-04\n",
      "Epoch 75/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 3.2366 - mae: 3.2366 - mse: 16.5787 - val_loss: 7.6697 - val_mae: 7.6697 - val_mse: 73.6601 - lr: 1.0000e-04\n",
      "Epoch 76/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 3.2174 - mae: 3.2174 - mse: 15.9271 - val_loss: 8.3048 - val_mae: 8.3048 - val_mse: 83.5861 - lr: 1.0000e-04\n",
      "Epoch 77/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 3.1737 - mae: 3.1737 - mse: 16.1705 - val_loss: 8.1033 - val_mae: 8.1033 - val_mse: 80.0389 - lr: 1.0000e-04\n",
      "Epoch 78/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 3.1433 - mae: 3.1433 - mse: 15.7471 - val_loss: 7.7666 - val_mae: 7.7666 - val_mse: 74.4429 - lr: 1.0000e-04\n",
      "Epoch 79/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 3.1616 - mae: 3.1616 - mse: 16.1607 - val_loss: 8.0698 - val_mae: 8.0698 - val_mse: 79.0096 - lr: 1.0000e-04\n",
      "Epoch 80/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 3.0901 - mae: 3.0901 - mse: 15.5750 - val_loss: 7.9134 - val_mae: 7.9134 - val_mse: 76.2919 - lr: 1.0000e-04\n",
      "Epoch 81/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 3.0744 - mae: 3.0744 - mse: 15.4171 - val_loss: 7.8355 - val_mae: 7.8355 - val_mse: 74.8543 - lr: 1.0000e-04\n",
      "Epoch 82/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 3.0570 - mae: 3.0570 - mse: 15.2975 - val_loss: 7.8341 - val_mae: 7.8341 - val_mse: 74.6292 - lr: 1.0000e-04\n",
      "Epoch 83/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 3.0361 - mae: 3.0361 - mse: 15.0612 - val_loss: 8.0979 - val_mae: 8.0979 - val_mse: 78.6337 - lr: 1.0000e-04\n",
      "Epoch 84/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 3.0415 - mae: 3.0415 - mse: 14.9055 - val_loss: 7.9916 - val_mae: 7.9916 - val_mse: 76.7461 - lr: 1.0000e-04\n",
      "Epoch 85/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 3.0126 - mae: 3.0126 - mse: 14.9509 - val_loss: 7.8139 - val_mae: 7.8139 - val_mse: 73.7706 - lr: 1.0000e-04\n",
      "Epoch 86/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.9796 - mae: 2.9796 - mse: 14.4859 - val_loss: 8.0800 - val_mae: 8.0800 - val_mse: 77.8495 - lr: 1.0000e-04\n",
      "Epoch 87/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 3.0017 - mae: 3.0017 - mse: 14.5807 - val_loss: 8.4436 - val_mae: 8.4436 - val_mse: 83.7174 - lr: 1.0000e-04\n",
      "Epoch 88/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 3.0536 - mae: 3.0536 - mse: 14.5799 - val_loss: 8.4127 - val_mae: 8.4127 - val_mse: 83.0697 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.9191 - mae: 2.9191 - mse: 13.8047 - val_loss: 6.9535 - val_mae: 6.9535 - val_mse: 60.4890 - lr: 1.0000e-04\n",
      "Epoch 90/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 3.1504 - mae: 3.1504 - mse: 15.8846 - val_loss: 6.0606 - val_mae: 6.0606 - val_mse: 47.6676 - lr: 1.0000e-04\n",
      "Epoch 91/1000\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 3.2438 - mae: 3.2438 - mse: 18.1153 - val_loss: 6.9922 - val_mae: 6.9922 - val_mse: 60.7736 - lr: 1.0000e-04\n",
      "Epoch 92/1000\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 3.0289 - mae: 3.0289 - mse: 14.7432 - val_loss: 8.4686 - val_mae: 8.4686 - val_mse: 83.5169 - lr: 1.0000e-04\n",
      "Epoch 93/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 2.9726 - mae: 2.9726 - mse: 13.5917 - val_loss: 8.4896 - val_mae: 8.4896 - val_mse: 83.7432 - lr: 1.0000e-04\n",
      "Epoch 94/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.9209 - mae: 2.9209 - mse: 13.2445 - val_loss: 7.5688 - val_mae: 7.5688 - val_mse: 68.8126 - lr: 1.0000e-04\n",
      "Epoch 95/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.8584 - mae: 2.8584 - mse: 13.4588 - val_loss: 7.2901 - val_mae: 7.2901 - val_mse: 64.5684 - lr: 1.0000e-04\n",
      "Epoch 96/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2.8714 - mae: 2.8714 - mse: 13.7328 - val_loss: 7.3337 - val_mae: 7.3337 - val_mse: 65.1168 - lr: 1.0000e-04\n",
      "Epoch 97/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2.8569 - mae: 2.8569 - mse: 13.4582 - val_loss: 7.6311 - val_mae: 7.6311 - val_mse: 69.4805 - lr: 1.0000e-04\n",
      "Epoch 98/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.8194 - mae: 2.8194 - mse: 13.0128 - val_loss: 8.2022 - val_mae: 8.2022 - val_mse: 78.4393 - lr: 1.0000e-04\n",
      "Epoch 99/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.8693 - mae: 2.8693 - mse: 12.7805 - val_loss: 8.3357 - val_mae: 8.3357 - val_mse: 80.5507 - lr: 1.0000e-04\n",
      "Epoch 100/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.8242 - mae: 2.8242 - mse: 12.5134 - val_loss: 7.2452 - val_mae: 7.2452 - val_mse: 63.4281 - lr: 1.0000e-04\n",
      "Epoch 101/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.8165 - mae: 2.8165 - mse: 13.5011 - val_loss: 6.3530 - val_mae: 6.3530 - val_mse: 51.1768 - lr: 1.0000e-04\n",
      "Epoch 102/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 3.0081 - mae: 3.0081 - mse: 15.7244 - val_loss: 6.7374 - val_mae: 6.7374 - val_mse: 56.1142 - lr: 1.0000e-04\n",
      "Epoch 103/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.8779 - mae: 2.8779 - mse: 14.1901 - val_loss: 7.8315 - val_mae: 7.8315 - val_mse: 71.9744 - lr: 1.0000e-04\n",
      "Epoch 104/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2.7778 - mae: 2.7778 - mse: 12.5505 - val_loss: 8.1839 - val_mae: 8.1839 - val_mse: 77.5440 - lr: 1.0000e-04\n",
      "Epoch 105/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2.8203 - mae: 2.8203 - mse: 12.4586 - val_loss: 7.4851 - val_mae: 7.4851 - val_mse: 66.5066 - lr: 1.0000e-04\n",
      "Epoch 106/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.8123 - mae: 2.8123 - mse: 13.0803 - val_loss: 6.6720 - val_mae: 6.6720 - val_mse: 54.8915 - lr: 1.0000e-04\n",
      "Epoch 107/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2.8311 - mae: 2.8311 - mse: 14.2111 - val_loss: 7.1889 - val_mae: 7.1889 - val_mse: 61.9760 - lr: 1.0000e-04\n",
      "Epoch 108/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2.7654 - mae: 2.7654 - mse: 13.1759 - val_loss: 7.8002 - val_mae: 7.8002 - val_mse: 71.0600 - lr: 1.0000e-04\n",
      "Epoch 109/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2.7383 - mae: 2.7383 - mse: 12.2769 - val_loss: 7.7749 - val_mae: 7.7749 - val_mse: 70.5631 - lr: 1.0000e-04\n",
      "Epoch 110/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.7158 - mae: 2.7158 - mse: 12.1416 - val_loss: 7.3451 - val_mae: 7.3451 - val_mse: 63.9507 - lr: 1.0000e-04\n",
      "Epoch 111/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.6972 - mae: 2.6972 - mse: 12.5958 - val_loss: 6.7941 - val_mae: 6.7941 - val_mse: 56.0380 - lr: 1.0000e-04\n",
      "Epoch 112/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.7932 - mae: 2.7932 - mse: 13.7573 - val_loss: 6.8073 - val_mae: 6.8073 - val_mse: 56.1263 - lr: 1.0000e-04\n",
      "Epoch 113/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.7765 - mae: 2.7765 - mse: 13.2071 - val_loss: 7.6488 - val_mae: 7.6488 - val_mse: 68.2353 - lr: 1.0000e-04\n",
      "Epoch 114/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.6803 - mae: 2.6803 - mse: 11.8322 - val_loss: 8.3262 - val_mae: 8.3262 - val_mse: 78.9977 - lr: 1.0000e-04\n",
      "Epoch 115/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.7707 - mae: 2.7707 - mse: 11.8659 - val_loss: 8.0712 - val_mae: 8.0712 - val_mse: 74.7195 - lr: 1.0000e-04\n",
      "Epoch 116/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.7456 - mae: 2.7456 - mse: 12.1778 - val_loss: 7.1633 - val_mae: 7.1633 - val_mse: 60.7736 - lr: 1.0000e-04\n",
      "Epoch 117/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.6604 - mae: 2.6604 - mse: 12.4722 - val_loss: 7.0622 - val_mae: 7.0622 - val_mse: 59.2124 - lr: 1.0000e-04\n",
      "Epoch 118/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.6689 - mae: 2.6689 - mse: 12.7613 - val_loss: 7.4027 - val_mae: 7.4027 - val_mse: 64.0151 - lr: 1.0000e-04\n",
      "Epoch 119/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.5988 - mae: 2.5988 - mse: 11.7197 - val_loss: 8.2540 - val_mae: 8.2540 - val_mse: 77.2404 - lr: 1.0000e-04\n",
      "Epoch 120/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.7555 - mae: 2.7555 - mse: 11.7433 - val_loss: 8.3557 - val_mae: 8.3557 - val_mse: 78.8223 - lr: 1.0000e-04\n",
      "Epoch 121/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.7082 - mae: 2.7082 - mse: 11.5076 - val_loss: 7.1507 - val_mae: 7.1507 - val_mse: 60.0280 - lr: 1.0000e-04\n",
      "Epoch 122/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.6252 - mae: 2.6252 - mse: 12.3667 - val_loss: 6.3816 - val_mae: 6.3816 - val_mse: 49.5330 - lr: 1.0000e-04\n",
      "Epoch 123/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.7555 - mae: 2.7555 - mse: 13.5754 - val_loss: 7.5886 - val_mae: 7.5886 - val_mse: 66.3337 - lr: 1.0000e-04\n",
      "Epoch 124/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.5759 - mae: 2.5759 - mse: 11.3729 - val_loss: 8.1890 - val_mae: 8.1890 - val_mse: 75.7114 - lr: 1.0000e-04\n",
      "Epoch 125/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8732 - mae: 2.8732 - mse: 12.4562\n",
      "Epoch 00125: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.6866 - mae: 2.6866 - mse: 11.1785 - val_loss: 7.8558 - val_mae: 7.8558 - val_mse: 70.2807 - lr: 1.0000e-04\n",
      "Epoch 126/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.5864 - mae: 2.5864 - mse: 11.0948 - val_loss: 7.7154 - val_mae: 7.7154 - val_mse: 68.0834 - lr: 1.0000e-05\n",
      "Epoch 127/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.5635 - mae: 2.5635 - mse: 11.1953 - val_loss: 7.5117 - val_mae: 7.5117 - val_mse: 64.9686 - lr: 1.0000e-05\n",
      "Epoch 128/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.5673 - mae: 2.5673 - mse: 11.4956 - val_loss: 7.3540 - val_mae: 7.3540 - val_mse: 62.6118 - lr: 1.0000e-05\n",
      "Epoch 129/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.5496 - mae: 2.5496 - mse: 11.5975 - val_loss: 7.3060 - val_mae: 7.3060 - val_mse: 61.8969 - lr: 1.0000e-05\n",
      "Epoch 130/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.5478 - mae: 2.5478 - mse: 11.6751 - val_loss: 7.2994 - val_mae: 7.2994 - val_mse: 61.7886 - lr: 1.0000e-05\n",
      "Epoch 131/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.5460 - mae: 2.5460 - mse: 11.6589 - val_loss: 7.3073 - val_mae: 7.3073 - val_mse: 61.8910 - lr: 1.0000e-05\n",
      "Epoch 132/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.5464 - mae: 2.5464 - mse: 11.6663 - val_loss: 7.2577 - val_mae: 7.2577 - val_mse: 61.1530 - lr: 1.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 133/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.5416 - mae: 2.5416 - mse: 11.7647 - val_loss: 7.1446 - val_mae: 7.1446 - val_mse: 59.5045 - lr: 1.0000e-05\n",
      "Epoch 134/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.5488 - mae: 2.5488 - mse: 11.9330 - val_loss: 7.0887 - val_mae: 7.0887 - val_mse: 58.6917 - lr: 1.0000e-05\n",
      "Epoch 135/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.5577 - mae: 2.5577 - mse: 12.0445 - val_loss: 7.0860 - val_mae: 7.0860 - val_mse: 58.6357 - lr: 1.0000e-05\n",
      "Epoch 136/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 2.5565 - mae: 2.5565 - mse: 12.0470 - val_loss: 7.1288 - val_mae: 7.1288 - val_mse: 59.2302 - lr: 1.0000e-05\n",
      "Epoch 137/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.5394 - mae: 2.5394 - mse: 11.9326 - val_loss: 7.2195 - val_mae: 7.2195 - val_mse: 60.5234 - lr: 1.0000e-05\n",
      "Epoch 138/1000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 2.5324 - mae: 2.5324 - mse: 11.7540 - val_loss: 7.3642 - val_mae: 7.3642 - val_mse: 62.6252 - lr: 1.0000e-05\n",
      "Epoch 139/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.5312 - mae: 2.5312 - mse: 11.5447 - val_loss: 7.4655 - val_mae: 7.4655 - val_mse: 64.1177 - lr: 1.0000e-05\n",
      "Epoch 140/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.5372 - mae: 2.5372 - mse: 11.3817 - val_loss: 7.5201 - val_mae: 7.5201 - val_mse: 64.9293 - lr: 1.0000e-05\n",
      "Epoch 141/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.5376 - mae: 2.5376 - mse: 11.3202 - val_loss: 7.5179 - val_mae: 7.5179 - val_mse: 64.8871 - lr: 1.0000e-05\n",
      "Epoch 142/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.5371 - mae: 2.5371 - mse: 11.3133 - val_loss: 7.4849 - val_mae: 7.4849 - val_mse: 64.3827 - lr: 1.0000e-05\n",
      "Epoch 143/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.5292 - mae: 2.5292 - mse: 11.3357 - val_loss: 7.3684 - val_mae: 7.3684 - val_mse: 62.6404 - lr: 1.0000e-05\n",
      "Epoch 144/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.5170 - mae: 2.5170 - mse: 11.4772 - val_loss: 7.1751 - val_mae: 7.1751 - val_mse: 59.8184 - lr: 1.0000e-05\n",
      "Epoch 145/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.5282 - mae: 2.5282 - mse: 11.8543 - val_loss: 6.9713 - val_mae: 6.9713 - val_mse: 56.9236 - lr: 1.0000e-05\n",
      "Epoch 146/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.5595 - mae: 2.5595 - mse: 12.2341 - val_loss: 6.9008 - val_mae: 6.9008 - val_mse: 55.9363 - lr: 1.0000e-05\n",
      "Epoch 147/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.5771 - mae: 2.5771 - mse: 12.3995 - val_loss: 6.9229 - val_mae: 6.9229 - val_mse: 56.2351 - lr: 1.0000e-05\n",
      "Epoch 148/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.5678 - mae: 2.5678 - mse: 12.3216 - val_loss: 7.0715 - val_mae: 7.0715 - val_mse: 58.3114 - lr: 1.0000e-05\n",
      "Epoch 149/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2.5257 - mae: 2.5257 - mse: 11.9759 - val_loss: 7.2234 - val_mae: 7.2234 - val_mse: 60.4801 - lr: 1.0000e-05\n",
      "Epoch 150/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.5097 - mae: 2.5097 - mse: 11.6614 - val_loss: 7.3574 - val_mae: 7.3574 - val_mse: 62.4281 - lr: 1.0000e-05\n",
      "Epoch 151/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.5141 - mae: 2.5141 - mse: 11.4428 - val_loss: 7.4977 - val_mae: 7.4977 - val_mse: 64.5085 - lr: 1.0000e-05\n",
      "Epoch 152/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.5322 - mae: 2.5322 - mse: 11.3326 - val_loss: 7.6020 - val_mae: 7.6020 - val_mse: 66.0801 - lr: 1.0000e-05\n",
      "Epoch 153/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.5462 - mae: 2.5462 - mse: 11.1948 - val_loss: 7.5894 - val_mae: 7.5894 - val_mse: 65.8858 - lr: 1.0000e-05\n",
      "Epoch 154/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.5371 - mae: 2.5371 - mse: 11.2045 - val_loss: 7.4556 - val_mae: 7.4556 - val_mse: 63.8694 - lr: 1.0000e-05\n",
      "Epoch 155/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.5185 - mae: 2.5185 - mse: 11.3490 - val_loss: 7.2517 - val_mae: 7.2517 - val_mse: 60.8638 - lr: 1.0000e-05\n",
      "Epoch 156/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.5077 - mae: 2.5077 - mse: 11.6698 - val_loss: 7.0872 - val_mae: 7.0872 - val_mse: 58.5012 - lr: 1.0000e-05\n",
      "Epoch 157/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.5256 - mae: 2.5256 - mse: 11.9582 - val_loss: 7.0769 - val_mae: 7.0769 - val_mse: 58.3490 - lr: 1.0000e-05\n",
      "Epoch 158/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.5175 - mae: 2.5175 - mse: 11.9057 - val_loss: 7.2044 - val_mae: 7.2044 - val_mse: 60.1658 - lr: 1.0000e-05\n",
      "Epoch 159/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.5017 - mae: 2.5017 - mse: 11.6546 - val_loss: 7.3610 - val_mae: 7.3610 - val_mse: 62.4422 - lr: 1.0000e-05\n",
      "Epoch 160/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.5184 - mae: 2.5184 - mse: 11.4394 - val_loss: 7.4732 - val_mae: 7.4732 - val_mse: 64.1003 - lr: 1.0000e-05\n",
      "Epoch 161/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.5182 - mae: 2.5182 - mse: 11.2923 - val_loss: 7.4784 - val_mae: 7.4784 - val_mse: 64.1730 - lr: 1.0000e-05\n",
      "Epoch 162/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.5178 - mae: 2.5178 - mse: 11.2830 - val_loss: 7.4262 - val_mae: 7.4262 - val_mse: 63.3888 - lr: 1.0000e-05\n",
      "Epoch 163/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.5085 - mae: 2.5085 - mse: 11.3403 - val_loss: 7.2872 - val_mae: 7.2872 - val_mse: 61.3391 - lr: 1.0000e-05\n",
      "Epoch 164/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4976 - mae: 2.4976 - mse: 11.5555 - val_loss: 7.1201 - val_mae: 7.1201 - val_mse: 58.9283 - lr: 1.0000e-05\n",
      "Epoch 165/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.5115 - mae: 2.5115 - mse: 11.8481 - val_loss: 7.0481 - val_mae: 7.0481 - val_mse: 57.9042 - lr: 1.0000e-05\n",
      "Epoch 166/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.5267 - mae: 2.5267 - mse: 11.9857 - val_loss: 7.1022 - val_mae: 7.1022 - val_mse: 58.6640 - lr: 1.0000e-05\n",
      "Epoch 167/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.5098 - mae: 2.5098 - mse: 11.8564 - val_loss: 7.1615 - val_mae: 7.1615 - val_mse: 59.5039 - lr: 1.0000e-05\n",
      "Epoch 168/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.5081 - mae: 2.5081 - mse: 11.7441 - val_loss: 7.2082 - val_mae: 7.2082 - val_mse: 60.1686 - lr: 1.0000e-05\n",
      "Epoch 169/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4937 - mae: 2.4937 - mse: 11.6352 - val_loss: 7.1727 - val_mae: 7.1727 - val_mse: 59.6510 - lr: 1.0000e-05\n",
      "Epoch 170/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4948 - mae: 2.4948 - mse: 11.6909 - val_loss: 7.1385 - val_mae: 7.1385 - val_mse: 59.1504 - lr: 1.0000e-05\n",
      "Epoch 171/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.4990 - mae: 2.4990 - mse: 11.7537 - val_loss: 7.1223 - val_mae: 7.1223 - val_mse: 58.9093 - lr: 1.0000e-05\n",
      "Epoch 172/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 2.4994 - mae: 2.4994 - mse: 11.7774 - val_loss: 7.1445 - val_mae: 7.1445 - val_mse: 59.2194 - lr: 1.0000e-05\n",
      "Epoch 173/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4915 - mae: 2.4915 - mse: 11.7064 - val_loss: 7.2331 - val_mae: 7.2331 - val_mse: 60.4885 - lr: 1.0000e-05\n",
      "Epoch 174/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4864 - mae: 2.4864 - mse: 11.5472 - val_loss: 7.3296 - val_mae: 7.3296 - val_mse: 61.8872 - lr: 1.0000e-05\n",
      "Epoch 175/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4891 - mae: 2.4891 - mse: 11.3733 - val_loss: 7.3902 - val_mae: 7.3902 - val_mse: 62.7693 - lr: 1.0000e-05\n",
      "Epoch 176/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.4918 - mae: 2.4918 - mse: 11.3111 - val_loss: 7.4173 - val_mae: 7.4173 - val_mse: 63.1618 - lr: 1.0000e-05\n",
      "Epoch 177/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4933 - mae: 2.4933 - mse: 11.2676 - val_loss: 7.4316 - val_mae: 7.4316 - val_mse: 63.3660 - lr: 1.0000e-05\n",
      "Epoch 178/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.4939 - mae: 2.4939 - mse: 11.2518 - val_loss: 7.4080 - val_mae: 7.4080 - val_mse: 63.0084 - lr: 1.0000e-05\n",
      "Epoch 179/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 2.4901 - mae: 2.4901 - mse: 11.3008 - val_loss: 7.3532 - val_mae: 7.3532 - val_mse: 62.1884 - lr: 1.0000e-05\n",
      "Epoch 180/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4815 - mae: 2.4815 - mse: 11.3781 - val_loss: 7.2606 - val_mae: 7.2606 - val_mse: 60.8243 - lr: 1.0000e-05\n",
      "Epoch 181/1000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 2.4801 - mae: 2.4801 - mse: 11.5047 - val_loss: 7.1692 - val_mae: 7.1692 - val_mse: 59.4950 - lr: 1.0000e-05\n",
      "Epoch 182/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4847 - mae: 2.4847 - mse: 11.6666 - val_loss: 7.1970 - val_mae: 7.1970 - val_mse: 59.8866 - lr: 1.0000e-05\n",
      "Epoch 183/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4754 - mae: 2.4754 - mse: 11.5531 - val_loss: 7.3321 - val_mae: 7.3321 - val_mse: 61.8458 - lr: 1.0000e-05\n",
      "Epoch 184/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4737 - mae: 2.4737 - mse: 11.3199 - val_loss: 7.4898 - val_mae: 7.4898 - val_mse: 64.1810 - lr: 1.0000e-05\n",
      "Epoch 185/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4925 - mae: 2.4925 - mse: 11.0958 - val_loss: 7.5957 - val_mae: 7.5957 - val_mse: 65.7747 - lr: 1.0000e-05\n",
      "Epoch 186/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.5028 - mae: 2.5028 - mse: 11.0239 - val_loss: 7.5779 - val_mae: 7.5779 - val_mse: 65.5006 - lr: 1.0000e-05\n",
      "Epoch 187/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 2.4936 - mae: 2.4936 - mse: 11.0246 - val_loss: 7.4438 - val_mae: 7.4438 - val_mse: 63.4794 - lr: 1.0000e-05\n",
      "Epoch 188/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4770 - mae: 2.4770 - mse: 11.2283 - val_loss: 7.2891 - val_mae: 7.2891 - val_mse: 61.1904 - lr: 1.0000e-05\n",
      "Epoch 189/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4629 - mae: 2.4629 - mse: 11.4035 - val_loss: 7.1372 - val_mae: 7.1372 - val_mse: 58.9928 - lr: 1.0000e-05\n",
      "Epoch 190/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 2.4868 - mae: 2.4868 - mse: 11.7283 - val_loss: 7.0299 - val_mae: 7.0299 - val_mse: 57.4675 - lr: 1.0000e-05\n",
      "Epoch 191/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.5078 - mae: 2.5078 - mse: 11.9097 - val_loss: 7.0760 - val_mae: 7.0760 - val_mse: 58.1157 - lr: 1.0000e-05\n",
      "Epoch 192/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 2.4992 - mae: 2.4992 - mse: 11.7867 - val_loss: 7.2266 - val_mae: 7.2266 - val_mse: 60.2689 - lr: 1.0000e-05\n",
      "Epoch 193/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.4636 - mae: 2.4636 - mse: 11.5056 - val_loss: 7.3693 - val_mae: 7.3693 - val_mse: 62.3504 - lr: 1.0000e-05\n",
      "Epoch 194/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4752 - mae: 2.4752 - mse: 11.2744 - val_loss: 7.4507 - val_mae: 7.4507 - val_mse: 63.5558 - lr: 1.0000e-05\n",
      "Epoch 195/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4754 - mae: 2.4754 - mse: 11.1423 - val_loss: 7.4123 - val_mae: 7.4123 - val_mse: 62.9822 - lr: 1.0000e-05\n",
      "Epoch 196/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4711 - mae: 2.4711 - mse: 11.2271 - val_loss: 7.3165 - val_mae: 7.3165 - val_mse: 61.5659 - lr: 1.0000e-05\n",
      "Epoch 197/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.4590 - mae: 2.4590 - mse: 11.3361 - val_loss: 7.2082 - val_mae: 7.2082 - val_mse: 59.9879 - lr: 1.0000e-05\n",
      "Epoch 198/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4656 - mae: 2.4656 - mse: 11.5331 - val_loss: 7.1416 - val_mae: 7.1416 - val_mse: 59.0267 - lr: 1.0000e-05\n",
      "Epoch 199/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4742 - mae: 2.4742 - mse: 11.6178 - val_loss: 7.1466 - val_mae: 7.1466 - val_mse: 59.0933 - lr: 1.0000e-05\n",
      "Epoch 200/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4761 - mae: 2.4761 - mse: 11.6358 - val_loss: 7.1994 - val_mae: 7.1994 - val_mse: 59.8450 - lr: 1.0000e-05\n",
      "Epoch 201/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.4558 - mae: 2.4558 - mse: 11.4280 - val_loss: 7.4004 - val_mae: 7.4004 - val_mse: 62.7777 - lr: 1.0000e-05\n",
      "Epoch 202/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.4632 - mae: 2.4632 - mse: 11.1404 - val_loss: 7.5549 - val_mae: 7.5549 - val_mse: 65.0840 - lr: 1.0000e-05\n",
      "Epoch 203/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4789 - mae: 2.4789 - mse: 10.9453 - val_loss: 7.6271 - val_mae: 7.6271 - val_mse: 66.1764 - lr: 1.0000e-05\n",
      "Epoch 204/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4877 - mae: 2.4877 - mse: 10.8909 - val_loss: 7.6311 - val_mae: 7.6311 - val_mse: 66.2340 - lr: 1.0000e-05\n",
      "Epoch 205/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.4910 - mae: 2.4910 - mse: 10.9188 - val_loss: 7.6034 - val_mae: 7.6034 - val_mse: 65.8071 - lr: 1.0000e-05\n",
      "Epoch 206/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4798 - mae: 2.4798 - mse: 10.8985 - val_loss: 7.5567 - val_mae: 7.5567 - val_mse: 65.0962 - lr: 1.0000e-05\n",
      "Epoch 207/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4742 - mae: 2.4742 - mse: 10.9849 - val_loss: 7.4394 - val_mae: 7.4394 - val_mse: 63.3332 - lr: 1.0000e-05\n",
      "Epoch 208/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.4546 - mae: 2.4546 - mse: 11.1128 - val_loss: 7.3226 - val_mae: 7.3226 - val_mse: 61.6036 - lr: 1.0000e-05\n",
      "Epoch 209/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4549 - mae: 2.4549 - mse: 11.2815 - val_loss: 7.2430 - val_mae: 7.2430 - val_mse: 60.4389 - lr: 1.0000e-05\n",
      "Epoch 210/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4571 - mae: 2.4571 - mse: 11.3926 - val_loss: 7.3272 - val_mae: 7.3272 - val_mse: 61.6647 - lr: 1.0000e-05\n",
      "Epoch 211/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4500 - mae: 2.4500 - mse: 11.2340 - val_loss: 7.4676 - val_mae: 7.4676 - val_mse: 63.7389 - lr: 1.0000e-05\n",
      "Epoch 212/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4528 - mae: 2.4528 - mse: 11.0169 - val_loss: 7.5424 - val_mae: 7.5424 - val_mse: 64.8579 - lr: 1.0000e-05\n",
      "Epoch 213/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.4576 - mae: 2.4576 - mse: 10.8956 - val_loss: 7.6468 - val_mae: 7.6468 - val_mse: 66.4389 - lr: 1.0000e-05\n",
      "Epoch 214/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.4712 - mae: 2.4712 - mse: 10.7820 - val_loss: 7.7323 - val_mae: 7.7323 - val_mse: 67.7498 - lr: 1.0000e-05\n",
      "Epoch 215/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4833 - mae: 2.4833 - mse: 10.7066 - val_loss: 7.7335 - val_mae: 7.7335 - val_mse: 67.7654 - lr: 1.0000e-05\n",
      "Epoch 216/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4790 - mae: 2.4790 - mse: 10.7173 - val_loss: 7.6367 - val_mae: 7.6367 - val_mse: 66.2722 - lr: 1.0000e-05\n",
      "Epoch 217/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.4595 - mae: 2.4595 - mse: 10.7799 - val_loss: 7.4770 - val_mae: 7.4770 - val_mse: 63.8472 - lr: 1.0000e-05\n",
      "Epoch 218/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.4412 - mae: 2.4412 - mse: 11.0336 - val_loss: 7.2853 - val_mae: 7.2853 - val_mse: 61.0020 - lr: 1.0000e-05\n",
      "Epoch 219/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 2.4438 - mae: 2.4438 - mse: 11.2674 - val_loss: 7.1445 - val_mae: 7.1445 - val_mse: 58.9551 - lr: 1.0000e-05\n",
      "Epoch 220/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4796 - mae: 2.4796 - mse: 11.5687 - val_loss: 7.1201 - val_mae: 7.1201 - val_mse: 58.5962 - lr: 1.0000e-05\n",
      "Epoch 221/1000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 2.4750 - mae: 2.4750 - mse: 11.5803 - val_loss: 7.2770 - val_mae: 7.2770 - val_mse: 60.8497 - lr: 1.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 222/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4472 - mae: 2.4472 - mse: 11.2001 - val_loss: 7.4853 - val_mae: 7.4853 - val_mse: 63.9193 - lr: 1.0000e-05\n",
      "Epoch 223/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4429 - mae: 2.4429 - mse: 10.9168 - val_loss: 7.5739 - val_mae: 7.5739 - val_mse: 65.2469 - lr: 1.0000e-05\n",
      "Epoch 224/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4483 - mae: 2.4483 - mse: 10.8282 - val_loss: 7.5485 - val_mae: 7.5485 - val_mse: 64.8571 - lr: 1.0000e-05\n",
      "Epoch 225/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5845 - mae: 2.5845 - mse: 12.0200\n",
      "Epoch 00225: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4453 - mae: 2.4453 - mse: 10.8787 - val_loss: 7.4617 - val_mae: 7.4617 - val_mse: 63.5513 - lr: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss = \"mae\", optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), metrics=[\"mae\",\"mse\"])\n",
    "history = model.fit(final_x_zeros[:42],final_y_zeros[:42],\n",
    "                    epochs = 1000,\n",
    "                    validation_data=(final_x_zeros[42:], final_y_zeros[42:]), \n",
    "                    callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", \n",
    "                                                            patience=200, \n",
    "                                                            restore_best_weights=True),\n",
    "                          tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", \n",
    "                                                               patience=100, \n",
    "                                                               verbose=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e369207b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 27ms/step - loss: 4.3033 - mae: 4.3033 - mse: 40.9895\n",
      "INFO:tensorflow:Assets written to: conv1d_itc\\assets\n"
     ]
    }
   ],
   "source": [
    "model.evaluate(final_x_zeros[42:],final_y_zeros[42:])\n",
    "model.save(\"conv1d_itc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6ccda8",
   "metadata": {},
   "source": [
    "# BINGO!!! NOW LETS MAKE THE FINAL FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9c266f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_scores_comparator(add_time,add_posts,bert,stock,l):\n",
    "    '''\n",
    "    Args:\n",
    "    add_time: address of the file that has time series data of the stock\n",
    "    add_posts: address of the  file that has all the posts of the stock, labelled as spam or not spam\n",
    "    bert: address of the bert sentiment analysis model\n",
    "    stock: name of the stock\n",
    "    l: list of all models that the target model's results have to be compared to\n",
    "\n",
    "    Returns:\n",
    "    A pandas dataframe containing the MAEs and MSEs of all the models\n",
    "    '''\n",
    "    import pandas as pd\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import tensorflow_hub as hub\n",
    "    import tensorflow_text as text\n",
    "    df_stock = pd.read_csv(add_time,\n",
    "                     parse_dates = [\"Date\"],\n",
    "                     index_col = [\"Date\"])\n",
    "    df_posts = pd.read_excel(add_posts)\n",
    "    # removing the unnecessary columns\n",
    "    df_posts.drop([\"Unnamed: 0\"], axis=1,inplace=True)\n",
    "    # removing spam posts\n",
    "    df_posts = df_posts[df_itc_posts.Spam==0.0]\n",
    "    df_posts.drop([\"Spam\"],axis=1,inplace=True)\n",
    "    # sliding a window of 7 days and adding all the TIs\n",
    "    from stock_helper import prepare_data\n",
    "    x,y = prepare_data(df_stock)\n",
    "    # slicing the data\n",
    "    final_x = x[np.datetime64(\"2021-11-13\"):]\n",
    "    final_y = y[np.datetime64(\"2021-11-13\"):]\n",
    "    # reversing the posts data\n",
    "    df_posts = df_posts[::-1]\n",
    "    final_posts = df_posts[7:]\n",
    "    # loading the sentiment analysis model\n",
    "    sent_model = tf.keras.models.load_model(bert)\n",
    "    # removing duplicates from the data\n",
    "    final_posts.drop_duplicates(subset=['Messages'])\n",
    "    # calculating the sentiments score\n",
    "    sentiments = []\n",
    "    prev = np.datetime64(\"2015-11-12 21:31:26\")\n",
    "    for i in final_y.index:\n",
    "        total=0\n",
    "        cnt=0\n",
    "        for j in final_posts.itertuples():\n",
    "            _,msg,time = j\n",
    "            if np.datetime64(time)<np.datetime64(i) and np.datetime64(time)>prev:\n",
    "                total += tf.squeeze(sent_model.predict([msg])).numpy()\n",
    "                cnt+=1\n",
    "        prev = np.datetime64(i)\n",
    "        if(cnt==0):\n",
    "            sentiments.append(0)\n",
    "        else:\n",
    "            sentiments.append(total/cnt)\n",
    "    # getting indices where sentiments score is 0\n",
    "    zero_index = []\n",
    "    for i,j in enumerate(sentiments):\n",
    "        if(j==0):\n",
    "            zero_index.append(i)\n",
    "    # removing all the zero values indices\n",
    "    sentiments = np.delete(sentiments,zero_index)\n",
    "    final_x_zeros = final_x.copy()\n",
    "    final_y_zeros = final_y.copy()\n",
    "    final_y_zeros = final_y_zeros.to_frame()\n",
    "    final_x_zeros['removal_assist'] = np.arange(0,len(final_x),1)\n",
    "    final_y_zeros['removal_assist'] = np.arange(0,len(final_x),1)\n",
    "    final_y_zeros = final_y_zeros[final_y_zeros.removal_assist.isin(zero_index)==False]\n",
    "    final_x_zeros = final_x_zeros[final_x_zeros.removal_assist.isin(zero_index)==False]\n",
    "    # removing the added helper column\n",
    "    final_x_zeros.drop([\"removal_assist\"], axis=1,inplace=True)\n",
    "    final_y_zeros.drop([\"removal_assist\"], axis=1, inplace=True)\n",
    "    # loading the nbeats model for the given stock\n",
    "    model_nbeats = tf.keras.models.load_model(\"nbeats_\" + stock)\n",
    "    # making predictions and building the final dataframe \n",
    "    preds = tf.squeeze(model_nbeats.predict(final_x_zeros)).numpy()\n",
    "    final_df = pd.DataFrame({\"Predictions\": preds,\n",
    "                        \"Sentiments\":sentiments})\n",
    "    # building and training the target model\n",
    "    tf.random.set_seed(42)\n",
    "    inputs = tf.keras.Input(shape=(2))\n",
    "    x = tf.keras.layers.Dense(8, activation=\"selu\", kernel_initializer=\"lecun_normal\")(inputs)\n",
    "    x = tf.keras.layers.Dense(8, activation=\"selu\", kernel_initializer=\"lecun_normal\")(x)\n",
    "    x = tf.keras.layers.Dense(8, activation=\"selu\", kernel_initializer=\"lecun_normal\")(x)\n",
    "    outputs = tf.keras.layers.Dense(1)(x)\n",
    "    model = tf.keras.models.Model(inputs=inputs,outputs=outputs)\n",
    "    \n",
    "    xtrain,ytrain,xtest,ytest = final_df[:42],final_y_zeros[:42],final_df[42:],final_y_zeros[42:]\n",
    "    \n",
    "    model.compile(loss = \"mae\", optimizer = tf.keras.optimizers.Adam(), metrics=[\"mae\",\"mse\"])\n",
    "    history = model.fit(xtrain,ytrain,\n",
    "                        epochs = 1000,\n",
    "                        validation_data=(xtest,ytest), \n",
    "                        callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", \n",
    "                                                                patience=200, \n",
    "                                                                restore_best_weights=True),\n",
    "                              tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", \n",
    "                                                                   patience=100, \n",
    "                                                                   verbose=1)])\n",
    "    target_result = model.evaluate(xtest,ytest)\n",
    "    dict_res = {\n",
    "        \"Target_model\":target_result[1]\n",
    "    }\n",
    "    df = pd.DataFrame(dict_res)\n",
    "    frames = []\n",
    "    frames.append(df)\n",
    "    # getting the results of all the other models\n",
    "    for i in l:\n",
    "        name = i + \"_\" + stock\n",
    "        model = tf.keras.models.load_model(name)\n",
    "        result = model.evaluate(final_x_zeros[42:], final_y_zeros[42:])\n",
    "        dict_res = {\n",
    "         i:result[1]\n",
    "        }\n",
    "        df = pd.DataFrame(dict_res)\n",
    "        frames.append(df)\n",
    "    return pd.concat(frames, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a0a10009",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'Spam'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\HARSHV~1\\AppData\\Local\\Temp/ipykernel_16364/3722225777.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m df = model_scores_comparator(\"itc/ITC.NS.csv\", \"itc/itc_final_posts.xlsx\", \n\u001b[0m\u001b[0;32m      2\u001b[0m                              \"final_bert\", \"itc\", [\"conv1d\"])\n",
      "\u001b[1;32mC:\\Users\\HARSHV~1\\AppData\\Local\\Temp/ipykernel_16364/1805989469.py\u001b[0m in \u001b[0;36mmodel_scores_comparator\u001b[1;34m(add_time, add_posts, bert, stock, l)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mdf_posts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Unnamed: 0\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;31m# removing spam posts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mdf_posts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_posts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_itc_posts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSpam\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[0mdf_posts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Spam\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;31m# sliding a window of 7 days and adding all the TIs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\harshvardhan bhosale\\onedrive\\desktop\\webscrapping\\ws_env\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5485\u001b[0m         ):\n\u001b[0;32m   5486\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5487\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5489\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'Spam'"
     ]
    }
   ],
   "source": [
    "df = model_scores_comparator(\"itc/ITC.NS.csv\", \"itc/itc_final_posts.xlsx\", \n",
    "                             \"final_bert\", \"itc\", [\"conv1d\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7c707885",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "If using all scalar values, you must pass an index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\HARSHV~1\\AppData\\Local\\Temp/ipykernel_16364/2611571271.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;34m\"f\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m }\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\harshvardhan bhosale\\onedrive\\desktop\\webscrapping\\ws_env\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mfrom_dict\u001b[1;34m(cls, data, orient, dtype, columns)\u001b[0m\n\u001b[0;32m   1591\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"only recognize index or columns for orient\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1593\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1594\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1595\u001b[0m     def to_numpy(\n",
      "\u001b[1;32mc:\\users\\harshvardhan bhosale\\onedrive\\desktop\\webscrapping\\ws_env\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    612\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m             \u001b[1;31m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 614\u001b[1;33m             \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    615\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\harshvardhan bhosale\\onedrive\\desktop\\webscrapping\\ws_env\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    462\u001b[0m         \u001b[1;31m# TODO: can we get rid of the dt64tz special case above?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 464\u001b[1;33m     return arrays_to_mgr(\n\u001b[0m\u001b[0;32m    465\u001b[0m         \u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtyp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconsolidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    466\u001b[0m     )\n",
      "\u001b[1;32mc:\\users\\harshvardhan bhosale\\onedrive\\desktop\\webscrapping\\ws_env\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, arr_names, index, columns, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;31m# figure out the index, if necessary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_extract_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\harshvardhan bhosale\\onedrive\\desktop\\webscrapping\\ws_env\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    623\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mindexes\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mraw_lengths\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"If using all scalar values, you must pass an index\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhave_series\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: If using all scalar values, you must pass an index"
     ]
    }
   ],
   "source": [
    "z = res[0]\n",
    "dict1 = {\n",
    "    \"f\":z\n",
    "}\n",
    "g = pd.DataFrame.from_dict(dict1)\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11733d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ws_env",
   "language": "python",
   "name": "ws_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
