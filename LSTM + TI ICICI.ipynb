{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9208946",
   "metadata": {},
   "source": [
    "# PREPARING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62749679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "df_stock = pd.read_csv(\"icici/ICICIBANK.NS.csv\",\n",
    "                 parse_dates = [\"Date\"],\n",
    "                 index_col = [\"Date\"])\n",
    "df_posts = pd.read_excel(\"icici/icici_bank_final_posts.xlsx\")\n",
    "# removing the unnecessary columns\n",
    "df_posts.drop([\"Unnamed: 0\"], axis=1,inplace=True)\n",
    "# removing spam posts\n",
    "df_posts = df_posts[df_posts.Spam==0.0]\n",
    "df_posts.drop([\"Spam\"],axis=1,inplace=True)\n",
    "# sliding a window of 7 days and adding all the TIs\n",
    "from stock_helper import prepare_data\n",
    "x,y = prepare_data(df_stock)\n",
    "# slicing the data\n",
    "final_x = x[np.datetime64(\"2021-11-13\"):]\n",
    "final_y = y[np.datetime64(\"2021-11-13\"):]\n",
    "# reversing the posts data\n",
    "df_posts = df_posts[::-1]\n",
    "final_posts = df_posts[7:]\n",
    "# loading the sentiment analysis model\n",
    "sent_model = tf.keras.models.load_model(\"final_bert\")\n",
    "# removing duplicates from the data\n",
    "final_posts.drop_duplicates(subset=['Messages'])\n",
    "# calculating the sentiments score\n",
    "sentiments = []\n",
    "prev = np.datetime64(\"2015-11-12 21:31:26\")\n",
    "for i in final_y.index:\n",
    "    total=0\n",
    "    cnt=0\n",
    "    for j in final_posts.itertuples():\n",
    "        _,msg,time = j\n",
    "        if np.datetime64(time)<np.datetime64(i) and np.datetime64(time)>prev:\n",
    "            total += tf.squeeze(sent_model.predict([msg])).numpy()\n",
    "            cnt+=1\n",
    "    prev = np.datetime64(i)\n",
    "    if(cnt==0):\n",
    "        sentiments.append(0)\n",
    "    else:\n",
    "        sentiments.append(total/cnt)\n",
    "# getting indices where sentiments score is 0\n",
    "zero_index = []\n",
    "for i,j in enumerate(sentiments):\n",
    "    if(j==0):\n",
    "        zero_index.append(i)\n",
    "# removing all the zero values indices\n",
    "sentiments = np.delete(sentiments,zero_index)\n",
    "final_x_zeros = final_x.copy()\n",
    "final_y_zeros = final_y.copy()\n",
    "final_y_zeros = final_y_zeros.to_frame()\n",
    "final_x_zeros['removal_assist'] = np.arange(0,len(final_x),1)\n",
    "final_y_zeros['removal_assist'] = np.arange(0,len(final_x),1)\n",
    "final_y_zeros = final_y_zeros[final_y_zeros.removal_assist.isin(zero_index)==False]\n",
    "final_x_zeros = final_x_zeros[final_x_zeros.removal_assist.isin(zero_index)==False]\n",
    "# removing the added helper column\n",
    "final_x_zeros.drop([\"removal_assist\"], axis=1,inplace=True)\n",
    "final_y_zeros.drop([\"removal_assist\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32641bde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.46964598, 0.7110796 , 0.830842  , ..., 0.47434157, 0.5546069 ,\n",
       "        0.7407407 ],\n",
       "       [0.49432564, 0.72036934, 0.819458  , ..., 0.37842518, 0.53776145,\n",
       "        0.7037037 ],\n",
       "       [0.5260892 , 0.73180676, 0.8039284 , ..., 0.33568484, 0.5818709 ,\n",
       "        0.5925926 ],\n",
       "       ...,\n",
       "       [0.9687238 , 0.98316383, 0.7430401 , ..., 0.23337865, 0.32989419,\n",
       "        0.25925925],\n",
       "       [0.965055  , 0.9778986 , 0.73816586, ..., 0.21261415, 0.42023087,\n",
       "        0.25925925],\n",
       "       [0.96308565, 0.94291973, 0.68118954, ..., 0.34671447, 0.65432334,\n",
       "        0.25925925]], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mms = MinMaxScaler()\n",
    "final_x_zeros_scaled = mms.fit_transform(final_x_zeros)\n",
    "final_x_zeros_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9d501f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-11-16</th>\n",
       "      <td>764.049988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-17</th>\n",
       "      <td>761.299988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-18</th>\n",
       "      <td>762.900024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-22</th>\n",
       "      <td>752.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-23</th>\n",
       "      <td>751.849976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-24</th>\n",
       "      <td>760.200012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-25</th>\n",
       "      <td>751.049988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-26</th>\n",
       "      <td>722.200012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-29</th>\n",
       "      <td>718.400024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30</th>\n",
       "      <td>714.349976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-01</th>\n",
       "      <td>727.700012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-02</th>\n",
       "      <td>722.400024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-03</th>\n",
       "      <td>716.299988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-06</th>\n",
       "      <td>709.549988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-07</th>\n",
       "      <td>734.650024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-08</th>\n",
       "      <td>753.400024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-09</th>\n",
       "      <td>755.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-10</th>\n",
       "      <td>758.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-13</th>\n",
       "      <td>754.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-14</th>\n",
       "      <td>758.650024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-15</th>\n",
       "      <td>752.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-16</th>\n",
       "      <td>741.150024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-20</th>\n",
       "      <td>709.950012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-21</th>\n",
       "      <td>720.349976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-22</th>\n",
       "      <td>732.799988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-23</th>\n",
       "      <td>731.299988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-24</th>\n",
       "      <td>727.099976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-27</th>\n",
       "      <td>736.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-28</th>\n",
       "      <td>735.799988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-29</th>\n",
       "      <td>735.700012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-30</th>\n",
       "      <td>735.700012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-03</th>\n",
       "      <td>764.700012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-04</th>\n",
       "      <td>772.849976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-05</th>\n",
       "      <td>788.049988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-06</th>\n",
       "      <td>785.049988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-07</th>\n",
       "      <td>793.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-10</th>\n",
       "      <td>810.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-11</th>\n",
       "      <td>810.650024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-12</th>\n",
       "      <td>823.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-13</th>\n",
       "      <td>824.700012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-20</th>\n",
       "      <td>810.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-21</th>\n",
       "      <td>804.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-24</th>\n",
       "      <td>798.450012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-25</th>\n",
       "      <td>801.650024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-27</th>\n",
       "      <td>794.650024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-28</th>\n",
       "      <td>781.150024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-31</th>\n",
       "      <td>788.799988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-01</th>\n",
       "      <td>810.299988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-02</th>\n",
       "      <td>813.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-03</th>\n",
       "      <td>808.950012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-04</th>\n",
       "      <td>805.049988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-08</th>\n",
       "      <td>792.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-09</th>\n",
       "      <td>802.799988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-11</th>\n",
       "      <td>790.799988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Close\n",
       "Date                  \n",
       "2021-11-16  764.049988\n",
       "2021-11-17  761.299988\n",
       "2021-11-18  762.900024\n",
       "2021-11-22  752.000000\n",
       "2021-11-23  751.849976\n",
       "2021-11-24  760.200012\n",
       "2021-11-25  751.049988\n",
       "2021-11-26  722.200012\n",
       "2021-11-29  718.400024\n",
       "2021-11-30  714.349976\n",
       "2021-12-01  727.700012\n",
       "2021-12-02  722.400024\n",
       "2021-12-03  716.299988\n",
       "2021-12-06  709.549988\n",
       "2021-12-07  734.650024\n",
       "2021-12-08  753.400024\n",
       "2021-12-09  755.000000\n",
       "2021-12-10  758.000000\n",
       "2021-12-13  754.000000\n",
       "2021-12-14  758.650024\n",
       "2021-12-15  752.250000\n",
       "2021-12-16  741.150024\n",
       "2021-12-20  709.950012\n",
       "2021-12-21  720.349976\n",
       "2021-12-22  732.799988\n",
       "2021-12-23  731.299988\n",
       "2021-12-24  727.099976\n",
       "2021-12-27  736.000000\n",
       "2021-12-28  735.799988\n",
       "2021-12-29  735.700012\n",
       "2021-12-30  735.700012\n",
       "2022-01-03  764.700012\n",
       "2022-01-04  772.849976\n",
       "2022-01-05  788.049988\n",
       "2022-01-06  785.049988\n",
       "2022-01-07  793.250000\n",
       "2022-01-10  810.750000\n",
       "2022-01-11  810.650024\n",
       "2022-01-12  823.750000\n",
       "2022-01-13  824.700012\n",
       "2022-01-20  810.250000\n",
       "2022-01-21  804.500000\n",
       "2022-01-24  798.450012\n",
       "2022-01-25  801.650024\n",
       "2022-01-27  794.650024\n",
       "2022-01-28  781.150024\n",
       "2022-01-31  788.799988\n",
       "2022-02-01  810.299988\n",
       "2022-02-02  813.750000\n",
       "2022-02-03  808.950012\n",
       "2022-02-04  805.049988\n",
       "2022-02-08  792.500000\n",
       "2022-02-09  802.799988\n",
       "2022-02-11  790.799988"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_y_zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7400f718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-11-16</th>\n",
       "      <td>0.926458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-17</th>\n",
       "      <td>0.923124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-18</th>\n",
       "      <td>0.925064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-22</th>\n",
       "      <td>0.911847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-23</th>\n",
       "      <td>0.911665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-24</th>\n",
       "      <td>0.921790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-25</th>\n",
       "      <td>0.910695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-26</th>\n",
       "      <td>0.875712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-29</th>\n",
       "      <td>0.871105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30</th>\n",
       "      <td>0.866194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-01</th>\n",
       "      <td>0.882381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-02</th>\n",
       "      <td>0.875955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-03</th>\n",
       "      <td>0.868558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-06</th>\n",
       "      <td>0.860373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-07</th>\n",
       "      <td>0.890809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-08</th>\n",
       "      <td>0.913544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-09</th>\n",
       "      <td>0.915484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-10</th>\n",
       "      <td>0.919122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-13</th>\n",
       "      <td>0.914272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-14</th>\n",
       "      <td>0.919910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-15</th>\n",
       "      <td>0.912150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-16</th>\n",
       "      <td>0.898690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-20</th>\n",
       "      <td>0.860859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-21</th>\n",
       "      <td>0.873469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-22</th>\n",
       "      <td>0.888566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-23</th>\n",
       "      <td>0.886747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-24</th>\n",
       "      <td>0.881654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-27</th>\n",
       "      <td>0.892446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-28</th>\n",
       "      <td>0.892203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-29</th>\n",
       "      <td>0.892082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-30</th>\n",
       "      <td>0.892082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-03</th>\n",
       "      <td>0.927246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-04</th>\n",
       "      <td>0.937129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-05</th>\n",
       "      <td>0.955560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-06</th>\n",
       "      <td>0.951922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-07</th>\n",
       "      <td>0.961865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-10</th>\n",
       "      <td>0.983085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-11</th>\n",
       "      <td>0.982964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-12</th>\n",
       "      <td>0.998848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-13</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-20</th>\n",
       "      <td>0.982478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-21</th>\n",
       "      <td>0.975506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-24</th>\n",
       "      <td>0.968170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-25</th>\n",
       "      <td>0.972050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-27</th>\n",
       "      <td>0.963563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-28</th>\n",
       "      <td>0.947193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-31</th>\n",
       "      <td>0.956469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-01</th>\n",
       "      <td>0.982539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-02</th>\n",
       "      <td>0.986722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-03</th>\n",
       "      <td>0.980902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-04</th>\n",
       "      <td>0.976173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-08</th>\n",
       "      <td>0.960956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-09</th>\n",
       "      <td>0.973445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-11</th>\n",
       "      <td>0.958894</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Close\n",
       "Date                \n",
       "2021-11-16  0.926458\n",
       "2021-11-17  0.923124\n",
       "2021-11-18  0.925064\n",
       "2021-11-22  0.911847\n",
       "2021-11-23  0.911665\n",
       "2021-11-24  0.921790\n",
       "2021-11-25  0.910695\n",
       "2021-11-26  0.875712\n",
       "2021-11-29  0.871105\n",
       "2021-11-30  0.866194\n",
       "2021-12-01  0.882381\n",
       "2021-12-02  0.875955\n",
       "2021-12-03  0.868558\n",
       "2021-12-06  0.860373\n",
       "2021-12-07  0.890809\n",
       "2021-12-08  0.913544\n",
       "2021-12-09  0.915484\n",
       "2021-12-10  0.919122\n",
       "2021-12-13  0.914272\n",
       "2021-12-14  0.919910\n",
       "2021-12-15  0.912150\n",
       "2021-12-16  0.898690\n",
       "2021-12-20  0.860859\n",
       "2021-12-21  0.873469\n",
       "2021-12-22  0.888566\n",
       "2021-12-23  0.886747\n",
       "2021-12-24  0.881654\n",
       "2021-12-27  0.892446\n",
       "2021-12-28  0.892203\n",
       "2021-12-29  0.892082\n",
       "2021-12-30  0.892082\n",
       "2022-01-03  0.927246\n",
       "2022-01-04  0.937129\n",
       "2022-01-05  0.955560\n",
       "2022-01-06  0.951922\n",
       "2022-01-07  0.961865\n",
       "2022-01-10  0.983085\n",
       "2022-01-11  0.982964\n",
       "2022-01-12  0.998848\n",
       "2022-01-13  1.000000\n",
       "2022-01-20  0.982478\n",
       "2022-01-21  0.975506\n",
       "2022-01-24  0.968170\n",
       "2022-01-25  0.972050\n",
       "2022-01-27  0.963563\n",
       "2022-01-28  0.947193\n",
       "2022-01-31  0.956469\n",
       "2022-02-01  0.982539\n",
       "2022-02-02  0.986722\n",
       "2022-02-03  0.980902\n",
       "2022-02-04  0.976173\n",
       "2022-02-08  0.960956\n",
       "2022-02-09  0.973445\n",
       "2022-02-11  0.958894"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = final_y_zeros.max()\n",
    "final_y_zeros= final_y_zeros/k\n",
    "final_y_zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71244b32",
   "metadata": {},
   "source": [
    "# GETTING THE NBEATS FOR COMPARISON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "092105f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 654ms/step - loss: 10.6543 - mae: 10.6543 - mse: 175.6807\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[10.654330253601074, 10.654330253601074, 175.6807098388672]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_nbeats = tf.keras.models.load_model(\"nbeats_icici\")\n",
    "model_nbeats.evaluate(final_x_zeros[43:],final_y_zeros[43:]*k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5b8bd8",
   "metadata": {},
   "source": [
    "# BUILDING AND TRAINING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e86461a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 71)]              0         \n",
      "                                                                 \n",
      " lambda_1 (Lambda)           (None, 1, 71)             0         \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 1, 128)            102400    \n",
      "                                                                 \n",
      " lstm_6 (LSTM)               (None, 1, 128)            131584    \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 1, 128)            131584    \n",
      "                                                                 \n",
      " lstm_8 (LSTM)               (None, 1, 128)            131584    \n",
      "                                                                 \n",
      " lstm_9 (LSTM)               (None, 128)               131584    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 628,865\n",
      "Trainable params: 628,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "inputs = tf.keras.layers.Input(shape=(71))\n",
    "x = tf.keras.layers.Lambda(lambda x : tf.expand_dims(x,axis=1))(inputs)\n",
    "x = tf.keras.layers.LSTM(128,return_sequences=True,dropout=0.1)(x)\n",
    "x = tf.keras.layers.LSTM(128,return_sequences=True, dropout=0.1)(x)\n",
    "x = tf.keras.layers.LSTM(128,return_sequences=True,dropout=0.1)(x)\n",
    "x = tf.keras.layers.LSTM(128,return_sequences=True, dropout=0.1)(x)\n",
    "x = tf.keras.layers.LSTM(128)(x)\n",
    "output = tf.keras.layers.Dense(1)(x)\n",
    "model = tf.keras.models.Model(inputs = inputs, outputs = output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f4b4906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "2/2 [==============================] - 6s 1s/step - loss: 0.9149 - mae: 0.9149 - mse: 0.8386 - val_loss: 0.9581 - val_mae: 0.9581 - val_mse: 0.9181 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.9040 - mae: 0.9040 - mse: 0.8188 - val_loss: 0.9455 - val_mae: 0.9455 - val_mse: 0.8941 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.8914 - mae: 0.8914 - mse: 0.7962 - val_loss: 0.9297 - val_mae: 0.9297 - val_mse: 0.8644 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.8760 - mae: 0.8760 - mse: 0.7689 - val_loss: 0.9081 - val_mae: 0.9081 - val_mse: 0.8247 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.8556 - mae: 0.8556 - mse: 0.7334 - val_loss: 0.8761 - val_mae: 0.8761 - val_mse: 0.7678 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.8270 - mae: 0.8270 - mse: 0.6851 - val_loss: 0.8252 - val_mae: 0.8252 - val_mse: 0.6812 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.7851 - mae: 0.7851 - mse: 0.6174 - val_loss: 0.7392 - val_mae: 0.7392 - val_mse: 0.5467 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.7155 - mae: 0.7155 - mse: 0.5132 - val_loss: 0.5909 - val_mae: 0.5909 - val_mse: 0.3498 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.6015 - mae: 0.6015 - mse: 0.3641 - val_loss: 0.3290 - val_mae: 0.3290 - val_mse: 0.1100 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4114 - mae: 0.4114 - mse: 0.1809 - val_loss: 0.1332 - val_mae: 0.1332 - val_mse: 0.0227 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.1803 - mae: 0.1803 - mse: 0.0515 - val_loss: 0.7726 - val_mae: 0.7726 - val_mse: 0.6062 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.3422 - mae: 0.3422 - mse: 0.1956 - val_loss: 0.7232 - val_mae: 0.7232 - val_mse: 0.5302 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.3008 - mae: 0.3008 - mse: 0.1521 - val_loss: 0.3755 - val_mae: 0.3755 - val_mse: 0.1454 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.1600 - mae: 0.1600 - mse: 0.0407 - val_loss: 0.0833 - val_mae: 0.0833 - val_mse: 0.0094 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.1641 - mae: 0.1641 - mse: 0.0341 - val_loss: 0.0513 - val_mae: 0.0513 - val_mse: 0.0034 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.1982 - mae: 0.1982 - mse: 0.0498 - val_loss: 0.0451 - val_mae: 0.0451 - val_mse: 0.0026 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.1618 - mae: 0.1618 - mse: 0.0350 - val_loss: 0.0740 - val_mae: 0.0740 - val_mse: 0.0069 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.1149 - mae: 0.1149 - mse: 0.0183 - val_loss: 0.2303 - val_mae: 0.2303 - val_mse: 0.0546 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.1072 - mae: 0.1072 - mse: 0.0159 - val_loss: 0.3271 - val_mae: 0.3271 - val_mse: 0.1085 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.1402 - mae: 0.1402 - mse: 0.0335 - val_loss: 0.3031 - val_mae: 0.3031 - val_mse: 0.0930 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.1277 - mae: 0.1277 - mse: 0.0277 - val_loss: 0.1681 - val_mae: 0.1681 - val_mse: 0.0291 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.0782 - mae: 0.0782 - mse: 0.0085 - val_loss: 0.0406 - val_mae: 0.0406 - val_mse: 0.0023 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.0785 - mae: 0.0785 - mse: 0.0092 - val_loss: 0.0217 - val_mae: 0.0217 - val_mse: 5.1913e-04 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0843 - mae: 0.0843 - mse: 0.0102 - val_loss: 0.0295 - val_mae: 0.0295 - val_mse: 0.0014 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0682 - mae: 0.0682 - mse: 0.0069 - val_loss: 0.0956 - val_mae: 0.0956 - val_mse: 0.0096 - lr: 0.0010\n",
      "Epoch 26/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0539 - mae: 0.0539 - mse: 0.0040 - val_loss: 0.1296 - val_mae: 0.1296 - val_mse: 0.0172 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0677 - mae: 0.0677 - mse: 0.0083 - val_loss: 0.1042 - val_mae: 0.1042 - val_mse: 0.0112 - lr: 0.0010\n",
      "Epoch 28/1000\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0594 - mae: 0.0594 - mse: 0.0056 - val_loss: 0.0363 - val_mae: 0.0363 - val_mse: 0.0016 - lr: 0.0010\n",
      "Epoch 29/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.0573 - mae: 0.0573 - mse: 0.0049 - val_loss: 0.0184 - val_mae: 0.0184 - val_mse: 4.2345e-04 - lr: 0.0010\n",
      "Epoch 30/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.0639 - mae: 0.0639 - mse: 0.0060 - val_loss: 0.0154 - val_mae: 0.0154 - val_mse: 4.5069e-04 - lr: 0.0010\n",
      "Epoch 31/1000\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0472 - mae: 0.0472 - mse: 0.0035 - val_loss: 0.0772 - val_mae: 0.0772 - val_mse: 0.0062 - lr: 0.0010\n",
      "Epoch 32/1000\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0477 - mae: 0.0477 - mse: 0.0031 - val_loss: 0.0816 - val_mae: 0.0816 - val_mse: 0.0069 - lr: 0.0010\n",
      "Epoch 33/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0483 - mae: 0.0483 - mse: 0.0036 - val_loss: 0.0259 - val_mae: 0.0259 - val_mse: 9.0381e-04 - lr: 0.0010\n",
      "Epoch 34/1000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0369 - mae: 0.0369 - mse: 0.0022 - val_loss: 0.0144 - val_mae: 0.0144 - val_mse: 2.5932e-04 - lr: 0.0010\n",
      "Epoch 35/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.0452 - mae: 0.0452 - mse: 0.0030 - val_loss: 0.0129 - val_mae: 0.0129 - val_mse: 2.2491e-04 - lr: 0.0010\n",
      "Epoch 36/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0349 - mae: 0.0349 - mse: 0.0019 - val_loss: 0.0396 - val_mae: 0.0396 - val_mse: 0.0018 - lr: 0.0010\n",
      "Epoch 37/1000\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0361 - mae: 0.0361 - mse: 0.0021 - val_loss: 0.0463 - val_mae: 0.0463 - val_mse: 0.0024 - lr: 0.0010\n",
      "Epoch 38/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.0365 - mae: 0.0365 - mse: 0.0019 - val_loss: 0.0126 - val_mae: 0.0126 - val_mse: 2.1103e-04 - lr: 0.0010\n",
      "Epoch 39/1000\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0286 - mae: 0.0286 - mse: 0.0014 - val_loss: 0.0171 - val_mae: 0.0171 - val_mse: 4.2255e-04 - lr: 0.0010\n",
      "Epoch 40/1000\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0359 - mae: 0.0359 - mse: 0.0017 - val_loss: 0.0132 - val_mae: 0.0132 - val_mse: 2.5030e-04 - lr: 0.0010\n",
      "Epoch 41/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0288 - mae: 0.0288 - mse: 0.0013 - val_loss: 0.0128 - val_mae: 0.0128 - val_mse: 2.0858e-04 - lr: 0.0010\n",
      "Epoch 42/1000\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0273 - mae: 0.0273 - mse: 0.0012 - val_loss: 0.0145 - val_mae: 0.0145 - val_mse: 3.4736e-04 - lr: 0.0010\n",
      "Epoch 43/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0254 - mae: 0.0254 - mse: 9.1491e-04 - val_loss: 0.0126 - val_mae: 0.0126 - val_mse: 2.1188e-04 - lr: 0.0010\n",
      "Epoch 44/1000\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0242 - mae: 0.0242 - mse: 8.7388e-04 - val_loss: 0.0127 - val_mae: 0.0127 - val_mse: 2.4462e-04 - lr: 0.0010\n",
      "Epoch 45/1000\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0239 - mae: 0.0239 - mse: 9.1533e-04 - val_loss: 0.0119 - val_mae: 0.0119 - val_mse: 1.7783e-04 - lr: 0.0010\n",
      "Epoch 46/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0294 - mae: 0.0294 - mse: 0.0013 - val_loss: 0.0125 - val_mae: 0.0125 - val_mse: 2.2879e-04 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0258 - mae: 0.0258 - mse: 0.0010 - val_loss: 0.0163 - val_mae: 0.0163 - val_mse: 3.9053e-04 - lr: 0.0010\n",
      "Epoch 48/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0245 - mae: 0.0245 - mse: 8.9507e-04 - val_loss: 0.0188 - val_mae: 0.0188 - val_mse: 4.9027e-04 - lr: 0.0010\n",
      "Epoch 49/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0200 - mae: 0.0200 - mse: 6.4352e-04 - val_loss: 0.0207 - val_mae: 0.0207 - val_mse: 5.8149e-04 - lr: 0.0010\n",
      "Epoch 50/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0208 - mae: 0.0208 - mse: 7.6719e-04 - val_loss: 0.0245 - val_mae: 0.0245 - val_mse: 7.6816e-04 - lr: 0.0010\n",
      "Epoch 51/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0276 - mae: 0.0276 - mse: 0.0012 - val_loss: 0.0123 - val_mae: 0.0123 - val_mse: 2.2742e-04 - lr: 0.0010\n",
      "Epoch 52/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0238 - mae: 0.0238 - mse: 8.7783e-04 - val_loss: 0.0120 - val_mae: 0.0120 - val_mse: 1.9638e-04 - lr: 0.0010\n",
      "Epoch 53/1000\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0206 - mae: 0.0206 - mse: 6.7056e-04 - val_loss: 0.0236 - val_mae: 0.0236 - val_mse: 7.2555e-04 - lr: 0.0010\n",
      "Epoch 54/1000\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0229 - mae: 0.0229 - mse: 7.8120e-04 - val_loss: 0.0205 - val_mae: 0.0205 - val_mse: 5.6861e-04 - lr: 0.0010\n",
      "Epoch 55/1000\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0248 - mae: 0.0248 - mse: 9.8560e-04 - val_loss: 0.0125 - val_mae: 0.0125 - val_mse: 2.4412e-04 - lr: 0.0010\n",
      "Epoch 56/1000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0248 - mae: 0.0248 - mse: 0.0010 - val_loss: 0.0223 - val_mae: 0.0223 - val_mse: 6.6357e-04 - lr: 0.0010\n",
      "Epoch 57/1000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0229 - mae: 0.0229 - mse: 8.1720e-04 - val_loss: 0.0415 - val_mae: 0.0415 - val_mse: 0.0019 - lr: 0.0010\n",
      "Epoch 58/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0255 - mae: 0.0255 - mse: 0.0010 - val_loss: 0.0201 - val_mae: 0.0201 - val_mse: 5.4899e-04 - lr: 0.0010\n",
      "Epoch 59/1000\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0263 - mae: 0.0263 - mse: 0.0010 - val_loss: 0.0118 - val_mae: 0.0118 - val_mse: 1.8073e-04 - lr: 0.0010\n",
      "Epoch 60/1000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0273 - mae: 0.0273 - mse: 0.0012 - val_loss: 0.0119 - val_mae: 0.0119 - val_mse: 2.0845e-04 - lr: 0.0010\n",
      "Epoch 61/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0252 - mae: 0.0252 - mse: 9.4404e-04 - val_loss: 0.0257 - val_mae: 0.0257 - val_mse: 8.2097e-04 - lr: 0.0010\n",
      "Epoch 62/1000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0193 - mae: 0.0193 - mse: 6.4030e-04 - val_loss: 0.0292 - val_mae: 0.0292 - val_mse: 0.0010 - lr: 0.0010\n",
      "Epoch 63/1000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0244 - mae: 0.0244 - mse: 8.4435e-04 - val_loss: 0.0119 - val_mae: 0.0119 - val_mse: 1.7778e-04 - lr: 0.0010\n",
      "Epoch 64/1000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0277 - mae: 0.0277 - mse: 0.0012 - val_loss: 0.0192 - val_mae: 0.0192 - val_mse: 5.1158e-04 - lr: 0.0010\n",
      "Epoch 65/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0249 - mae: 0.0249 - mse: 9.1928e-04 - val_loss: 0.0344 - val_mae: 0.0344 - val_mse: 0.0013 - lr: 0.0010\n",
      "Epoch 66/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0201 - mae: 0.0201 - mse: 6.8788e-04 - val_loss: 0.0274 - val_mae: 0.0274 - val_mse: 9.1923e-04 - lr: 0.0010\n",
      "Epoch 67/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0175 - mae: 0.0175 - mse: 5.1310e-04 - val_loss: 0.0143 - val_mae: 0.0143 - val_mse: 3.1350e-04 - lr: 0.0010\n",
      "Epoch 68/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0244 - mae: 0.0244 - mse: 8.4978e-04 - val_loss: 0.0130 - val_mae: 0.0130 - val_mse: 2.6804e-04 - lr: 0.0010\n",
      "Epoch 69/1000\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0197 - mae: 0.0197 - mse: 6.2136e-04 - val_loss: 0.0347 - val_mae: 0.0347 - val_mse: 0.0014 - lr: 0.0010\n",
      "Epoch 70/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0269 - mae: 0.0269 - mse: 9.9457e-04 - val_loss: 0.0289 - val_mae: 0.0289 - val_mse: 0.0010 - lr: 0.0010\n",
      "Epoch 71/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0246 - mae: 0.0246 - mse: 8.8188e-04 - val_loss: 0.0126 - val_mae: 0.0126 - val_mse: 2.4445e-04 - lr: 0.0010\n",
      "Epoch 72/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0300 - mae: 0.0300 - mse: 0.0012 - val_loss: 0.0132 - val_mae: 0.0132 - val_mse: 2.7403e-04 - lr: 0.0010\n",
      "Epoch 73/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0256 - mae: 0.0256 - mse: 9.7087e-04 - val_loss: 0.0331 - val_mae: 0.0331 - val_mse: 0.0013 - lr: 0.0010\n",
      "Epoch 74/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0209 - mae: 0.0209 - mse: 7.6051e-04 - val_loss: 0.0130 - val_mae: 0.0130 - val_mse: 2.6678e-04 - lr: 0.0010\n",
      "Epoch 75/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0246 - mae: 0.0246 - mse: 9.6033e-04 - val_loss: 0.0163 - val_mae: 0.0163 - val_mse: 3.9297e-04 - lr: 0.0010\n",
      "Epoch 76/1000\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0187 - mae: 0.0187 - mse: 5.7299e-04 - val_loss: 0.0364 - val_mae: 0.0364 - val_mse: 0.0015 - lr: 0.0010\n",
      "Epoch 77/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0256 - mae: 0.0256 - mse: 9.9375e-04 - val_loss: 0.0239 - val_mae: 0.0239 - val_mse: 7.4524e-04 - lr: 0.0010\n",
      "Epoch 78/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0239 - mae: 0.0239 - mse: 9.1023e-04 - val_loss: 0.0123 - val_mae: 0.0123 - val_mse: 1.8550e-04 - lr: 0.0010\n",
      "Epoch 79/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0281 - mae: 0.0281 - mse: 0.0013 - val_loss: 0.0263 - val_mae: 0.0263 - val_mse: 8.6889e-04 - lr: 0.0010\n",
      "Epoch 80/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0211 - mae: 0.0211 - mse: 7.3456e-04 - val_loss: 0.0426 - val_mae: 0.0426 - val_mse: 0.0020 - lr: 0.0010\n",
      "Epoch 81/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0281 - mae: 0.0281 - mse: 0.0011 - val_loss: 0.0212 - val_mae: 0.0212 - val_mse: 6.0887e-04 - lr: 0.0010\n",
      "Epoch 82/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0196 - mae: 0.0196 - mse: 6.2275e-04 - val_loss: 0.0159 - val_mae: 0.0159 - val_mse: 3.7310e-04 - lr: 0.0010\n",
      "Epoch 83/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0209 - mae: 0.0209 - mse: 6.9012e-04 - val_loss: 0.0302 - val_mae: 0.0302 - val_mse: 0.0011 - lr: 0.0010\n",
      "Epoch 84/1000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0210 - mae: 0.0210 - mse: 6.8485e-04 - val_loss: 0.0255 - val_mae: 0.0255 - val_mse: 8.3091e-04 - lr: 0.0010\n",
      "Epoch 85/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0157 - mae: 0.0157 - mse: 4.2694e-04 - val_loss: 0.0215 - val_mae: 0.0215 - val_mse: 6.2204e-04 - lr: 0.0010\n",
      "Epoch 86/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0225 - mae: 0.0225 - mse: 8.0970e-04 - val_loss: 0.0366 - val_mae: 0.0366 - val_mse: 0.0015 - lr: 0.0010\n",
      "Epoch 87/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0174 - mae: 0.0174 - mse: 4.8304e-04 - val_loss: 0.0379 - val_mae: 0.0379 - val_mse: 0.0016 - lr: 0.0010\n",
      "Epoch 88/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0218 - mae: 0.0218 - mse: 7.2161e-04 - val_loss: 0.0126 - val_mae: 0.0126 - val_mse: 1.8855e-04 - lr: 0.0010\n",
      "Epoch 89/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0312 - mae: 0.0312 - mse: 0.0014 - val_loss: 0.0132 - val_mae: 0.0132 - val_mse: 2.4821e-04 - lr: 0.0010\n",
      "Epoch 90/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0224 - mae: 0.0224 - mse: 7.5945e-04 - val_loss: 0.0494 - val_mae: 0.0494 - val_mse: 0.0026 - lr: 0.0010\n",
      "Epoch 91/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0283 - mae: 0.0283 - mse: 0.0013 - val_loss: 0.0510 - val_mae: 0.0510 - val_mse: 0.0028 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0261 - mae: 0.0261 - mse: 0.0011 - val_loss: 0.0126 - val_mae: 0.0126 - val_mse: 1.9292e-04 - lr: 0.0010\n",
      "Epoch 93/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0299 - mae: 0.0299 - mse: 0.0012 - val_loss: 0.0128 - val_mae: 0.0128 - val_mse: 1.9356e-04 - lr: 0.0010\n",
      "Epoch 94/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0224 - mae: 0.0224 - mse: 9.3971e-04 - val_loss: 0.0449 - val_mae: 0.0449 - val_mse: 0.0022 - lr: 0.0010\n",
      "Epoch 95/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0248 - mae: 0.0248 - mse: 8.1443e-04 - val_loss: 0.0443 - val_mae: 0.0443 - val_mse: 0.0021 - lr: 0.0010\n",
      "Epoch 96/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0244 - mae: 0.0244 - mse: 8.7407e-04 - val_loss: 0.0127 - val_mae: 0.0127 - val_mse: 1.9203e-04 - lr: 0.0010\n",
      "Epoch 97/1000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0238 - mae: 0.0238 - mse: 8.6898e-04 - val_loss: 0.0142 - val_mae: 0.0142 - val_mse: 2.9049e-04 - lr: 0.0010\n",
      "Epoch 98/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0215 - mae: 0.0215 - mse: 6.7843e-04 - val_loss: 0.0376 - val_mae: 0.0376 - val_mse: 0.0016 - lr: 0.0010\n",
      "Epoch 99/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0214 - mae: 0.0214 - mse: 6.7693e-04 - val_loss: 0.0293 - val_mae: 0.0293 - val_mse: 0.0010 - lr: 0.0010\n",
      "Epoch 100/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0199 - mae: 0.0199 - mse: 5.2238e-04 - val_loss: 0.0140 - val_mae: 0.0140 - val_mse: 2.6041e-04 - lr: 0.0010\n",
      "Epoch 101/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0267 - mae: 0.0267 - mse: 0.0011 - val_loss: 0.0133 - val_mae: 0.0133 - val_mse: 2.0699e-04 - lr: 0.0010\n",
      "Epoch 102/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0213 - mae: 0.0213 - mse: 7.5478e-04 - val_loss: 0.0200 - val_mae: 0.0200 - val_mse: 5.5962e-04 - lr: 0.0010\n",
      "Epoch 103/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0221 - mae: 0.0221 - mse: 7.6411e-04 - val_loss: 0.0266 - val_mae: 0.0266 - val_mse: 9.1221e-04 - lr: 0.0010\n",
      "Epoch 104/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0186 - mae: 0.0186 - mse: 4.8437e-04 - val_loss: 0.0243 - val_mae: 0.0243 - val_mse: 7.8430e-04 - lr: 0.0010\n",
      "Epoch 105/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0153 - mae: 0.0153 - mse: 3.6753e-04 - val_loss: 0.0225 - val_mae: 0.0225 - val_mse: 6.8612e-04 - lr: 0.0010\n",
      "Epoch 106/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0157 - mae: 0.0157 - mse: 4.1099e-04 - val_loss: 0.0346 - val_mae: 0.0346 - val_mse: 0.0014 - lr: 0.0010\n",
      "Epoch 107/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0222 - mae: 0.0222 - mse: 7.8347e-04 - val_loss: 0.0207 - val_mae: 0.0207 - val_mse: 5.9571e-04 - lr: 0.0010\n",
      "Epoch 108/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0198 - mae: 0.0198 - mse: 5.8305e-04 - val_loss: 0.0199 - val_mae: 0.0199 - val_mse: 5.5947e-04 - lr: 0.0010\n",
      "Epoch 109/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0198 - mae: 0.0198 - mse: 6.2582e-04 - val_loss: 0.0206 - val_mae: 0.0206 - val_mse: 5.8974e-04 - lr: 0.0010\n",
      "Epoch 110/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0171 - mae: 0.0171 - mse: 4.4822e-04 - val_loss: 0.0185 - val_mae: 0.0185 - val_mse: 4.8099e-04 - lr: 0.0010\n",
      "Epoch 111/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0139 - mae: 0.0139 - mse: 3.3197e-04 - val_loss: 0.0325 - val_mae: 0.0325 - val_mse: 0.0013 - lr: 0.0010\n",
      "Epoch 112/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0199 - mae: 0.0199 - mse: 6.7011e-04 - val_loss: 0.0213 - val_mae: 0.0213 - val_mse: 6.3335e-04 - lr: 0.0010\n",
      "Epoch 113/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0171 - mae: 0.0171 - mse: 5.1864e-04 - val_loss: 0.0170 - val_mae: 0.0170 - val_mse: 3.8814e-04 - lr: 0.0010\n",
      "Epoch 114/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0196 - mae: 0.0196 - mse: 6.0872e-04 - val_loss: 0.0386 - val_mae: 0.0386 - val_mse: 0.0017 - lr: 0.0010\n",
      "Epoch 115/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0205 - mae: 0.0205 - mse: 6.4996e-04 - val_loss: 0.0375 - val_mae: 0.0375 - val_mse: 0.0016 - lr: 0.0010\n",
      "Epoch 116/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0220 - mae: 0.0220 - mse: 7.0073e-04 - val_loss: 0.0161 - val_mae: 0.0161 - val_mse: 3.3235e-04 - lr: 0.0010\n",
      "Epoch 117/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0236 - mae: 0.0236 - mse: 7.8403e-04 - val_loss: 0.0315 - val_mae: 0.0315 - val_mse: 0.0012 - lr: 0.0010\n",
      "Epoch 118/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0191 - mae: 0.0191 - mse: 5.6976e-04 - val_loss: 0.0339 - val_mae: 0.0339 - val_mse: 0.0014 - lr: 0.0010\n",
      "Epoch 119/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0174 - mae: 0.0174 - mse: 4.6761e-04 - val_loss: 0.0175 - val_mae: 0.0175 - val_mse: 3.9917e-04 - lr: 0.0010\n",
      "Epoch 120/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0185 - mae: 0.0185 - mse: 5.1620e-04 - val_loss: 0.0184 - val_mae: 0.0184 - val_mse: 4.4892e-04 - lr: 0.0010\n",
      "Epoch 121/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0195 - mae: 0.0195 - mse: 6.3079e-04 - val_loss: 0.0462 - val_mae: 0.0462 - val_mse: 0.0024 - lr: 0.0010\n",
      "Epoch 122/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0216 - mae: 0.0216 - mse: 6.9760e-04 - val_loss: 0.0489 - val_mae: 0.0489 - val_mse: 0.0026 - lr: 0.0010\n",
      "Epoch 123/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0228 - mae: 0.0228 - mse: 8.2433e-04 - val_loss: 0.0215 - val_mae: 0.0215 - val_mse: 6.5119e-04 - lr: 0.0010\n",
      "Epoch 124/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0180 - mae: 0.0180 - mse: 5.6449e-04 - val_loss: 0.0183 - val_mae: 0.0183 - val_mse: 4.2792e-04 - lr: 0.0010\n",
      "Epoch 125/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0201 - mae: 0.0201 - mse: 5.5634e-04 - val_loss: 0.0399 - val_mae: 0.0399 - val_mse: 0.0019 - lr: 0.0010\n",
      "Epoch 126/1000\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0206 - mae: 0.0206 - mse: 6.4964e-04 - val_loss: 0.0206 - val_mae: 0.0206 - val_mse: 5.6655e-04 - lr: 0.0010\n",
      "Epoch 127/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0209 - mae: 0.0209 - mse: 6.4113e-04 - val_loss: 0.0240 - val_mae: 0.0240 - val_mse: 8.1606e-04 - lr: 0.0010\n",
      "Epoch 128/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0181 - mae: 0.0181 - mse: 4.9852e-04 - val_loss: 0.0405 - val_mae: 0.0405 - val_mse: 0.0019 - lr: 0.0010\n",
      "Epoch 129/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0208 - mae: 0.0208 - mse: 6.8551e-04 - val_loss: 0.0238 - val_mae: 0.0238 - val_mse: 7.9611e-04 - lr: 0.0010\n",
      "Epoch 130/1000\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0170 - mae: 0.0170 - mse: 5.2597e-04 - val_loss: 0.0215 - val_mae: 0.0215 - val_mse: 6.3924e-04 - lr: 0.0010\n",
      "Epoch 131/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0218 - mae: 0.0218 - mse: 6.8155e-04 - val_loss: 0.0408 - val_mae: 0.0408 - val_mse: 0.0019 - lr: 0.0010\n",
      "Epoch 132/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0191 - mae: 0.0191 - mse: 5.0610e-04 - val_loss: 0.0349 - val_mae: 0.0349 - val_mse: 0.0015 - lr: 0.0010\n",
      "Epoch 133/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0207 - mae: 0.0207 - mse: 6.2363e-04 - val_loss: 0.0180 - val_mae: 0.0180 - val_mse: 4.0393e-04 - lr: 0.0010\n",
      "Epoch 134/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0211 - mae: 0.0211 - mse: 6.6554e-04 - val_loss: 0.0256 - val_mae: 0.0256 - val_mse: 8.9892e-04 - lr: 0.0010\n",
      "Epoch 135/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0177 - mae: 0.0177 - mse: 5.1516e-04 - val_loss: 0.0484 - val_mae: 0.0484 - val_mse: 0.0026 - lr: 0.0010\n",
      "Epoch 136/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0225 - mae: 0.0225 - mse: 7.5912e-04 - val_loss: 0.0291 - val_mae: 0.0291 - val_mse: 0.0011 - lr: 0.0010\n",
      "Epoch 137/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0175 - mae: 0.0175 - mse: 4.6862e-04 - val_loss: 0.0217 - val_mae: 0.0217 - val_mse: 6.5779e-04 - lr: 0.0010\n",
      "Epoch 138/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0146 - mae: 0.0146 - mse: 3.3937e-04 - val_loss: 0.0267 - val_mae: 0.0267 - val_mse: 9.6287e-04 - lr: 0.0010\n",
      "Epoch 139/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0141 - mae: 0.0141 - mse: 4.4209e-04 - val_loss: 0.0219 - val_mae: 0.0219 - val_mse: 6.6947e-04 - lr: 0.0010\n",
      "Epoch 140/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0149 - mae: 0.0149 - mse: 4.2476e-04 - val_loss: 0.0157 - val_mae: 0.0157 - val_mse: 2.8402e-04 - lr: 0.0010\n",
      "Epoch 141/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0235 - mae: 0.0235 - mse: 8.0485e-04 - val_loss: 0.0211 - val_mae: 0.0211 - val_mse: 6.1913e-04 - lr: 0.0010\n",
      "Epoch 142/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0167 - mae: 0.0167 - mse: 3.8088e-04 - val_loss: 0.0363 - val_mae: 0.0363 - val_mse: 0.0016 - lr: 0.0010\n",
      "Epoch 143/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0203 - mae: 0.0203 - mse: 5.6347e-04 - val_loss: 0.0195 - val_mae: 0.0195 - val_mse: 5.0496e-04 - lr: 0.0010\n",
      "Epoch 144/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0172 - mae: 0.0172 - mse: 4.9110e-04 - val_loss: 0.0144 - val_mae: 0.0144 - val_mse: 2.7316e-04 - lr: 0.0010\n",
      "Epoch 145/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0284 - mae: 0.0284 - mse: 0.0010\n",
      "Epoch 00145: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0271 - mae: 0.0271 - mse: 9.5217e-04 - val_loss: 0.0251 - val_mae: 0.0251 - val_mse: 8.6196e-04 - lr: 0.0010\n",
      "Epoch 146/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0152 - mae: 0.0152 - mse: 3.6811e-04 - val_loss: 0.0297 - val_mae: 0.0297 - val_mse: 0.0011 - lr: 1.0000e-04\n",
      "Epoch 147/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0143 - mae: 0.0143 - mse: 2.7904e-04 - val_loss: 0.0326 - val_mae: 0.0326 - val_mse: 0.0013 - lr: 1.0000e-04\n",
      "Epoch 148/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0173 - mae: 0.0173 - mse: 4.0556e-04 - val_loss: 0.0331 - val_mae: 0.0331 - val_mse: 0.0014 - lr: 1.0000e-04\n",
      "Epoch 149/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0186 - mae: 0.0186 - mse: 5.1623e-04 - val_loss: 0.0316 - val_mae: 0.0316 - val_mse: 0.0013 - lr: 1.0000e-04\n",
      "Epoch 150/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0196 - mae: 0.0196 - mse: 5.6697e-04 - val_loss: 0.0284 - val_mae: 0.0284 - val_mse: 0.0011 - lr: 1.0000e-04\n",
      "Epoch 151/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0161 - mae: 0.0161 - mse: 3.8650e-04 - val_loss: 0.0255 - val_mae: 0.0255 - val_mse: 8.8822e-04 - lr: 1.0000e-04\n",
      "Epoch 152/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0154 - mae: 0.0154 - mse: 3.5316e-04 - val_loss: 0.0242 - val_mae: 0.0242 - val_mse: 8.1210e-04 - lr: 1.0000e-04\n",
      "Epoch 153/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0183 - mae: 0.0183 - mse: 4.5774e-04 - val_loss: 0.0227 - val_mae: 0.0227 - val_mse: 7.1943e-04 - lr: 1.0000e-04\n",
      "Epoch 154/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0155 - mae: 0.0155 - mse: 4.0603e-04 - val_loss: 0.0218 - val_mae: 0.0218 - val_mse: 6.6116e-04 - lr: 1.0000e-04\n",
      "Epoch 155/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0167 - mae: 0.0167 - mse: 4.3558e-04 - val_loss: 0.0219 - val_mae: 0.0219 - val_mse: 6.6454e-04 - lr: 1.0000e-04\n",
      "Epoch 156/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0168 - mae: 0.0168 - mse: 4.7307e-04 - val_loss: 0.0226 - val_mae: 0.0226 - val_mse: 7.1242e-04 - lr: 1.0000e-04\n",
      "Epoch 157/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0174 - mae: 0.0174 - mse: 4.7706e-04 - val_loss: 0.0243 - val_mae: 0.0243 - val_mse: 8.1478e-04 - lr: 1.0000e-04\n",
      "Epoch 158/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0162 - mae: 0.0162 - mse: 3.7209e-04 - val_loss: 0.0266 - val_mae: 0.0266 - val_mse: 9.5471e-04 - lr: 1.0000e-04\n",
      "Epoch 159/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0165 - mae: 0.0165 - mse: 4.0307e-04 - val_loss: 0.0294 - val_mae: 0.0294 - val_mse: 0.0011 - lr: 1.0000e-04\n",
      "Epoch 160/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0188 - mae: 0.0188 - mse: 5.2997e-04 - val_loss: 0.0308 - val_mae: 0.0308 - val_mse: 0.0012 - lr: 1.0000e-04\n",
      "Epoch 161/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0177 - mae: 0.0177 - mse: 4.8976e-04 - val_loss: 0.0283 - val_mae: 0.0283 - val_mse: 0.0011 - lr: 1.0000e-04\n",
      "Epoch 162/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0165 - mae: 0.0165 - mse: 4.4842e-04 - val_loss: 0.0256 - val_mae: 0.0256 - val_mse: 8.9738e-04 - lr: 1.0000e-04\n",
      "Epoch 163/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0184 - mae: 0.0184 - mse: 5.2005e-04 - val_loss: 0.0238 - val_mae: 0.0238 - val_mse: 7.9281e-04 - lr: 1.0000e-04\n",
      "Epoch 164/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0185 - mae: 0.0185 - mse: 5.0738e-04 - val_loss: 0.0229 - val_mae: 0.0229 - val_mse: 7.3360e-04 - lr: 1.0000e-04\n",
      "Epoch 165/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0144 - mae: 0.0144 - mse: 3.1304e-04 - val_loss: 0.0244 - val_mae: 0.0244 - val_mse: 8.2790e-04 - lr: 1.0000e-04\n",
      "Epoch 166/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0162 - mae: 0.0162 - mse: 5.1762e-04 - val_loss: 0.0281 - val_mae: 0.0281 - val_mse: 0.0011 - lr: 1.0000e-04\n",
      "Epoch 167/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0151 - mae: 0.0151 - mse: 3.3885e-04 - val_loss: 0.0306 - val_mae: 0.0306 - val_mse: 0.0012 - lr: 1.0000e-04\n",
      "Epoch 168/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0135 - mae: 0.0135 - mse: 2.9592e-04 - val_loss: 0.0312 - val_mae: 0.0312 - val_mse: 0.0012 - lr: 1.0000e-04\n",
      "Epoch 169/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0191 - mae: 0.0191 - mse: 5.6837e-04 - val_loss: 0.0283 - val_mae: 0.0283 - val_mse: 0.0011 - lr: 1.0000e-04\n",
      "Epoch 170/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0172 - mae: 0.0172 - mse: 4.5004e-04 - val_loss: 0.0257 - val_mae: 0.0257 - val_mse: 9.0340e-04 - lr: 1.0000e-04\n",
      "Epoch 171/1000\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0150 - mae: 0.0150 - mse: 3.5557e-04 - val_loss: 0.0238 - val_mae: 0.0238 - val_mse: 7.9321e-04 - lr: 1.0000e-04\n",
      "Epoch 172/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0175 - mae: 0.0175 - mse: 5.0598e-04 - val_loss: 0.0214 - val_mae: 0.0214 - val_mse: 6.3344e-04 - lr: 1.0000e-04\n",
      "Epoch 173/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0166 - mae: 0.0166 - mse: 3.8965e-04 - val_loss: 0.0203 - val_mae: 0.0203 - val_mse: 5.4443e-04 - lr: 1.0000e-04\n",
      "Epoch 174/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0168 - mae: 0.0168 - mse: 4.7711e-04 - val_loss: 0.0202 - val_mae: 0.0202 - val_mse: 5.3967e-04 - lr: 1.0000e-04\n",
      "Epoch 175/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0179 - mae: 0.0179 - mse: 5.2722e-04 - val_loss: 0.0210 - val_mae: 0.0210 - val_mse: 6.0067e-04 - lr: 1.0000e-04\n",
      "Epoch 176/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0173 - mae: 0.0173 - mse: 4.6273e-04 - val_loss: 0.0223 - val_mae: 0.0223 - val_mse: 6.9127e-04 - lr: 1.0000e-04\n",
      "Epoch 177/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0155 - mae: 0.0155 - mse: 3.8159e-04 - val_loss: 0.0239 - val_mae: 0.0239 - val_mse: 8.0033e-04 - lr: 1.0000e-04\n",
      "Epoch 178/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0184 - mae: 0.0184 - mse: 4.8654e-04 - val_loss: 0.0260 - val_mae: 0.0260 - val_mse: 9.2282e-04 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 179/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0191 - mae: 0.0191 - mse: 5.4811e-04 - val_loss: 0.0262 - val_mae: 0.0262 - val_mse: 9.3367e-04 - lr: 1.0000e-04\n",
      "Epoch 180/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0139 - mae: 0.0139 - mse: 3.0346e-04 - val_loss: 0.0261 - val_mae: 0.0261 - val_mse: 9.3177e-04 - lr: 1.0000e-04\n",
      "Epoch 181/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0191 - mae: 0.0191 - mse: 5.7344e-04 - val_loss: 0.0258 - val_mae: 0.0258 - val_mse: 9.1126e-04 - lr: 1.0000e-04\n",
      "Epoch 182/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0149 - mae: 0.0149 - mse: 3.4888e-04 - val_loss: 0.0255 - val_mae: 0.0255 - val_mse: 8.9554e-04 - lr: 1.0000e-04\n",
      "Epoch 183/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0152 - mae: 0.0152 - mse: 3.6724e-04 - val_loss: 0.0267 - val_mae: 0.0267 - val_mse: 9.6574e-04 - lr: 1.0000e-04\n",
      "Epoch 184/1000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0172 - mae: 0.0172 - mse: 4.1543e-04 - val_loss: 0.0260 - val_mae: 0.0260 - val_mse: 9.2438e-04 - lr: 1.0000e-04\n",
      "Epoch 185/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0149 - mae: 0.0149 - mse: 3.5918e-04 - val_loss: 0.0238 - val_mae: 0.0238 - val_mse: 7.9388e-04 - lr: 1.0000e-04\n",
      "Epoch 186/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0196 - mae: 0.0196 - mse: 5.5206e-04 - val_loss: 0.0218 - val_mae: 0.0218 - val_mse: 6.6174e-04 - lr: 1.0000e-04\n",
      "Epoch 187/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0163 - mae: 0.0163 - mse: 3.5679e-04 - val_loss: 0.0218 - val_mae: 0.0218 - val_mse: 6.5643e-04 - lr: 1.0000e-04\n",
      "Epoch 188/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0175 - mae: 0.0175 - mse: 5.1629e-04 - val_loss: 0.0248 - val_mae: 0.0248 - val_mse: 8.4963e-04 - lr: 1.0000e-04\n",
      "Epoch 189/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0187 - mae: 0.0187 - mse: 4.7173e-04 - val_loss: 0.0292 - val_mae: 0.0292 - val_mse: 0.0011 - lr: 1.0000e-04\n",
      "Epoch 190/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0162 - mae: 0.0162 - mse: 3.8977e-04 - val_loss: 0.0325 - val_mae: 0.0325 - val_mse: 0.0013 - lr: 1.0000e-04\n",
      "Epoch 191/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0185 - mae: 0.0185 - mse: 4.8070e-04 - val_loss: 0.0326 - val_mae: 0.0326 - val_mse: 0.0013 - lr: 1.0000e-04\n",
      "Epoch 192/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0137 - mae: 0.0137 - mse: 3.6772e-04 - val_loss: 0.0321 - val_mae: 0.0321 - val_mse: 0.0013 - lr: 1.0000e-04\n",
      "Epoch 193/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0168 - mae: 0.0168 - mse: 4.6973e-04 - val_loss: 0.0297 - val_mae: 0.0297 - val_mse: 0.0011 - lr: 1.0000e-04\n",
      "Epoch 194/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0134 - mae: 0.0134 - mse: 2.9763e-04 - val_loss: 0.0275 - val_mae: 0.0275 - val_mse: 0.0010 - lr: 1.0000e-04\n",
      "Epoch 195/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0161 - mae: 0.0161 - mse: 4.1178e-04 - val_loss: 0.0263 - val_mae: 0.0263 - val_mse: 9.4356e-04 - lr: 1.0000e-04\n",
      "Epoch 196/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0145 - mae: 0.0145 - mse: 3.7400e-04 - val_loss: 0.0249 - val_mae: 0.0249 - val_mse: 8.5732e-04 - lr: 1.0000e-04\n",
      "Epoch 197/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0167 - mae: 0.0167 - mse: 4.3020e-04 - val_loss: 0.0243 - val_mae: 0.0243 - val_mse: 8.2555e-04 - lr: 1.0000e-04\n",
      "Epoch 198/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0171 - mae: 0.0171 - mse: 4.4388e-04 - val_loss: 0.0234 - val_mae: 0.0234 - val_mse: 7.6224e-04 - lr: 1.0000e-04\n",
      "Epoch 199/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0163 - mae: 0.0163 - mse: 4.3762e-04 - val_loss: 0.0229 - val_mae: 0.0229 - val_mse: 7.3099e-04 - lr: 1.0000e-04\n",
      "Epoch 200/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0152 - mae: 0.0152 - mse: 3.3957e-04 - val_loss: 0.0231 - val_mae: 0.0231 - val_mse: 7.4261e-04 - lr: 1.0000e-04\n",
      "Epoch 201/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0142 - mae: 0.0142 - mse: 2.8710e-04 - val_loss: 0.0223 - val_mae: 0.0223 - val_mse: 6.8848e-04 - lr: 1.0000e-04\n",
      "Epoch 202/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0174 - mae: 0.0174 - mse: 4.8113e-04 - val_loss: 0.0229 - val_mae: 0.0229 - val_mse: 7.3297e-04 - lr: 1.0000e-04\n",
      "Epoch 203/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0146 - mae: 0.0146 - mse: 3.2364e-04 - val_loss: 0.0251 - val_mae: 0.0251 - val_mse: 8.7558e-04 - lr: 1.0000e-04\n",
      "Epoch 204/1000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0134 - mae: 0.0134 - mse: 3.1660e-04 - val_loss: 0.0281 - val_mae: 0.0281 - val_mse: 0.0011 - lr: 1.0000e-04\n",
      "Epoch 205/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0168 - mae: 0.0168 - mse: 5.0956e-04 - val_loss: 0.0310 - val_mae: 0.0310 - val_mse: 0.0012 - lr: 1.0000e-04\n",
      "Epoch 206/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0203 - mae: 0.0203 - mse: 6.8558e-04 - val_loss: 0.0317 - val_mae: 0.0317 - val_mse: 0.0013 - lr: 1.0000e-04\n",
      "Epoch 207/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0160 - mae: 0.0160 - mse: 4.3243e-04 - val_loss: 0.0282 - val_mae: 0.0282 - val_mse: 0.0011 - lr: 1.0000e-04\n",
      "Epoch 208/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0146 - mae: 0.0146 - mse: 3.4927e-04 - val_loss: 0.0245 - val_mae: 0.0245 - val_mse: 8.3692e-04 - lr: 1.0000e-04\n",
      "Epoch 209/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0136 - mae: 0.0136 - mse: 3.2732e-04 - val_loss: 0.0225 - val_mae: 0.0225 - val_mse: 7.0360e-04 - lr: 1.0000e-04\n",
      "Epoch 210/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0197 - mae: 0.0197 - mse: 6.7067e-04 - val_loss: 0.0209 - val_mae: 0.0209 - val_mse: 5.8041e-04 - lr: 1.0000e-04\n",
      "Epoch 211/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0163 - mae: 0.0163 - mse: 4.2121e-04 - val_loss: 0.0197 - val_mae: 0.0197 - val_mse: 4.9223e-04 - lr: 1.0000e-04\n",
      "Epoch 212/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0144 - mae: 0.0144 - mse: 3.4634e-04 - val_loss: 0.0199 - val_mae: 0.0199 - val_mse: 5.0799e-04 - lr: 1.0000e-04\n",
      "Epoch 213/1000\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0162 - mae: 0.0162 - mse: 4.5206e-04 - val_loss: 0.0218 - val_mae: 0.0218 - val_mse: 6.5910e-04 - lr: 1.0000e-04\n",
      "Epoch 214/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0193 - mae: 0.0193 - mse: 5.8779e-04 - val_loss: 0.0269 - val_mae: 0.0269 - val_mse: 9.8840e-04 - lr: 1.0000e-04\n",
      "Epoch 215/1000\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0165 - mae: 0.0165 - mse: 4.0232e-04 - val_loss: 0.0316 - val_mae: 0.0316 - val_mse: 0.0013 - lr: 1.0000e-04\n",
      "Epoch 216/1000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0190 - mae: 0.0190 - mse: 5.9861e-04 - val_loss: 0.0346 - val_mae: 0.0346 - val_mse: 0.0015 - lr: 1.0000e-04\n",
      "Epoch 217/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0165 - mae: 0.0165 - mse: 4.2938e-04 - val_loss: 0.0357 - val_mae: 0.0357 - val_mse: 0.0015 - lr: 1.0000e-04\n",
      "Epoch 218/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0169 - mae: 0.0169 - mse: 4.6232e-04 - val_loss: 0.0334 - val_mae: 0.0334 - val_mse: 0.0014 - lr: 1.0000e-04\n",
      "Epoch 219/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0168 - mae: 0.0168 - mse: 4.0566e-04 - val_loss: 0.0272 - val_mae: 0.0272 - val_mse: 0.0010 - lr: 1.0000e-04\n",
      "Epoch 220/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0141 - mae: 0.0141 - mse: 3.3931e-04 - val_loss: 0.0219 - val_mae: 0.0219 - val_mse: 6.6191e-04 - lr: 1.0000e-04\n",
      "Epoch 221/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0135 - mae: 0.0135 - mse: 2.7707e-04 - val_loss: 0.0196 - val_mae: 0.0196 - val_mse: 4.9295e-04 - lr: 1.0000e-04\n",
      "Epoch 222/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0161 - mae: 0.0161 - mse: 4.0554e-04 - val_loss: 0.0199 - val_mae: 0.0199 - val_mse: 5.1690e-04 - lr: 1.0000e-04\n",
      "Epoch 223/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0161 - mae: 0.0161 - mse: 4.0828e-04 - val_loss: 0.0224 - val_mae: 0.0224 - val_mse: 6.9978e-04 - lr: 1.0000e-04\n",
      "Epoch 224/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0146 - mae: 0.0146 - mse: 3.2066e-04 - val_loss: 0.0253 - val_mae: 0.0253 - val_mse: 8.8388e-04 - lr: 1.0000e-04\n",
      "Epoch 225/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0164 - mae: 0.0164 - mse: 4.1840e-04 - val_loss: 0.0273 - val_mae: 0.0273 - val_mse: 0.0010 - lr: 1.0000e-04\n",
      "Epoch 226/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0161 - mae: 0.0161 - mse: 3.7800e-04 - val_loss: 0.0269 - val_mae: 0.0269 - val_mse: 9.8505e-04 - lr: 1.0000e-04\n",
      "Epoch 227/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0151 - mae: 0.0151 - mse: 3.6780e-04 - val_loss: 0.0249 - val_mae: 0.0249 - val_mse: 8.5616e-04 - lr: 1.0000e-04\n",
      "Epoch 228/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0176 - mae: 0.0176 - mse: 4.9696e-04 - val_loss: 0.0232 - val_mae: 0.0232 - val_mse: 7.5152e-04 - lr: 1.0000e-04\n",
      "Epoch 229/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0131 - mae: 0.0131 - mse: 2.4557e-04 - val_loss: 0.0225 - val_mae: 0.0225 - val_mse: 7.0345e-04 - lr: 1.0000e-04\n",
      "Epoch 230/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0164 - mae: 0.0164 - mse: 4.4168e-04 - val_loss: 0.0232 - val_mae: 0.0232 - val_mse: 7.5421e-04 - lr: 1.0000e-04\n",
      "Epoch 231/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0187 - mae: 0.0187 - mse: 5.0373e-04 - val_loss: 0.0247 - val_mae: 0.0247 - val_mse: 8.4555e-04 - lr: 1.0000e-04\n",
      "Epoch 232/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0159 - mae: 0.0159 - mse: 4.0077e-04 - val_loss: 0.0262 - val_mae: 0.0262 - val_mse: 9.3558e-04 - lr: 1.0000e-04\n",
      "Epoch 233/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0135 - mae: 0.0135 - mse: 3.3499e-04 - val_loss: 0.0272 - val_mae: 0.0272 - val_mse: 9.9945e-04 - lr: 1.0000e-04\n",
      "Epoch 234/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0153 - mae: 0.0153 - mse: 3.9993e-04 - val_loss: 0.0275 - val_mae: 0.0275 - val_mse: 0.0010 - lr: 1.0000e-04\n",
      "Epoch 235/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0153 - mae: 0.0153 - mse: 3.4180e-04 - val_loss: 0.0272 - val_mae: 0.0272 - val_mse: 0.0010 - lr: 1.0000e-04\n",
      "Epoch 236/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0149 - mae: 0.0149 - mse: 3.2514e-04 - val_loss: 0.0269 - val_mae: 0.0269 - val_mse: 9.7856e-04 - lr: 1.0000e-04\n",
      "Epoch 237/1000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0153 - mae: 0.0153 - mse: 3.2638e-04 - val_loss: 0.0251 - val_mae: 0.0251 - val_mse: 8.6759e-04 - lr: 1.0000e-04\n",
      "Epoch 238/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0139 - mae: 0.0139 - mse: 3.4485e-04 - val_loss: 0.0234 - val_mae: 0.0234 - val_mse: 7.6486e-04 - lr: 1.0000e-04\n",
      "Epoch 239/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0164 - mae: 0.0164 - mse: 4.0323e-04 - val_loss: 0.0229 - val_mae: 0.0229 - val_mse: 7.3036e-04 - lr: 1.0000e-04\n",
      "Epoch 240/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0172 - mae: 0.0172 - mse: 4.9558e-04 - val_loss: 0.0219 - val_mae: 0.0219 - val_mse: 6.6473e-04 - lr: 1.0000e-04\n",
      "Epoch 241/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0193 - mae: 0.0193 - mse: 5.1068e-04 - val_loss: 0.0210 - val_mae: 0.0210 - val_mse: 5.9813e-04 - lr: 1.0000e-04\n",
      "Epoch 242/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0131 - mae: 0.0131 - mse: 2.6149e-04 - val_loss: 0.0217 - val_mae: 0.0217 - val_mse: 6.5235e-04 - lr: 1.0000e-04\n",
      "Epoch 243/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0180 - mae: 0.0180 - mse: 5.1040e-04 - val_loss: 0.0243 - val_mae: 0.0243 - val_mse: 8.2041e-04 - lr: 1.0000e-04\n",
      "Epoch 244/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0141 - mae: 0.0141 - mse: 3.4286e-04 - val_loss: 0.0266 - val_mae: 0.0266 - val_mse: 9.6483e-04 - lr: 1.0000e-04\n",
      "Epoch 245/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0159 - mae: 0.0159 - mse: 3.7822e-04\n",
      "Epoch 00245: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0161 - mae: 0.0161 - mse: 3.8245e-04 - val_loss: 0.0288 - val_mae: 0.0288 - val_mse: 0.0011 - lr: 1.0000e-04\n",
      "Epoch 246/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0174 - mae: 0.0174 - mse: 5.0921e-04 - val_loss: 0.0288 - val_mae: 0.0288 - val_mse: 0.0011 - lr: 1.0000e-05\n",
      "Epoch 247/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0153 - mae: 0.0153 - mse: 3.9873e-04 - val_loss: 0.0285 - val_mae: 0.0285 - val_mse: 0.0011 - lr: 1.0000e-05\n",
      "Epoch 248/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0145 - mae: 0.0145 - mse: 3.7665e-04 - val_loss: 0.0283 - val_mae: 0.0283 - val_mse: 0.0011 - lr: 1.0000e-05\n",
      "Epoch 249/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0165 - mae: 0.0165 - mse: 4.4172e-04 - val_loss: 0.0281 - val_mae: 0.0281 - val_mse: 0.0011 - lr: 1.0000e-05\n",
      "Epoch 250/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0160 - mae: 0.0160 - mse: 3.7361e-04 - val_loss: 0.0280 - val_mae: 0.0280 - val_mse: 0.0010 - lr: 1.0000e-05\n",
      "Epoch 251/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0165 - mae: 0.0165 - mse: 3.9764e-04 - val_loss: 0.0277 - val_mae: 0.0277 - val_mse: 0.0010 - lr: 1.0000e-05\n",
      "Epoch 252/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0156 - mae: 0.0156 - mse: 4.2560e-04 - val_loss: 0.0273 - val_mae: 0.0273 - val_mse: 0.0010 - lr: 1.0000e-05\n",
      "Epoch 253/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0152 - mae: 0.0152 - mse: 3.3002e-04 - val_loss: 0.0268 - val_mae: 0.0268 - val_mse: 9.7647e-04 - lr: 1.0000e-05\n",
      "Epoch 254/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0177 - mae: 0.0177 - mse: 5.4612e-04 - val_loss: 0.0261 - val_mae: 0.0261 - val_mse: 9.2976e-04 - lr: 1.0000e-05\n",
      "Epoch 255/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0152 - mae: 0.0152 - mse: 3.8745e-04 - val_loss: 0.0253 - val_mae: 0.0253 - val_mse: 8.8289e-04 - lr: 1.0000e-05\n",
      "Epoch 256/1000\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0138 - mae: 0.0138 - mse: 3.1391e-04 - val_loss: 0.0246 - val_mae: 0.0246 - val_mse: 8.4047e-04 - lr: 1.0000e-05\n",
      "Epoch 257/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0159 - mae: 0.0159 - mse: 3.7336e-04 - val_loss: 0.0240 - val_mae: 0.0240 - val_mse: 8.0310e-04 - lr: 1.0000e-05\n",
      "Epoch 258/1000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0181 - mae: 0.0181 - mse: 4.4284e-04 - val_loss: 0.0236 - val_mae: 0.0236 - val_mse: 7.7632e-04 - lr: 1.0000e-05\n",
      "Epoch 259/1000\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0170 - mae: 0.0170 - mse: 4.2374e-04 - val_loss: 0.0233 - val_mae: 0.0233 - val_mse: 7.5813e-04 - lr: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mae\",optimizer = tf.keras.optimizers.Adam(), metrics=[\"mae\",\"mse\"])\n",
    "history = model.fit(final_x_zeros_scaled[:43],final_y_zeros[:43],\n",
    "                    epochs = 1000,\n",
    "                    validation_data = (final_x_zeros_scaled[43:], final_y_zeros[43:]),\n",
    "                   callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", \n",
    "                                                        patience=200, \n",
    "                                                        restore_best_weights=True),\n",
    "                      tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", \n",
    "                                                           patience=100, \n",
    "                                                           verbose=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1af212",
   "metadata": {},
   "source": [
    "# FINE TUNING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8552812c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 252/2000\n",
      "2/2 [==============================] - 6s 1s/step - loss: 0.0252 - mae: 0.0252 - mse: 0.0010 - val_loss: 0.0183 - val_mae: 0.0183 - val_mse: 4.5804e-04 - lr: 1.0000e-04\n",
      "Epoch 253/2000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.0241 - mae: 0.0241 - mse: 9.3979e-04 - val_loss: 0.0268 - val_mae: 0.0268 - val_mse: 8.7666e-04 - lr: 1.0000e-04\n",
      "Epoch 254/2000\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0312 - mae: 0.0312 - mse: 0.0015 - val_loss: 0.0299 - val_mae: 0.0299 - val_mse: 0.0010 - lr: 1.0000e-04\n",
      "Epoch 255/2000\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0257 - mae: 0.0257 - mse: 0.0012 - val_loss: 0.0231 - val_mae: 0.0231 - val_mse: 6.9156e-04 - lr: 1.0000e-04\n",
      "Epoch 256/2000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.0218 - mae: 0.0218 - mse: 7.0300e-04 - val_loss: 0.0176 - val_mae: 0.0176 - val_mse: 4.3053e-04 - lr: 1.0000e-04\n",
      "Epoch 257/2000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.0183 - mae: 0.0183 - mse: 5.8081e-04 - val_loss: 0.0121 - val_mae: 0.0121 - val_mse: 2.2860e-04 - lr: 1.0000e-04\n",
      "Epoch 258/2000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.0256 - mae: 0.0256 - mse: 0.0010 - val_loss: 0.0118 - val_mae: 0.0118 - val_mse: 2.0204e-04 - lr: 1.0000e-04\n",
      "Epoch 259/2000\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0255 - mae: 0.0255 - mse: 9.2921e-04 - val_loss: 0.0139 - val_mae: 0.0139 - val_mse: 2.9413e-04 - lr: 1.0000e-04\n",
      "Epoch 260/2000\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0204 - mae: 0.0204 - mse: 7.1985e-04 - val_loss: 0.0187 - val_mae: 0.0187 - val_mse: 4.7846e-04 - lr: 1.0000e-04\n",
      "Epoch 261/2000\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0243 - mae: 0.0243 - mse: 9.7385e-04 - val_loss: 0.0251 - val_mae: 0.0251 - val_mse: 7.8913e-04 - lr: 1.0000e-04\n",
      "Epoch 262/2000\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0269 - mae: 0.0269 - mse: 0.0011 - val_loss: 0.0236 - val_mae: 0.0236 - val_mse: 7.1776e-04 - lr: 1.0000e-04\n",
      "Epoch 263/2000\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0215 - mae: 0.0215 - mse: 7.8104e-04 - val_loss: 0.0181 - val_mae: 0.0181 - val_mse: 4.5223e-04 - lr: 1.0000e-04\n",
      "Epoch 264/2000\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0177 - mae: 0.0177 - mse: 5.9659e-04 - val_loss: 0.0147 - val_mae: 0.0147 - val_mse: 3.2681e-04 - lr: 1.0000e-04\n",
      "Epoch 265/2000\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0256 - mae: 0.0256 - mse: 9.6945e-04 - val_loss: 0.0148 - val_mae: 0.0148 - val_mse: 3.2802e-04 - lr: 1.0000e-04\n",
      "Epoch 266/2000\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0282 - mae: 0.0282 - mse: 0.0011 - val_loss: 0.0180 - val_mae: 0.0180 - val_mse: 4.5114e-04 - lr: 1.0000e-04\n",
      "Epoch 267/2000\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0209 - mae: 0.0209 - mse: 6.7040e-04 - val_loss: 0.0234 - val_mae: 0.0234 - val_mse: 7.1224e-04 - lr: 1.0000e-04\n",
      "Epoch 268/2000\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.0217 - mae: 0.0217 - mse: 7.6560e-04 - val_loss: 0.0282 - val_mae: 0.0282 - val_mse: 9.6175e-04 - lr: 1.0000e-04\n",
      "Epoch 269/2000\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0274 - mae: 0.0274 - mse: 0.0011 - val_loss: 0.0293 - val_mae: 0.0293 - val_mse: 0.0010 - lr: 1.0000e-04\n",
      "Epoch 270/2000\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0231 - mae: 0.0231 - mse: 8.1006e-04 - val_loss: 0.0276 - val_mae: 0.0276 - val_mse: 9.2702e-04 - lr: 1.0000e-04\n",
      "Epoch 271/2000\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0211 - mae: 0.0211 - mse: 7.2961e-04 - val_loss: 0.0234 - val_mae: 0.0234 - val_mse: 7.1732e-04 - lr: 1.0000e-04\n",
      "Epoch 272/2000\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0222 - mae: 0.0222 - mse: 8.8645e-04 - val_loss: 0.0182 - val_mae: 0.0182 - val_mse: 4.5933e-04 - lr: 1.0000e-04\n",
      "Epoch 273/2000\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0214 - mae: 0.0214 - mse: 7.3971e-04 - val_loss: 0.0145 - val_mae: 0.0145 - val_mse: 3.1576e-04 - lr: 1.0000e-04\n",
      "Epoch 274/2000\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0239 - mae: 0.0239 - mse: 8.4218e-04 - val_loss: 0.0140 - val_mae: 0.0140 - val_mse: 2.9822e-04 - lr: 1.0000e-04\n",
      "Epoch 275/2000\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0200 - mae: 0.0200 - mse: 5.4862e-04 - val_loss: 0.0160 - val_mae: 0.0160 - val_mse: 3.7415e-04 - lr: 1.0000e-04\n",
      "Epoch 276/2000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0198 - mae: 0.0198 - mse: 6.2441e-04 - val_loss: 0.0194 - val_mae: 0.0194 - val_mse: 5.1196e-04 - lr: 1.0000e-04\n",
      "Epoch 277/2000\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0224 - mae: 0.0224 - mse: 7.9299e-04 - val_loss: 0.0213 - val_mae: 0.0213 - val_mse: 6.0549e-04 - lr: 1.0000e-04\n",
      "Epoch 278/2000\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0228 - mae: 0.0228 - mse: 9.0471e-04 - val_loss: 0.0224 - val_mae: 0.0224 - val_mse: 6.5824e-04 - lr: 1.0000e-04\n",
      "Epoch 279/2000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0222 - mae: 0.0222 - mse: 7.7668e-04 - val_loss: 0.0239 - val_mae: 0.0239 - val_mse: 7.4143e-04 - lr: 1.0000e-04\n",
      "Epoch 280/2000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0244 - mae: 0.0244 - mse: 9.6976e-04 - val_loss: 0.0262 - val_mae: 0.0262 - val_mse: 8.5544e-04 - lr: 1.0000e-04\n",
      "Epoch 281/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0214 - mae: 0.0214 - mse: 7.2923e-04 - val_loss: 0.0265 - val_mae: 0.0265 - val_mse: 8.6965e-04 - lr: 1.0000e-04\n",
      "Epoch 282/2000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0259 - mae: 0.0259 - mse: 9.7005e-04 - val_loss: 0.0213 - val_mae: 0.0213 - val_mse: 6.0355e-04 - lr: 1.0000e-04\n",
      "Epoch 283/2000\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0195 - mae: 0.0195 - mse: 5.9900e-04 - val_loss: 0.0180 - val_mae: 0.0180 - val_mse: 4.5359e-04 - lr: 1.0000e-04\n",
      "Epoch 284/2000\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0201 - mae: 0.0201 - mse: 6.4621e-04 - val_loss: 0.0155 - val_mae: 0.0155 - val_mse: 3.5657e-04 - lr: 1.0000e-04\n",
      "Epoch 285/2000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0229 - mae: 0.0229 - mse: 8.0864e-04 - val_loss: 0.0157 - val_mae: 0.0157 - val_mse: 3.6250e-04 - lr: 1.0000e-04\n",
      "Epoch 286/2000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0235 - mae: 0.0235 - mse: 8.7773e-04 - val_loss: 0.0186 - val_mae: 0.0186 - val_mse: 4.7948e-04 - lr: 1.0000e-04\n",
      "Epoch 287/2000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0219 - mae: 0.0219 - mse: 7.4697e-04 - val_loss: 0.0221 - val_mae: 0.0221 - val_mse: 6.4685e-04 - lr: 1.0000e-04\n",
      "Epoch 288/2000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0231 - mae: 0.0231 - mse: 8.7490e-04 - val_loss: 0.0255 - val_mae: 0.0255 - val_mse: 8.2654e-04 - lr: 1.0000e-04\n",
      "Epoch 289/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0216 - mae: 0.0216 - mse: 7.5373e-04 - val_loss: 0.0255 - val_mae: 0.0255 - val_mse: 8.2413e-04 - lr: 1.0000e-04\n",
      "Epoch 290/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0265 - mae: 0.0265 - mse: 0.0011 - val_loss: 0.0212 - val_mae: 0.0212 - val_mse: 5.9899e-04 - lr: 1.0000e-04\n",
      "Epoch 291/2000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0261 - mae: 0.0261 - mse: 0.0012 - val_loss: 0.0162 - val_mae: 0.0162 - val_mse: 3.8390e-04 - lr: 1.0000e-04\n",
      "Epoch 292/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0234 - mae: 0.0234 - mse: 8.7244e-04 - val_loss: 0.0157 - val_mae: 0.0157 - val_mse: 3.6198e-04 - lr: 1.0000e-04\n",
      "Epoch 293/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0226 - mae: 0.0226 - mse: 7.8272e-04 - val_loss: 0.0155 - val_mae: 0.0155 - val_mse: 3.5329e-04 - lr: 1.0000e-04\n",
      "Epoch 294/2000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0230 - mae: 0.0230 - mse: 8.3995e-04 - val_loss: 0.0173 - val_mae: 0.0173 - val_mse: 4.2962e-04 - lr: 1.0000e-04\n",
      "Epoch 295/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0198 - mae: 0.0198 - mse: 7.1953e-04 - val_loss: 0.0193 - val_mae: 0.0193 - val_mse: 5.1144e-04 - lr: 1.0000e-04\n",
      "Epoch 296/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0236 - mae: 0.0236 - mse: 7.9373e-04 - val_loss: 0.0179 - val_mae: 0.0179 - val_mse: 4.5217e-04 - lr: 1.0000e-04\n",
      "Epoch 297/2000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0244 - mae: 0.0244 - mse: 8.9679e-04 - val_loss: 0.0185 - val_mae: 0.0185 - val_mse: 4.7606e-04 - lr: 1.0000e-04\n",
      "Epoch 298/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0198 - mae: 0.0198 - mse: 7.6213e-04 - val_loss: 0.0186 - val_mae: 0.0186 - val_mse: 4.8203e-04 - lr: 1.0000e-04\n",
      "Epoch 299/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0238 - mae: 0.0238 - mse: 9.0146e-04 - val_loss: 0.0193 - val_mae: 0.0193 - val_mse: 5.1287e-04 - lr: 1.0000e-04\n",
      "Epoch 300/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0200 - mae: 0.0200 - mse: 6.1367e-04 - val_loss: 0.0210 - val_mae: 0.0210 - val_mse: 5.9294e-04 - lr: 1.0000e-04\n",
      "Epoch 301/2000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0196 - mae: 0.0196 - mse: 6.9037e-04 - val_loss: 0.0252 - val_mae: 0.0252 - val_mse: 8.1524e-04 - lr: 1.0000e-04\n",
      "Epoch 302/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0216 - mae: 0.0216 - mse: 7.7474e-04 - val_loss: 0.0283 - val_mae: 0.0283 - val_mse: 9.8047e-04 - lr: 1.0000e-04\n",
      "Epoch 303/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0228 - mae: 0.0228 - mse: 8.1713e-04 - val_loss: 0.0291 - val_mae: 0.0291 - val_mse: 0.0010 - lr: 1.0000e-04\n",
      "Epoch 304/2000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0245 - mae: 0.0245 - mse: 9.6661e-04 - val_loss: 0.0260 - val_mae: 0.0260 - val_mse: 8.5969e-04 - lr: 1.0000e-04\n",
      "Epoch 305/2000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0219 - mae: 0.0219 - mse: 8.6866e-04 - val_loss: 0.0235 - val_mae: 0.0235 - val_mse: 7.2836e-04 - lr: 1.0000e-04\n",
      "Epoch 306/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0217 - mae: 0.0217 - mse: 6.9357e-04 - val_loss: 0.0226 - val_mae: 0.0226 - val_mse: 6.7846e-04 - lr: 1.0000e-04\n",
      "Epoch 307/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0216 - mae: 0.0216 - mse: 7.6689e-04 - val_loss: 0.0239 - val_mae: 0.0239 - val_mse: 7.5190e-04 - lr: 1.0000e-04\n",
      "Epoch 308/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0229 - mae: 0.0229 - mse: 8.1565e-04 - val_loss: 0.0245 - val_mae: 0.0245 - val_mse: 7.8436e-04 - lr: 1.0000e-04\n",
      "Epoch 309/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0221 - mae: 0.0221 - mse: 6.3350e-04 - val_loss: 0.0228 - val_mae: 0.0228 - val_mse: 6.9242e-04 - lr: 1.0000e-04\n",
      "Epoch 310/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0214 - mae: 0.0214 - mse: 7.8289e-04 - val_loss: 0.0215 - val_mae: 0.0215 - val_mse: 6.2235e-04 - lr: 1.0000e-04\n",
      "Epoch 311/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0200 - mae: 0.0200 - mse: 8.4896e-04 - val_loss: 0.0189 - val_mae: 0.0189 - val_mse: 4.9818e-04 - lr: 1.0000e-04\n",
      "Epoch 312/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0230 - mae: 0.0230 - mse: 7.4130e-04 - val_loss: 0.0172 - val_mae: 0.0172 - val_mse: 4.2673e-04 - lr: 1.0000e-04\n",
      "Epoch 313/2000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0208 - mae: 0.0208 - mse: 6.7548e-04 - val_loss: 0.0170 - val_mae: 0.0170 - val_mse: 4.1648e-04 - lr: 1.0000e-04\n",
      "Epoch 314/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0211 - mae: 0.0211 - mse: 7.1209e-04 - val_loss: 0.0198 - val_mae: 0.0198 - val_mse: 5.4136e-04 - lr: 1.0000e-04\n",
      "Epoch 315/2000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0197 - mae: 0.0197 - mse: 7.0945e-04 - val_loss: 0.0248 - val_mae: 0.0248 - val_mse: 8.0212e-04 - lr: 1.0000e-04\n",
      "Epoch 316/2000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0248 - mae: 0.0248 - mse: 0.0010 - val_loss: 0.0267 - val_mae: 0.0267 - val_mse: 9.0200e-04 - lr: 1.0000e-04\n",
      "Epoch 317/2000\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0233 - mae: 0.0233 - mse: 8.9582e-04 - val_loss: 0.0257 - val_mae: 0.0257 - val_mse: 8.4662e-04 - lr: 1.0000e-04\n",
      "Epoch 318/2000\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0195 - mae: 0.0195 - mse: 5.9166e-04 - val_loss: 0.0238 - val_mae: 0.0238 - val_mse: 7.4687e-04 - lr: 1.0000e-04\n",
      "Epoch 319/2000\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0214 - mae: 0.0214 - mse: 6.9283e-04 - val_loss: 0.0218 - val_mae: 0.0218 - val_mse: 6.3975e-04 - lr: 1.0000e-04\n",
      "Epoch 320/2000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0164 - mae: 0.0164 - mse: 5.0091e-04 - val_loss: 0.0224 - val_mae: 0.0224 - val_mse: 6.7007e-04 - lr: 1.0000e-04\n",
      "Epoch 321/2000\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0218 - mae: 0.0218 - mse: 7.8414e-04 - val_loss: 0.0252 - val_mae: 0.0252 - val_mse: 8.2750e-04 - lr: 1.0000e-04\n",
      "Epoch 322/2000\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0220 - mae: 0.0220 - mse: 7.3734e-04 - val_loss: 0.0256 - val_mae: 0.0256 - val_mse: 8.4649e-04 - lr: 1.0000e-04\n",
      "Epoch 323/2000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0241 - mae: 0.0241 - mse: 8.7397e-04 - val_loss: 0.0232 - val_mae: 0.0232 - val_mse: 7.1642e-04 - lr: 1.0000e-04\n",
      "Epoch 324/2000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0189 - mae: 0.0189 - mse: 5.4261e-04 - val_loss: 0.0210 - val_mae: 0.0210 - val_mse: 5.9986e-04 - lr: 1.0000e-04\n",
      "Epoch 325/2000\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0263 - mae: 0.0263 - mse: 0.0010 - val_loss: 0.0213 - val_mae: 0.0213 - val_mse: 6.1642e-04 - lr: 1.0000e-04\n",
      "Epoch 326/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0194 - mae: 0.0194 - mse: 6.3758e-04 - val_loss: 0.0255 - val_mae: 0.0255 - val_mse: 8.4680e-04 - lr: 1.0000e-04\n",
      "Epoch 327/2000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0243 - mae: 0.0243 - mse: 8.5778e-04 - val_loss: 0.0297 - val_mae: 0.0297 - val_mse: 0.0011 - lr: 1.0000e-04\n",
      "Epoch 328/2000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0200 - mae: 0.0200 - mse: 6.3924e-04 - val_loss: 0.0298 - val_mae: 0.0298 - val_mse: 0.0011 - lr: 1.0000e-04\n",
      "Epoch 329/2000\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0232 - mae: 0.0232 - mse: 8.2016e-04 - val_loss: 0.0311 - val_mae: 0.0311 - val_mse: 0.0012 - lr: 1.0000e-04\n",
      "Epoch 330/2000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0218 - mae: 0.0218 - mse: 6.9361e-04 - val_loss: 0.0295 - val_mae: 0.0295 - val_mse: 0.0011 - lr: 1.0000e-04\n",
      "Epoch 331/2000\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0212 - mae: 0.0212 - mse: 6.8783e-04 - val_loss: 0.0249 - val_mae: 0.0249 - val_mse: 8.1847e-04 - lr: 1.0000e-04\n",
      "Epoch 332/2000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0181 - mae: 0.0181 - mse: 5.5846e-04 - val_loss: 0.0217 - val_mae: 0.0217 - val_mse: 6.3679e-04 - lr: 1.0000e-04\n",
      "Epoch 333/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0178 - mae: 0.0178 - mse: 5.9087e-04 - val_loss: 0.0228 - val_mae: 0.0228 - val_mse: 6.9799e-04 - lr: 1.0000e-04\n",
      "Epoch 334/2000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0205 - mae: 0.0205 - mse: 6.8689e-04 - val_loss: 0.0232 - val_mae: 0.0232 - val_mse: 7.2231e-04 - lr: 1.0000e-04\n",
      "Epoch 335/2000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0179 - mae: 0.0179 - mse: 5.1348e-04 - val_loss: 0.0236 - val_mae: 0.0236 - val_mse: 7.4361e-04 - lr: 1.0000e-04\n",
      "Epoch 336/2000\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0244 - mae: 0.0244 - mse: 8.3715e-04 - val_loss: 0.0253 - val_mae: 0.0253 - val_mse: 8.4223e-04 - lr: 1.0000e-04\n",
      "Epoch 337/2000\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0191 - mae: 0.0191 - mse: 5.3370e-04 - val_loss: 0.0263 - val_mae: 0.0263 - val_mse: 8.9598e-04 - lr: 1.0000e-04\n",
      "Epoch 338/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0199 - mae: 0.0199 - mse: 8.4114e-04 - val_loss: 0.0256 - val_mae: 0.0256 - val_mse: 8.5850e-04 - lr: 1.0000e-04\n",
      "Epoch 339/2000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0179 - mae: 0.0179 - mse: 4.5256e-04 - val_loss: 0.0253 - val_mae: 0.0253 - val_mse: 8.4237e-04 - lr: 1.0000e-04\n",
      "Epoch 340/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0237 - mae: 0.0237 - mse: 7.9350e-04 - val_loss: 0.0243 - val_mae: 0.0243 - val_mse: 7.8242e-04 - lr: 1.0000e-04\n",
      "Epoch 341/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0211 - mae: 0.0211 - mse: 7.2542e-04 - val_loss: 0.0209 - val_mae: 0.0209 - val_mse: 6.0317e-04 - lr: 1.0000e-04\n",
      "Epoch 342/2000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0198 - mae: 0.0198 - mse: 5.7914e-04 - val_loss: 0.0154 - val_mae: 0.0154 - val_mse: 3.3399e-04 - lr: 1.0000e-04\n",
      "Epoch 343/2000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0215 - mae: 0.0215 - mse: 7.1702e-04 - val_loss: 0.0141 - val_mae: 0.0141 - val_mse: 2.4940e-04 - lr: 1.0000e-04\n",
      "Epoch 344/2000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0255 - mae: 0.0255 - mse: 9.8526e-04 - val_loss: 0.0141 - val_mae: 0.0141 - val_mse: 2.5004e-04 - lr: 1.0000e-04\n",
      "Epoch 345/2000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0224 - mae: 0.0224 - mse: 7.7505e-04 - val_loss: 0.0170 - val_mae: 0.0170 - val_mse: 4.1201e-04 - lr: 1.0000e-04\n",
      "Epoch 346/2000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0193 - mae: 0.0193 - mse: 5.3251e-04 - val_loss: 0.0288 - val_mae: 0.0288 - val_mse: 0.0010 - lr: 1.0000e-04\n",
      "Epoch 347/2000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0257 - mae: 0.0257 - mse: 0.0011 - val_loss: 0.0414 - val_mae: 0.0414 - val_mse: 0.0019 - lr: 1.0000e-04\n",
      "Epoch 348/2000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0254 - mae: 0.0254 - mse: 0.0010 - val_loss: 0.0460 - val_mae: 0.0460 - val_mse: 0.0023 - lr: 1.0000e-04\n",
      "Epoch 349/2000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0269 - mae: 0.0269 - mse: 9.9579e-04 - val_loss: 0.0422 - val_mae: 0.0422 - val_mse: 0.0020 - lr: 1.0000e-04\n",
      "Epoch 350/2000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0217 - mae: 0.0217 - mse: 7.1123e-04 - val_loss: 0.0297 - val_mae: 0.0297 - val_mse: 0.0011 - lr: 1.0000e-04\n",
      "Epoch 351/2000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0234 - mae: 0.0234 - mse: 8.2762e-04 - val_loss: 0.0165 - val_mae: 0.0165 - val_mse: 3.8788e-04 - lr: 1.0000e-04\n",
      "Epoch 352/2000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0213 - mae: 0.0213 - mse: 6.7202e-04 - val_loss: 0.0137 - val_mae: 0.0137 - val_mse: 2.3025e-04 - lr: 1.0000e-04\n",
      "Epoch 353/2000\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0284 - mae: 0.0284 - mse: 0.0011 - val_loss: 0.0139 - val_mae: 0.0139 - val_mse: 2.3728e-04 - lr: 1.0000e-04\n",
      "Epoch 354/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0296 - mae: 0.0296 - mse: 0.0012 - val_loss: 0.0169 - val_mae: 0.0169 - val_mse: 4.0882e-04 - lr: 1.0000e-04\n",
      "Epoch 355/2000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0188 - mae: 0.0188 - mse: 5.8040e-04 - val_loss: 0.0251 - val_mae: 0.0251 - val_mse: 8.2936e-04 - lr: 1.0000e-04\n",
      "Epoch 356/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0186 - mae: 0.0186 - mse: 5.0836e-04 - val_loss: 0.0304 - val_mae: 0.0304 - val_mse: 0.0011 - lr: 1.0000e-04\n",
      "Epoch 357/2000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0221 - mae: 0.0221 - mse: 7.6611e-04 - val_loss: 0.0285 - val_mae: 0.0285 - val_mse: 0.0010 - lr: 1.0000e-04\n",
      "Epoch 358/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0192 - mae: 0.0192 - mse: 7.4526e-04\n",
      "Epoch 00358: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0212 - mae: 0.0212 - mse: 7.9017e-04 - val_loss: 0.0237 - val_mae: 0.0237 - val_mse: 7.4895e-04 - lr: 1.0000e-04\n",
      "Epoch 359/2000\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0203 - mae: 0.0203 - mse: 6.8291e-04 - val_loss: 0.0232 - val_mae: 0.0232 - val_mse: 7.2125e-04 - lr: 1.0000e-05\n",
      "Epoch 360/2000\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0164 - mae: 0.0164 - mse: 4.6581e-04 - val_loss: 0.0227 - val_mae: 0.0227 - val_mse: 6.9409e-04 - lr: 1.0000e-05\n",
      "Epoch 361/2000\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.0201 - mae: 0.0201 - mse: 6.5192e-04 - val_loss: 0.0224 - val_mae: 0.0224 - val_mse: 6.7737e-04 - lr: 1.0000e-05\n",
      "Epoch 362/2000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0220 - mae: 0.0220 - mse: 7.9611e-04 - val_loss: 0.0224 - val_mae: 0.0224 - val_mse: 6.7783e-04 - lr: 1.0000e-05\n",
      "Epoch 363/2000\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0154 - mae: 0.0154 - mse: 4.3010e-04 - val_loss: 0.0224 - val_mae: 0.0224 - val_mse: 6.7571e-04 - lr: 1.0000e-05\n",
      "Epoch 364/2000\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0212 - mae: 0.0212 - mse: 7.7077e-04 - val_loss: 0.0223 - val_mae: 0.0223 - val_mse: 6.7130e-04 - lr: 1.0000e-05\n",
      "Epoch 365/2000\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0137 - mae: 0.0137 - mse: 3.2030e-04 - val_loss: 0.0223 - val_mae: 0.0223 - val_mse: 6.7380e-04 - lr: 1.0000e-05\n",
      "Epoch 366/2000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0151 - mae: 0.0151 - mse: 3.8660e-04 - val_loss: 0.0223 - val_mae: 0.0223 - val_mse: 6.7346e-04 - lr: 1.0000e-05\n",
      "Epoch 367/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0201 - mae: 0.0201 - mse: 6.4746e-04 - val_loss: 0.0224 - val_mae: 0.0224 - val_mse: 6.7931e-04 - lr: 1.0000e-05\n",
      "Epoch 368/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0185 - mae: 0.0185 - mse: 5.4746e-04 - val_loss: 0.0225 - val_mae: 0.0225 - val_mse: 6.8177e-04 - lr: 1.0000e-05\n",
      "Epoch 369/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0210 - mae: 0.0210 - mse: 6.8920e-04 - val_loss: 0.0226 - val_mae: 0.0226 - val_mse: 6.8721e-04 - lr: 1.0000e-05\n",
      "Epoch 370/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0216 - mae: 0.0216 - mse: 8.3049e-04 - val_loss: 0.0228 - val_mae: 0.0228 - val_mse: 6.9986e-04 - lr: 1.0000e-05\n",
      "Epoch 371/2000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0233 - mae: 0.0233 - mse: 7.7494e-04 - val_loss: 0.0231 - val_mae: 0.0231 - val_mse: 7.1515e-04 - lr: 1.0000e-05\n",
      "Epoch 372/2000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0213 - mae: 0.0213 - mse: 6.7345e-04 - val_loss: 0.0233 - val_mae: 0.0233 - val_mse: 7.2522e-04 - lr: 1.0000e-05\n",
      "Epoch 373/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0203 - mae: 0.0203 - mse: 6.6220e-04 - val_loss: 0.0234 - val_mae: 0.0234 - val_mse: 7.3277e-04 - lr: 1.0000e-05\n",
      "Epoch 374/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0143 - mae: 0.0143 - mse: 3.3905e-04 - val_loss: 0.0235 - val_mae: 0.0235 - val_mse: 7.3626e-04 - lr: 1.0000e-05\n",
      "Epoch 375/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0216 - mae: 0.0216 - mse: 7.5135e-04 - val_loss: 0.0235 - val_mae: 0.0235 - val_mse: 7.3668e-04 - lr: 1.0000e-05\n",
      "Epoch 376/2000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0188 - mae: 0.0188 - mse: 5.3323e-04 - val_loss: 0.0235 - val_mae: 0.0235 - val_mse: 7.4031e-04 - lr: 1.0000e-05\n",
      "Epoch 377/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0219 - mae: 0.0219 - mse: 7.3505e-04 - val_loss: 0.0236 - val_mae: 0.0236 - val_mse: 7.4241e-04 - lr: 1.0000e-05\n",
      "Epoch 378/2000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0203 - mae: 0.0203 - mse: 6.0234e-04 - val_loss: 0.0233 - val_mae: 0.0233 - val_mse: 7.2581e-04 - lr: 1.0000e-05\n",
      "Epoch 379/2000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0187 - mae: 0.0187 - mse: 6.2085e-04 - val_loss: 0.0231 - val_mae: 0.0231 - val_mse: 7.1452e-04 - lr: 1.0000e-05\n",
      "Epoch 380/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0247 - mae: 0.0247 - mse: 0.0010 - val_loss: 0.0230 - val_mae: 0.0230 - val_mse: 7.0762e-04 - lr: 1.0000e-05\n",
      "Epoch 381/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0192 - mae: 0.0192 - mse: 6.1060e-04 - val_loss: 0.0230 - val_mae: 0.0230 - val_mse: 7.1064e-04 - lr: 1.0000e-05\n",
      "Epoch 382/2000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0207 - mae: 0.0207 - mse: 6.8268e-04 - val_loss: 0.0229 - val_mae: 0.0229 - val_mse: 7.0694e-04 - lr: 1.0000e-05\n",
      "Epoch 383/2000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0213 - mae: 0.0213 - mse: 6.9750e-04 - val_loss: 0.0229 - val_mae: 0.0229 - val_mse: 7.0271e-04 - lr: 1.0000e-05\n",
      "Epoch 384/2000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0237 - mae: 0.0237 - mse: 8.6093e-04 - val_loss: 0.0227 - val_mae: 0.0227 - val_mse: 6.9532e-04 - lr: 1.0000e-05\n",
      "Epoch 385/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0211 - mae: 0.0211 - mse: 7.0763e-04 - val_loss: 0.0229 - val_mae: 0.0229 - val_mse: 7.0628e-04 - lr: 1.0000e-05\n",
      "Epoch 386/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0160 - mae: 0.0160 - mse: 4.3093e-04 - val_loss: 0.0234 - val_mae: 0.0234 - val_mse: 7.3094e-04 - lr: 1.0000e-05\n",
      "Epoch 387/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0218 - mae: 0.0218 - mse: 7.3861e-04 - val_loss: 0.0238 - val_mae: 0.0238 - val_mse: 7.5529e-04 - lr: 1.0000e-05\n",
      "Epoch 388/2000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0240 - mae: 0.0240 - mse: 8.6388e-04 - val_loss: 0.0240 - val_mae: 0.0240 - val_mse: 7.6893e-04 - lr: 1.0000e-05\n",
      "Epoch 389/2000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0214 - mae: 0.0214 - mse: 6.5286e-04 - val_loss: 0.0243 - val_mae: 0.0243 - val_mse: 7.8578e-04 - lr: 1.0000e-05\n",
      "Epoch 390/2000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0180 - mae: 0.0180 - mse: 5.6896e-04 - val_loss: 0.0247 - val_mae: 0.0247 - val_mse: 8.0562e-04 - lr: 1.0000e-05\n",
      "Epoch 391/2000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0196 - mae: 0.0196 - mse: 6.6643e-04 - val_loss: 0.0247 - val_mae: 0.0247 - val_mse: 8.0727e-04 - lr: 1.0000e-05\n",
      "Epoch 392/2000\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0211 - mae: 0.0211 - mse: 6.9271e-04 - val_loss: 0.0245 - val_mae: 0.0245 - val_mse: 7.9723e-04 - lr: 1.0000e-05\n",
      "Epoch 393/2000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0193 - mae: 0.0193 - mse: 6.0281e-04 - val_loss: 0.0247 - val_mae: 0.0247 - val_mse: 8.0573e-04 - lr: 1.0000e-05\n",
      "Epoch 394/2000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0232 - mae: 0.0232 - mse: 8.4365e-04 - val_loss: 0.0246 - val_mae: 0.0246 - val_mse: 8.0466e-04 - lr: 1.0000e-05\n",
      "Epoch 395/2000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0195 - mae: 0.0195 - mse: 5.8582e-04 - val_loss: 0.0246 - val_mae: 0.0246 - val_mse: 8.0302e-04 - lr: 1.0000e-05\n",
      "Epoch 396/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0203 - mae: 0.0203 - mse: 6.1077e-04 - val_loss: 0.0244 - val_mae: 0.0244 - val_mse: 7.9261e-04 - lr: 1.0000e-05\n",
      "Epoch 397/2000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0222 - mae: 0.0222 - mse: 7.9163e-04 - val_loss: 0.0242 - val_mae: 0.0242 - val_mse: 7.7805e-04 - lr: 1.0000e-05\n",
      "Epoch 398/2000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0198 - mae: 0.0198 - mse: 6.2866e-04 - val_loss: 0.0239 - val_mae: 0.0239 - val_mse: 7.6221e-04 - lr: 1.0000e-05\n",
      "Epoch 399/2000\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0255 - mae: 0.0255 - mse: 0.0010 - val_loss: 0.0237 - val_mae: 0.0237 - val_mse: 7.5200e-04 - lr: 1.0000e-05\n",
      "Epoch 400/2000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0184 - mae: 0.0184 - mse: 5.6031e-04 - val_loss: 0.0237 - val_mae: 0.0237 - val_mse: 7.5156e-04 - lr: 1.0000e-05\n",
      "Epoch 401/2000\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0199 - mae: 0.0199 - mse: 6.3407e-04 - val_loss: 0.0237 - val_mae: 0.0237 - val_mse: 7.5095e-04 - lr: 1.0000e-05\n",
      "Epoch 402/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0183 - mae: 0.0183 - mse: 4.9695e-04 - val_loss: 0.0238 - val_mae: 0.0238 - val_mse: 7.5432e-04 - lr: 1.0000e-05\n",
      "Epoch 403/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0189 - mae: 0.0189 - mse: 5.9693e-04 - val_loss: 0.0237 - val_mae: 0.0237 - val_mse: 7.5010e-04 - lr: 1.0000e-05\n",
      "Epoch 404/2000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0165 - mae: 0.0165 - mse: 4.9453e-04 - val_loss: 0.0237 - val_mae: 0.0237 - val_mse: 7.5158e-04 - lr: 1.0000e-05\n",
      "Epoch 405/2000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0174 - mae: 0.0174 - mse: 4.7800e-04 - val_loss: 0.0239 - val_mae: 0.0239 - val_mse: 7.6343e-04 - lr: 1.0000e-05\n",
      "Epoch 406/2000\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0178 - mae: 0.0178 - mse: 5.2107e-04 - val_loss: 0.0243 - val_mae: 0.0243 - val_mse: 7.8195e-04 - lr: 1.0000e-05\n",
      "Epoch 407/2000\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0196 - mae: 0.0196 - mse: 6.4428e-04 - val_loss: 0.0247 - val_mae: 0.0247 - val_mse: 8.0880e-04 - lr: 1.0000e-05\n",
      "Epoch 408/2000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0222 - mae: 0.0222 - mse: 8.6267e-04 - val_loss: 0.0251 - val_mae: 0.0251 - val_mse: 8.3186e-04 - lr: 1.0000e-05\n",
      "Epoch 409/2000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0194 - mae: 0.0194 - mse: 6.4286e-04 - val_loss: 0.0253 - val_mae: 0.0253 - val_mse: 8.4453e-04 - lr: 1.0000e-05\n",
      "Epoch 410/2000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0210 - mae: 0.0210 - mse: 7.3268e-04 - val_loss: 0.0254 - val_mae: 0.0254 - val_mse: 8.4847e-04 - lr: 1.0000e-05\n",
      "Epoch 411/2000\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0197 - mae: 0.0197 - mse: 5.7780e-04 - val_loss: 0.0254 - val_mae: 0.0254 - val_mse: 8.4872e-04 - lr: 1.0000e-05\n",
      "Epoch 412/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0206 - mae: 0.0206 - mse: 6.2129e-04 - val_loss: 0.0252 - val_mae: 0.0252 - val_mse: 8.3642e-04 - lr: 1.0000e-05\n",
      "Epoch 413/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0183 - mae: 0.0183 - mse: 6.2194e-04 - val_loss: 0.0251 - val_mae: 0.0251 - val_mse: 8.3467e-04 - lr: 1.0000e-05\n",
      "Epoch 414/2000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0151 - mae: 0.0151 - mse: 3.9959e-04 - val_loss: 0.0255 - val_mae: 0.0255 - val_mse: 8.5395e-04 - lr: 1.0000e-05\n",
      "Epoch 415/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0159 - mae: 0.0159 - mse: 4.5629e-04 - val_loss: 0.0255 - val_mae: 0.0255 - val_mse: 8.5736e-04 - lr: 1.0000e-05\n",
      "Epoch 416/2000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0217 - mae: 0.0217 - mse: 7.8939e-04 - val_loss: 0.0255 - val_mae: 0.0255 - val_mse: 8.5635e-04 - lr: 1.0000e-05\n",
      "Epoch 417/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0166 - mae: 0.0166 - mse: 4.6011e-04 - val_loss: 0.0254 - val_mae: 0.0254 - val_mse: 8.5179e-04 - lr: 1.0000e-05\n",
      "Epoch 418/2000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0189 - mae: 0.0189 - mse: 5.9191e-04 - val_loss: 0.0254 - val_mae: 0.0254 - val_mse: 8.4964e-04 - lr: 1.0000e-05\n",
      "Epoch 419/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0179 - mae: 0.0179 - mse: 5.9340e-04 - val_loss: 0.0252 - val_mae: 0.0252 - val_mse: 8.4049e-04 - lr: 1.0000e-05\n",
      "Epoch 420/2000\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0211 - mae: 0.0211 - mse: 7.4350e-04 - val_loss: 0.0252 - val_mae: 0.0252 - val_mse: 8.3841e-04 - lr: 1.0000e-05\n",
      "Epoch 421/2000\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0202 - mae: 0.0202 - mse: 6.6510e-04 - val_loss: 0.0252 - val_mae: 0.0252 - val_mse: 8.4010e-04 - lr: 1.0000e-05\n",
      "Epoch 422/2000\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0164 - mae: 0.0164 - mse: 3.9749e-04 - val_loss: 0.0252 - val_mae: 0.0252 - val_mse: 8.3642e-04 - lr: 1.0000e-05\n",
      "Epoch 423/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0189 - mae: 0.0189 - mse: 5.8168e-04 - val_loss: 0.0254 - val_mae: 0.0254 - val_mse: 8.5003e-04 - lr: 1.0000e-05\n",
      "Epoch 424/2000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0218 - mae: 0.0218 - mse: 8.1614e-04 - val_loss: 0.0256 - val_mae: 0.0256 - val_mse: 8.6195e-04 - lr: 1.0000e-05\n",
      "Epoch 425/2000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0227 - mae: 0.0227 - mse: 7.5977e-04 - val_loss: 0.0258 - val_mae: 0.0258 - val_mse: 8.7186e-04 - lr: 1.0000e-05\n",
      "Epoch 426/2000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0209 - mae: 0.0209 - mse: 7.4196e-04 - val_loss: 0.0262 - val_mae: 0.0262 - val_mse: 8.9413e-04 - lr: 1.0000e-05\n",
      "Epoch 427/2000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0198 - mae: 0.0198 - mse: 7.1800e-04 - val_loss: 0.0266 - val_mae: 0.0266 - val_mse: 9.1671e-04 - lr: 1.0000e-05\n",
      "Epoch 428/2000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0218 - mae: 0.0218 - mse: 8.9068e-04 - val_loss: 0.0273 - val_mae: 0.0273 - val_mse: 9.5166e-04 - lr: 1.0000e-05\n",
      "Epoch 429/2000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0205 - mae: 0.0205 - mse: 6.8183e-04 - val_loss: 0.0279 - val_mae: 0.0279 - val_mse: 9.8453e-04 - lr: 1.0000e-05\n",
      "Epoch 430/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0185 - mae: 0.0185 - mse: 5.3306e-04 - val_loss: 0.0283 - val_mae: 0.0283 - val_mse: 0.0010 - lr: 1.0000e-05\n",
      "Epoch 431/2000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0194 - mae: 0.0194 - mse: 5.9641e-04 - val_loss: 0.0289 - val_mae: 0.0289 - val_mse: 0.0010 - lr: 1.0000e-05\n",
      "Epoch 432/2000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0192 - mae: 0.0192 - mse: 6.1854e-04 - val_loss: 0.0291 - val_mae: 0.0291 - val_mse: 0.0011 - lr: 1.0000e-05\n",
      "Epoch 433/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0165 - mae: 0.0165 - mse: 4.9093e-04 - val_loss: 0.0289 - val_mae: 0.0289 - val_mse: 0.0010 - lr: 1.0000e-05\n",
      "Epoch 434/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0200 - mae: 0.0200 - mse: 5.5001e-04 - val_loss: 0.0285 - val_mae: 0.0285 - val_mse: 0.0010 - lr: 1.0000e-05\n",
      "Epoch 435/2000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0199 - mae: 0.0199 - mse: 6.7142e-04 - val_loss: 0.0281 - val_mae: 0.0281 - val_mse: 9.9422e-04 - lr: 1.0000e-05\n",
      "Epoch 436/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0215 - mae: 0.0215 - mse: 6.9594e-04 - val_loss: 0.0275 - val_mae: 0.0275 - val_mse: 9.6548e-04 - lr: 1.0000e-05\n",
      "Epoch 437/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0162 - mae: 0.0162 - mse: 4.7313e-04 - val_loss: 0.0266 - val_mae: 0.0266 - val_mse: 9.1302e-04 - lr: 1.0000e-05\n",
      "Epoch 438/2000\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0179 - mae: 0.0179 - mse: 4.9047e-04 - val_loss: 0.0256 - val_mae: 0.0256 - val_mse: 8.6205e-04 - lr: 1.0000e-05\n",
      "Epoch 439/2000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0173 - mae: 0.0173 - mse: 4.9561e-04 - val_loss: 0.0252 - val_mae: 0.0252 - val_mse: 8.4088e-04 - lr: 1.0000e-05\n",
      "Epoch 440/2000\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0183 - mae: 0.0183 - mse: 5.7684e-04 - val_loss: 0.0250 - val_mae: 0.0250 - val_mse: 8.2599e-04 - lr: 1.0000e-05\n",
      "Epoch 441/2000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0184 - mae: 0.0184 - mse: 5.6802e-04 - val_loss: 0.0246 - val_mae: 0.0246 - val_mse: 8.0209e-04 - lr: 1.0000e-05\n",
      "Epoch 442/2000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0209 - mae: 0.0209 - mse: 5.9739e-04 - val_loss: 0.0245 - val_mae: 0.0245 - val_mse: 7.9390e-04 - lr: 1.0000e-05\n",
      "Epoch 443/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0225 - mae: 0.0225 - mse: 7.8752e-04 - val_loss: 0.0242 - val_mae: 0.0242 - val_mse: 7.8147e-04 - lr: 1.0000e-05\n",
      "Epoch 444/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0202 - mae: 0.0202 - mse: 6.8660e-04 - val_loss: 0.0238 - val_mae: 0.0238 - val_mse: 7.5387e-04 - lr: 1.0000e-05\n",
      "Epoch 445/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0193 - mae: 0.0193 - mse: 5.7907e-04 - val_loss: 0.0235 - val_mae: 0.0235 - val_mse: 7.3732e-04 - lr: 1.0000e-05\n",
      "Epoch 446/2000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0177 - mae: 0.0177 - mse: 4.8238e-04 - val_loss: 0.0234 - val_mae: 0.0234 - val_mse: 7.3575e-04 - lr: 1.0000e-05\n",
      "Epoch 447/2000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0200 - mae: 0.0200 - mse: 6.5721e-04 - val_loss: 0.0234 - val_mae: 0.0234 - val_mse: 7.3573e-04 - lr: 1.0000e-05\n",
      "Epoch 448/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0190 - mae: 0.0190 - mse: 5.7219e-04 - val_loss: 0.0236 - val_mae: 0.0236 - val_mse: 7.4474e-04 - lr: 1.0000e-05\n",
      "Epoch 449/2000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0161 - mae: 0.0161 - mse: 4.2525e-04 - val_loss: 0.0237 - val_mae: 0.0237 - val_mse: 7.5105e-04 - lr: 1.0000e-05\n",
      "Epoch 450/2000\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0190 - mae: 0.0190 - mse: 5.3607e-04 - val_loss: 0.0236 - val_mae: 0.0236 - val_mse: 7.4581e-04 - lr: 1.0000e-05\n",
      "Epoch 451/2000\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0233 - mae: 0.0233 - mse: 7.7589e-04 - val_loss: 0.0231 - val_mae: 0.0231 - val_mse: 7.1667e-04 - lr: 1.0000e-05\n",
      "Epoch 452/2000\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.0234 - mae: 0.0234 - mse: 7.6835e-04 - val_loss: 0.0225 - val_mae: 0.0225 - val_mse: 6.8544e-04 - lr: 1.0000e-05\n",
      "Epoch 453/2000\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.0179 - mae: 0.0179 - mse: 5.8684e-04 - val_loss: 0.0223 - val_mae: 0.0223 - val_mse: 6.7593e-04 - lr: 1.0000e-05\n",
      "Epoch 454/2000\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0178 - mae: 0.0178 - mse: 5.0484e-04 - val_loss: 0.0222 - val_mae: 0.0222 - val_mse: 6.7079e-04 - lr: 1.0000e-05\n",
      "Epoch 455/2000\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0191 - mae: 0.0191 - mse: 6.0992e-04 - val_loss: 0.0223 - val_mae: 0.0223 - val_mse: 6.7500e-04 - lr: 1.0000e-05\n",
      "Epoch 456/2000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0206 - mae: 0.0206 - mse: 6.4091e-04 - val_loss: 0.0226 - val_mae: 0.0226 - val_mse: 6.8742e-04 - lr: 1.0000e-05\n",
      "Epoch 457/2000\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.0175 - mae: 0.0175 - mse: 5.0264e-04 - val_loss: 0.0231 - val_mae: 0.0231 - val_mse: 7.1740e-04 - lr: 1.0000e-05\n",
      "Epoch 458/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0233 - mae: 0.0233 - mse: 8.0988e-04\n",
      "Epoch 00458: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.0223 - mae: 0.0223 - mse: 7.3993e-04 - val_loss: 0.0240 - val_mae: 0.0240 - val_mse: 7.6519e-04 - lr: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mae\",optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001), metrics=[\"mae\",\"mse\"])\n",
    "history = model.fit(final_x_zeros_scaled[:43],final_y_zeros[:43],\n",
    "                    epochs = 2000,\n",
    "                    initial_epoch = history.epoch[-1],\n",
    "                    validation_data = (final_x_zeros_scaled[43:], final_y_zeros[43:]),\n",
    "                   callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", \n",
    "                                                        patience=200, \n",
    "                                                        restore_best_weights=True),\n",
    "                      tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", \n",
    "                                                           patience=100, \n",
    "                                                           verbose=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "260648df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([811.1143 , 808.73364, 802.76324, 799.2455 , 799.18695, 801.7036 ,\n",
       "       803.1478 , 802.50586, 795.94183, 798.50183, 803.0028 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = tf.squeeze(model.predict(final_x_zeros_scaled[43:])).numpy()\n",
    "preds = preds*k.values[0]\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e14b7f4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107.05517578125"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae = sum(abs(preds-(tf.squeeze(final_y_zeros[43:].to_numpy()).numpy())*k.values[0]))\n",
    "mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7af7da9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([801.65, 794.65, 781.15, 788.8 , 810.3 , 813.75, 808.95, 805.05,\n",
       "       792.5 , 802.8 , 790.8 ], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tf.squeeze(final_y_zeros[43:].to_numpy()).numpy())*k.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27e2222e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 25). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: LSTM_icici\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: LSTM_icici\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x000001BF02BB6F70> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x000001C15F7E0BE0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x000001C18BFC9280> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x000001C18C1D2EE0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x000001C18C07D040> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "model.save(\"LSTM_icici\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ws_env",
   "language": "python",
   "name": "ws_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
